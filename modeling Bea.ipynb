{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install prophet == 1.1.6\n",
    "# pip install keras == 3.9.0 tensorflow == 2.19.0\n",
    "# pip install neuralprophet == 0.8.0\n",
    "# pytorch-forecasting-1.3.0\n",
    "#pip install nixtla == 0.6.6\n",
    "#pip install pmdarima == 2.0.4\n",
    "#pip install scikeras == 0.13.0\n",
    "# !pip install keras-tcn==3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#Check if Y is stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Modeling Pipeline (Feature selection, Scaling, Model testing)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    root_mean_squared_error  # New recommended way for RMSE\n",
    ")\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer, NBeats\n",
    "from pytorch_forecasting.data.timeseries import TimeSeriesDataSet\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit\n",
    "\n",
    "from nixtla import NixtlaClient\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)       # Set display width\n",
    "pd.set_option('display.max_colwidth', 100) # Show full feature lists\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # 4 decimal places\n",
    "\n",
    "# If you want to force standard notation (no scientific):\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x if abs(x) > 1e-4 else '%.4e' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded P1 from product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from product_dfs_folder\\P13.pkl\n",
      "Loaded P14 from product_dfs_folder\\P14.pkl\n",
      "Loaded P16 from product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from product_dfs_folder\\P5.pkl\n",
      "Loaded P6 from product_dfs_folder\\P6.pkl\n",
      "Loaded P8 from product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from product_dfs_folder\\P9.pkl\n",
      "Loaded Sales_CPI from product_dfs_folder\\Sales_CPI.pkl\n",
      "Loaded P1 from lagged_product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from lagged_product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from lagged_product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from lagged_product_dfs_folder\\P13.pkl\n",
      "Loaded P16 from lagged_product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from lagged_product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from lagged_product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from lagged_product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from lagged_product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from lagged_product_dfs_folder\\P5.pkl\n",
      "Loaded P8 from lagged_product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from lagged_product_dfs_folder\\P9.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_dfs_from_folder(folder_path):\n",
    "    \"\"\"Loads DataFrames from files in a specified folder and returns a dictionary.\"\"\"\n",
    "    dfs = {}\n",
    "    # List all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pkl\"):\n",
    "            key = file_name.replace(\".pkl\", \"\")  # Extract key from the file name\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the dataframe from the pickle file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                dfs[key] = pickle.load(f)\n",
    "            print(f\"Loaded {key} from {file_path}\")\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# Load both product_dfs and lagged_product_dfs from their respective folders\n",
    "product_dfs = load_dfs_from_folder(\"product_dfs_folder\")\n",
    "lagged_product_dfs = load_dfs_from_folder(\"lagged_product_dfs_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_id in product_dfs.keys():\n",
    "    product_dfs[product_id] = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "\n",
    "for product_id in lagged_product_dfs.keys():\n",
    "    lagged_product_dfs[product_id] = lagged_product_dfs[product_id].rename(columns={product_id: \"Sales\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>MAB_ELE_SHP840</th>\n",
       "      <th>PRI27276_org</th>\n",
       "      <th>PRO27826_org</th>\n",
       "      <th>MAB_ELE_PRO276</th>\n",
       "      <th>MAB_ELE_SHP1100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>35774028.5209</td>\n",
       "      <td>127.8088</td>\n",
       "      <td>109.1196</td>\n",
       "      <td>118.6708</td>\n",
       "      <td>124.2279</td>\n",
       "      <td>130.9893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>5063648.6000</td>\n",
       "      <td>117.6759</td>\n",
       "      <td>109.2248</td>\n",
       "      <td>120.4670</td>\n",
       "      <td>127.4041</td>\n",
       "      <td>132.9341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>37321267.9382</td>\n",
       "      <td>123.2801</td>\n",
       "      <td>109.3301</td>\n",
       "      <td>105.3787</td>\n",
       "      <td>120.5186</td>\n",
       "      <td>131.2613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>27090400.9380</td>\n",
       "      <td>111.0438</td>\n",
       "      <td>109.7510</td>\n",
       "      <td>107.1749</td>\n",
       "      <td>104.7763</td>\n",
       "      <td>113.0576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>34132093.4229</td>\n",
       "      <td>116.7369</td>\n",
       "      <td>109.8562</td>\n",
       "      <td>110.6476</td>\n",
       "      <td>109.5970</td>\n",
       "      <td>117.7047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sales  MAB_ELE_SHP840  PRI27276_org  PRO27826_org  MAB_ELE_PRO276  MAB_ELE_SHP1100\n",
       "month_year                                                                                           \n",
       "2018-10-01 35774028.5209        127.8088      109.1196      118.6708        124.2279         130.9893\n",
       "2018-11-01  5063648.6000        117.6759      109.2248      120.4670        127.4041         132.9341\n",
       "2018-12-01 37321267.9382        123.2801      109.3301      105.3787        120.5186         131.2613\n",
       "2019-01-01 27090400.9380        111.0438      109.7510      107.1749        104.7763         113.0576\n",
       "2019-02-01 34132093.4229        116.7369      109.8562      110.6476        109.5970         117.7047"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dfs['P1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>PRI27840_org</th>\n",
       "      <th>RohCOPPER1000_org</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>P8_lag_1</th>\n",
       "      <th>P8_lag_2</th>\n",
       "      <th>P8_lag_5</th>\n",
       "      <th>P8_lag_6</th>\n",
       "      <th>P8_lag_10</th>\n",
       "      <th>P8_ma_1</th>\n",
       "      <th>P8_ma_2</th>\n",
       "      <th>P8_ma_5</th>\n",
       "      <th>P8_ma_6</th>\n",
       "      <th>P8_ma_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>398332.2744</td>\n",
       "      <td>110.6561</td>\n",
       "      <td>75.7745</td>\n",
       "      <td>10</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>580778.2653</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>383206.7376</td>\n",
       "      <td>380139.7221</td>\n",
       "      <td>413852.9409</td>\n",
       "      <td>400303.6854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>111.0503</td>\n",
       "      <td>76.4355</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>518398.3785</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>742593.7831</td>\n",
       "      <td>514233.4804</td>\n",
       "      <td>497925.6504</td>\n",
       "      <td>457149.3767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>112014.9227</td>\n",
       "      <td>111.0322</td>\n",
       "      <td>76.4097</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>267418.3494</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>599435.1073</td>\n",
       "      <td>477114.1424</td>\n",
       "      <td>447197.0541</td>\n",
       "      <td>441609.0341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>540208.8029</td>\n",
       "      <td>111.0354</td>\n",
       "      <td>77.7720</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>372627.9465</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>326111.8628</td>\n",
       "      <td>501098.4985</td>\n",
       "      <td>487629.9191</td>\n",
       "      <td>458367.1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>491332.7200</td>\n",
       "      <td>111.2866</td>\n",
       "      <td>80.6535</td>\n",
       "      <td>14</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>491332.7200</td>\n",
       "      <td>515770.7614</td>\n",
       "      <td>525748.8024</td>\n",
       "      <td>499470.8688</td>\n",
       "      <td>471352.9383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Sales  PRI27840_org  RohCOPPER1000_org  time_idx  year  month     P8_lag_1     P8_lag_2    P8_lag_5    P8_lag_6   P8_lag_10      P8_ma_1     P8_ma_2     P8_ma_5     P8_ma_6    P8_ma_10\n",
       "month_year                                                                                                                                                                                                \n",
       "2019-08-01  398332.2744      110.6561            75.7745        10  2019      8  368081.2008  420287.0223 582419.0346 361474.5342 580778.2653  398332.2744 383206.7376 380139.7221 413852.9409 400303.6854\n",
       "2019-09-01 1086855.2918      111.0503            76.4355        11  2019      9  398332.2744  368081.2008 416386.5006 582419.0346 518398.3785 1086855.2918 742593.7831 514233.4804 497925.6504 457149.3767\n",
       "2019-10-01  112014.9227      111.0322            76.4097        12  2019     10 1086855.2918  398332.2744 297611.6126 416386.5006 267418.3494  112014.9227 599435.1073 477114.1424 447197.0541 441609.0341\n",
       "2019-11-01  540208.8029      111.0354            77.7720        13  2019     11  112014.9227 1086855.2918 420287.0223 297611.6126 372627.9465  540208.8029 326111.8628 501098.4985 487629.9191 458367.1197\n",
       "2019-12-01  491332.7200      111.2866            80.6535        14  2019     12  540208.8029  112014.9227 368081.2008 420287.0223 361474.5342  491332.7200 515770.7614 525748.8024 499470.8688 471352.9383"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_product_dfs['P8'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(train, val=None, test=None, outlier_treatment=True):\n",
    "\n",
    "    def winsorize_function(df, cols, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        \"\"\"Apply Winsorization only to the product sales column\"\"\"\n",
    "        df = df.copy()\n",
    "        bounds = {}\n",
    "        for col in cols:\n",
    "            q1 = df[col].quantile(lower_quantile)\n",
    "            q3 = df[col].quantile(upper_quantile)\n",
    "            df[col] = df[col].clip(lower=q1, upper=q3)\n",
    "            bounds[col] = (q1, q3)  # Store actual percentile values\n",
    "            print(f\"{col}: Winsorized Bounds -> Lower = {q1:.2f}, Upper = {q3:.2f}\")\n",
    "\n",
    "        return df, bounds\n",
    "        \n",
    "    def process_dataset(df, cols, is_train=True, bounds=None):\n",
    "        df = df.copy() \n",
    "        if is_train:\n",
    "            if outlier_treatment:\n",
    "                df, bounds = winsorize_function(df, cols)\n",
    "            else:\n",
    "                bounds = {}  \n",
    "        else:\n",
    "            if outlier_treatment:\n",
    "                if bounds is None:\n",
    "                    raise ValueError(\"Bounds must be provided for validation and test datasets.\")\n",
    "                for col in cols: \n",
    "                    if col in bounds:\n",
    "                        lower, upper = bounds[col]\n",
    "                        df[col] = df[col].clip(lower, upper)  # Corrected clipping\n",
    "        return (df, bounds) if is_train else df\n",
    "\n",
    "    # Process the training dataset\n",
    "    train, bounds = process_dataset(train, cols = train.columns, is_train=True)\n",
    "\n",
    "    # Process validation and test datasets with correct bounds\n",
    "    if val is not None:\n",
    "        val = process_dataset(val, cols = val.columns, is_train=False, bounds=bounds)\n",
    "\n",
    "    if test is not None:\n",
    "        test = process_dataset(test, cols = test.columns, is_train=False, bounds=bounds)\n",
    "        \n",
    "    # Return the datasets \n",
    "    if test is not None and val is not None:\n",
    "        return train, val, test\n",
    "    elif val is not None:\n",
    "        return train, val\n",
    "    elif test is not None:\n",
    "        return train, test\n",
    "    else:\n",
    "        return train\n",
    "\n",
    "def time_series_train_test_split(X, y, test_size=10):\n",
    "    \"\"\"Split time series data maintaining temporal order\"\"\"\n",
    "    split_idx = len(X) - test_size\n",
    "    return (\n",
    "        X.iloc[:split_idx], X.iloc[split_idx:],\n",
    "        y.iloc[:split_idx], y.iloc[split_idx:])\n",
    "\n",
    "def prepare_time_series_data(df_train, feature_set, target_col=\"Sales\", horizon=10, winsorize=False, scaling=False):\n",
    "\n",
    "    # Extract product data\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    y = data[[target_col]]  \n",
    "    X = data.drop(columns=[target_col])\n",
    "    \n",
    "    # Select features - handle empty feature_set case\n",
    "    if feature_set:\n",
    "        try:\n",
    "            # Convert feature_set to list if it's not already\n",
    "            if not isinstance(feature_set, list):\n",
    "                feature_set = [feature_set]\n",
    "            # Select only columns that exist in X\n",
    "            available_features = [f for f in feature_set if f in X.columns]\n",
    "            X = X[available_features]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Some features not found in DataFrame: {e}\")\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_val, y_train, y_val = time_series_train_test_split(X, y, test_size=horizon)\n",
    "\n",
    "    # Apply preprocessing steps\n",
    "    y_train, y_val = preprocessing_pipeline(y_train, y_val, test=None, outlier_treatment=winsorize)\n",
    "\n",
    "    # Scaling\n",
    "    if scaling: \n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_X.transform(X_val)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n",
    "        y_train_scaled = pd.DataFrame(y_train_scaled, index=y_train.index, columns=y_train.columns)\n",
    "        y_val_scaled = pd.DataFrame(y_val_scaled, index=y_val.index, columns=y_val.columns)\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled\n",
    "    else:\n",
    "        return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combinations(features, max_features=None):\n",
    "    \"\"\"Generate all possible feature combinations\"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = len(features)\n",
    "    \n",
    "    all_combinations = []\n",
    "    for r in range(1, max_features + 1):\n",
    "        all_combinations.extend(combinations(features, r))\n",
    "    \n",
    "    return [list(comb) for comb in all_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    data = []\n",
    "    \n",
    "    # Iterate over each result in the list\n",
    "    for result in results:\n",
    "        product_id = result.get('product_id', None)  # Safely get 'product_id'\n",
    "        winsorize = result.get('winsorize', None)  # Safely get 'winsorize'\n",
    "        \n",
    "        # Handle empty 'features' list\n",
    "        features = ', '.join(result['features']) if result['features'] else 'all'\n",
    "        \n",
    "        metrics = result.get('metrics', {})\n",
    "        \n",
    "        # Handle 'validation_predictions' list, use an empty list if missing\n",
    "        validation_predictions = result.get('validation_predictions', [])\n",
    "        \n",
    "        # Iterate over each validation prediction and create a row in the DataFrame\n",
    "        for pred in validation_predictions:\n",
    "            data.append({\n",
    "                'product_id': product_id,\n",
    "                'winsorize': winsorize,\n",
    "                'features': features,\n",
    "                'RMSE': metrics.get('RMSE', np.nan),  # Use np.nan if RMSE is missing\n",
    "                'MAPE': metrics.get('MAPE', np.nan),  # Use np.nan if MAPE is missing\n",
    "                'R2': metrics.get('R2', np.nan),  # Use np.nan if R2 is missing\n",
    "                'Overfit_Score': metrics.get('Overfit Score', np.nan),  # Use np.nan if 'Overfit Score' is missing\n",
    "                'validation_prediction': pred\n",
    "            })\n",
    "    \n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df_results = pd.DataFrame(data)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap:\n",
    "\n",
    "✅ Check if Y is stationary\n",
    "\n",
    "✅ Feature Selection → SHAP, RFE, or Correlation Matrix \n",
    "\n",
    "✅ Create a Pipeline → feature-engineering & AutoML \n",
    "\n",
    "✅ Base Models → ARIMA, XGBoost, Prophet, LSTMs\n",
    "\n",
    "✅ Models Evaluation → RMSE, MAE, and hyperparameter tuning \n",
    "\n",
    "✅ Deployment → MLOps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Modeling Pipeline (Feature selection, Scaling, Model testing):**\n",
    "\n",
    "#### 4.1. Train vs Validation Splitting Strategy\n",
    "\n",
    "Our pipeline is going to iterate over:\n",
    "\n",
    "- **Rolling Forecast Validation (Expanding Window)**: The model is trained on an expanding dataset, making predictions on a fixed-size validation set.\n",
    "Example: Train on 2010-2018 → Validate on 2019, then Train on 2010-2019 → Validate on 2020.\n",
    "- **Walk-Forward Validation:** One-step forecasting where the model predicts the next step, and the validation window moves forward one step at a time.\n",
    "\n",
    "#### 4.2. Scalers:\n",
    "- Min-Max Scaling (0-1)\n",
    "- Standard Scaling (Z-score, Mean=0, Std=1) \n",
    "\n",
    "#### 4.2. Feature Combinations\n",
    "\n",
    "- No macro features (baseline model).\n",
    "- Single-feature models.\n",
    "- Multi-feature models.\n",
    "\n",
    "Grid Search for Key Features rather than brute-force all combinations.???\n",
    "\n",
    "#### 4.4. Models\n",
    "\n",
    "First, base models and then we will tune hyperparameters of the best ones, wuth the best featues and best scalers, with the appropraite split method\n",
    "\n",
    "| **Model**         | **Stationary Required?** | **Uses Lags?** | **Handles Trends?** | **Uses Macro Data?** | **Uses TensorFlow/Keras?** | **How?** |\n",
    "|------------------|--------------------|------------|----------------|----------------|------------------|----------------------------|\n",
    "| **ARIMA (Adapted ARIMAX)** | ❌ No (if differenced) | ❌ No | ✅ Yes (Differencing) | ✅ Yes (Exogenous Variables) | ❌ No | Traditional statistical method, but can handle macro data with ARIMAX |\n",
    "| **Prophet**     | ❌ No | ✅ Yes (optional) | ✅ Yes | ✅ Yes | ❌ No | Handles trends/seasonality, allows external regressors |\n",
    "| **NeuralProphet** | ❌ No | ✅ Yes | ✅ Yes | ✅ Yes | ❌ No | Deep learning-inspired improvement of Prophet |\n",
    "| **XGBoost**     | ❌ No (Handles Trends) | ✅ Yes | ✅ Yes | ✅ Yes | ❌ No | Gradient boosting for structured time-series forecasting |\n",
    "| **LSTMs**       | ❌ No (Learns Sequences) | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes | Sequential modeling using TensorFlow/Keras |\n",
    "| **TFT (Temporal Fusion Transformer)** | ❌ No | ✅ Yes | ✅ Yes | ✅ Yes | ✅ Yes | Uses attention mechanisms in TensorFlow/Keras for multivariate forecasting |\n",
    "| **N-BEATS**     | ❌ No | ✅ Yes | ✅ Yes | ❌ No | ✅ Yes | Neural network-based forecasting using TensorFlow/Keras |\n",
    " add TimeGPT,  Google's TimesFM., LSTMs, Prophet,  NeuralProphet, ARIMA (Adapted ARIMAX)\n",
    "\n",
    "#### 4.5. Evaluation Metrics: \n",
    "- RMSE → Measures error magnitude.\n",
    "- MAPE → Percentage-based error, useful for business impact.\n",
    "- R² → Measures how well the model explains variance.\n",
    "- Overfit Score →  \n",
    "\n",
    "$$\n",
    "\\text{Overfit Score} = \\frac{\\text{Test RMSE} - \\text{Train RMSE}}{\\text{Train RMSE}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "    - < 0.1 (Underfit)\n",
    "    - 0.1 - 0.5 (Good Fit)\n",
    "    - > 0.5 (Overfit Warning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Predictions\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "    test_rmse = mean_squared_error(y_test, test_predictions, squared=False)\n",
    "    train_r2 = r2_score(y_train, train_predictions)\n",
    "    test_r2 = r2_score(y_test, test_predictions)\n",
    "    train_mape = mean_absolute_percentage_error(y_train, train_predictions)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_predictions)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"=== Training Set ===\")\n",
    "    print(f\"RMSE: {train_rmse:.3f}\")\n",
    "    print(f\"MAPE: {train_mape*100:.2f}%\")\n",
    "    print(f\"R²: {train_r2:.3f}\\n\")\n",
    "    \n",
    "    print(\"=== Test Set ===\")\n",
    "    print(f\"RMSE: {test_rmse:.3f}\")\n",
    "    print(f\"MAPE: {test_mape*100:.2f}%\")\n",
    "    print(f\"R²: {test_r2:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'train': {'RMSE': train_rmse, 'MAPE': train_mape, 'R2': train_r2},\n",
    "        'test': {'RMSE': test_rmse, 'MAPE': test_mape, 'R2': test_r2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(model, X_train, y_train, X_test, y_test, \n",
    "                          title=\"Model Performance\", show_confidence=True):\n",
    "    # Get predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                        subplot_titles=(\"Training Set Performance\", \"Test Set Performance\"),\n",
    "                        vertical_spacing=0.15)\n",
    "    \n",
    "    # Training set plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train.index,\n",
    "        y=y_train,\n",
    "        mode='lines',\n",
    "        name='Actual (Train)',\n",
    "        line=dict(color='black', width=2)\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train.index,\n",
    "        y=train_pred,\n",
    "        mode='lines',\n",
    "        name='Predicted (Train)',\n",
    "        line=dict(color='blue', width=1, dash='dash')\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Test set plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_test.index,\n",
    "        y=y_test,\n",
    "        mode='lines',\n",
    "        name='Actual (Test)',\n",
    "        line=dict(color='black', width=2),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_test.index,\n",
    "        y=test_pred,\n",
    "        mode='lines',\n",
    "        name='Predicted (Test)',\n",
    "        line=dict(color='red', width=1, dash='dash'),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    # Add confidence intervals if available and requested\n",
    "    if show_confidence and hasattr(model, 'predict_interval'):\n",
    "        try:\n",
    "            _, train_lower, train_upper = model.predict_interval(X_train)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(X_train.index) + list(X_train.index[::-1]),\n",
    "                y=list(train_upper) + list(train_lower[::-1]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0,100,80,0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                name='95% Confidence',\n",
    "                showlegend=False\n",
    "            ), row=1, col=1)\n",
    "            \n",
    "            _, test_lower, test_upper = model.predict_interval(X_test)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(X_test.index) + list(X_test.index[::-1]),\n",
    "                y=list(test_upper) + list(test_lower[::-1]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(100,0,80,0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                name='95% Confidence',\n",
    "                showlegend=False\n",
    "            ), row=2, col=1)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=2, col=1)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(history):\n",
    "    \"\"\"For models that provide training history (like neural networks)\"\"\"\n",
    "    if not hasattr(history, 'history'):\n",
    "        print(\"Model doesn't have training history to plot\")\n",
    "        return\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training loss\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.arange(len(history.history['loss'])),\n",
    "        y=history.history['loss'],\n",
    "        mode='lines',\n",
    "        name='Training Loss',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    # Plot validation loss if available\n",
    "    if 'val_loss' in history.history:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(len(history.history['val_loss'])),\n",
    "            y=history.history['val_loss'],\n",
    "            mode='lines',\n",
    "            name='Validation Loss',\n",
    "            line=dict(color='orange')\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Training Progress\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error\n",
    ")\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all relevant metrics for time series forecasting\"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': mean_squared_error(y_true, y_pred, squared=False),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,  # as percentage\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'Error_Std': np.std(y_true - y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Model Performance\"):\n",
    "    \"\"\"Print metrics in a consistent format\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "    print(f\"MAE: {metrics['MAE']:.3f}\")\n",
    "    print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    print(f\"R²: {metrics['R2']:.3f}\")\n",
    "    print(f\"Error Std: {metrics['Error_Std']:.3f}\")\n",
    "\n",
    "def plot_time_series_forecast(\n",
    "    train_series, \n",
    "    test_series, \n",
    "    predictions,\n",
    "    train_predictions=None,\n",
    "    confidence_intervals=None,\n",
    "    title=\"Forecast Performance\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Value\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create interactive forecast visualization with Plotly\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_series : pd.Series with training data\n",
    "    test_series : pd.Series with actual test values\n",
    "    predictions : array-like with predicted values\n",
    "    train_predictions : array-like with predictions on training set (optional)\n",
    "    confidence_intervals : tuple of (upper_bound, lower_bound) arrays\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train_series.index,\n",
    "        y=train_series,\n",
    "        mode='lines',\n",
    "        name='Training Data',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Training predictions if provided\n",
    "    if train_predictions is not None:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=train_series.index,\n",
    "            y=train_predictions,\n",
    "            mode='lines',\n",
    "            name='Training Predictions',\n",
    "            line=dict(color='#ff7f0e', width=1.5, dash='dot')\n",
    "        ))\n",
    "    \n",
    "    # Test data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_series.index,\n",
    "        y=test_series,\n",
    "        mode='lines',\n",
    "        name='Actual Values',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Predictions\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_series.index,\n",
    "        y=predictions,\n",
    "        mode='lines',\n",
    "        name='Predictions',\n",
    "        line=dict(color='#d62728', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Confidence intervals if provided\n",
    "    if confidence_intervals is not None:\n",
    "        upper, lower = confidence_intervals\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(test_series.index) + list(test_series.index[::-1]),\n",
    "            y=list(upper) + list(lower[::-1]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(173, 216, 230, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Confidence',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        margin=dict(l=20, r=20, t=40, b=20)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_training_history(\n",
    "    history,\n",
    "    metrics=['loss'],\n",
    "    val_metrics=None,\n",
    "    title=\"Training Progress\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot training history for models that provide it (like neural networks)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object or dict with training metrics\n",
    "    metrics : list of metrics to plot from training\n",
    "    val_metrics : list of corresponding validation metrics\n",
    "    \"\"\"\n",
    "    if not hasattr(history, 'history') and not isinstance(history, dict):\n",
    "        print(\"No training history available to plot\")\n",
    "        return None\n",
    "    \n",
    "    history_dict = history.history if hasattr(history, 'history') else history\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training metrics\n",
    "    for metric in metrics:\n",
    "        if metric in history_dict:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=np.arange(1, len(history_dict[metric])+1),\n",
    "                y=history_dict[metric],\n",
    "                mode='lines+markers',\n",
    "                name=f'Training {metric}',\n",
    "                line=dict(width=2)\n",
    "            ))\n",
    "    \n",
    "    # Plot validation metrics if available\n",
    "    if val_metrics:\n",
    "        for val_metric in val_metrics:\n",
    "            if val_metric in history_dict:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=np.arange(1, len(history_dict[val_metric])+1),\n",
    "                    y=history_dict[val_metric],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'Validation {val_metric}',\n",
    "                    line=dict(width=2, dash='dash')\n",
    "                ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Metric Value\",\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def evaluate_and_visualize(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_type='standard',\n",
    "    confidence_intervals=None,\n",
    "    training_history=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation and visualization wrapper\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fitted model\n",
    "    X_train, y_train : training data\n",
    "    X_test, y_test : test data\n",
    "    model_type : str - 'standard', 'sequential', 'prophet', etc.\n",
    "    confidence_intervals : tuple of (upper, lower) bounds\n",
    "    training_history : history object for models that track training\n",
    "    \"\"\"\n",
    "    # Get predictions based on model type\n",
    "    if model_type == 'sequential':  # For Keras models\n",
    "        train_pred = model.predict(X_train).flatten()\n",
    "        test_pred = model.predict(X_test).flatten()\n",
    "    elif model_type == 'prophet':\n",
    "        train_pred = model.predict(X_train)['yhat'].values\n",
    "        test_pred = model.predict(X_test)['yhat'].values\n",
    "    else:  # Standard sklearn-style models\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = calculate_metrics(y_train, train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, test_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(train_metrics, \"Training Set Performance\")\n",
    "    print_metrics(test_metrics, \"Test Set Performance\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    forecast_fig = plot_time_series_forecast(\n",
    "        train_series=y_train,\n",
    "        test_series=y_test,\n",
    "        predictions=test_pred,\n",
    "        train_predictions=train_pred,\n",
    "        confidence_intervals=confidence_intervals,\n",
    "        title=\"Model Forecast Performance\"\n",
    "    )\n",
    "    \n",
    "    if training_history is not None:\n",
    "        history_fig = plot_training_history(training_history)\n",
    "        return {\n",
    "            'metrics': {'train': train_metrics, 'test': test_metrics},\n",
    "            'forecast_plot': forecast_fig,\n",
    "            'training_plot': history_fig\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'metrics': {'train': train_metrics, 'test': test_metrics},\n",
    "        'forecast_plot': forecast_fig\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. TimeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = \"nixak-CIwSKQ0cRLIuFR1TYllLFVakTGx3WCY30YPEKfxG0lDQcE0akGo3GE4aMJO9XXbkKjdFaGDP5x6uSxQ6\"\n",
    ")\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_train=None, y_train_pred=None, print_metrics=False):\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,  # as percentage\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_train is not None and y_train_pred is not None:\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        metrics['Overfit Score'] = (metrics['RMSE'] - train_rmse) / max(train_rmse, 1e-10)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"\\n=== Metrics ===\")\n",
    "        print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "        print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"R²: {metrics['R2']:.3f}\")\n",
    "        if 'Overfit Score' in metrics:\n",
    "            print(f\"Overfit Score: {metrics['Overfit Score']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def convert_results_to_df(results):\n",
    "    \"\"\"Convert results to DataFrame format\"\"\"\n",
    "    if results is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame([{\n",
    "        'product_id': results['product_id'],\n",
    "        'winsorize': results['winsorize'],\n",
    "        'features': ', '.join(results['features']) if results['features'] else 'all',\n",
    "        'RMSE': results['metrics']['RMSE'],\n",
    "        'MAPE': results['metrics']['MAPE'],\n",
    "        'Overfit_Score': results['metrics'].get('Overfit Score', np.nan),\n",
    "        'method': results['method']\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_timegpt(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                              nixtla_client=None, winsorize=False, \n",
    "                              horizon=7):\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if nixtla_client is None:\n",
    "        raise ValueError(\"NixtlaClient instance is required.\")\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, feature_set, target_col, horizon, winsorize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    " \n",
    "    # --- Determine Feature Types ---\n",
    "    hist_exog = [f for f in feature_set if '_lag_' in f or '_ma_' in f] if feature_set else []\n",
    "    fut_exog = [f for f in feature_set if f not in hist_exog] if feature_set else []\n",
    "\n",
    "\n",
    "    # --- Prepare Future Exogenous Features ---\n",
    "    X_future = None\n",
    "    if fut_exog:\n",
    "        X_future = X_val[fut_exog].copy()\n",
    "        if not X_future.isnull().all().all():  \n",
    "            X_future.insert(0, 'month_year', X_val.index)\n",
    "        else:\n",
    "            X_future = None  # Avoid passing empty DataFrame\n",
    "\n",
    "    # --- Simple Forecast (12 ≤ obs < 36) ---\n",
    "    if len(X_train) < 36:\n",
    "        print(f\"Using simple forecast ({len(X_train)} < 36 obs)\")\n",
    "        try:\n",
    "            history = df_train.copy()\n",
    "            if feature_set:\n",
    "                history = history[feature_set + [target_col]]\n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history.reset_index(),\n",
    "                                              time_col=\"month_year\", target_col=target_col, h=horizon,\n",
    "                                              X_df=X_future,  # Always pass X_future if available\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)\n",
    "            \n",
    "            # Get training predictions\n",
    "            train_pred = nixtla_client.forecast(df=history.reset_index(),time_col=\"month_year\",\n",
    "                                                target_col=target_col,h=1,hist_exog_list=hist_exog if hist_exog else None\n",
    "                                                ).iloc[0][\"TimeGPT\"]\n",
    "            \n",
    "            train_predictions = [train_pred] * len(history)\n",
    "            \n",
    "            return {\n",
    "                'product_id': product_id,'winsorize': winsorize,'features': feature_set,\n",
    "                'metrics': calculate_metrics(y_true=y_val,y_pred=forecast[\"TimeGPT\"],\n",
    "                                             y_train=history[target_col],y_train_pred=train_predictions),\n",
    "                'method': 'simple','validation_predictions': forecast[\"TimeGPT\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"Simple forecast failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- Walk-Forward (≥36 obs) ---\n",
    "    print(f\"Using walk-forward ({len(X_train)} ≥ 36 obs)\")\n",
    "    predictions = []\n",
    "    history = X_train.copy()\n",
    "    history[target_col] = y_train.values\n",
    "    history = history.reset_index()\n",
    "    \n",
    "    # Get training predictions\n",
    "    try:\n",
    "        train_fit = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,h=1,\n",
    "                                           hist_exog_list=hist_exog if hist_exog else None)\n",
    "        train_predictions = [train_fit[\"TimeGPT\"].iloc[0]] * len(y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Training pred failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = None\n",
    "            if fut_exog:\n",
    "                X_future_step = X_val.iloc[[i]][fut_exog].copy()\n",
    "                if not X_future_step.isnull().all().all():  \n",
    "                    X_future_step.insert(0, 'month_year', X_val.index[i])\n",
    "                else:\n",
    "                    X_future_step = None  \n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,\n",
    "                                              h=1, X_df=X_future_step if fut_exog else None, # FUTURE features (macro)\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)  # HISTORICAL features (lags, moving averages)\n",
    "            \n",
    "            pred = forecast[\"TimeGPT\"].iloc[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            new_row = X_val.iloc[i].copy()\n",
    "            new_row['month_year'] = X_val.index[i]\n",
    "            new_row[target_col] = pred\n",
    "            history = pd.concat([history, pd.DataFrame([new_row])])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {'product_id': product_id,'winsorize': winsorize,'features': feature_set,'metrics': metrics,\n",
    "            'method': 'walkforward','validation_predictions': predictions}\n",
    "\n",
    "\n",
    "def find_best_timegpt_config(product_id, df_train, nixtla_client, lagged_df=None, target_col=\"Sales\"):\n",
    "\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    print(normal_feature_combinations)\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                result = choose_parameters_timegpt(\n",
    "                    product_id=product_id,df_train=data, feature_set=features, target_col=target_col,\n",
    "                    nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    result = choose_parameters_timegpt(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_timegpt_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,\n",
    "            nixtla_client=nixtla_client,target_col=\"Sales\")\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "\n",
    "all_results = []  # List to store all results\n",
    "best_configs = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_timegpt, results_timegpt = process_product_parallel(product_id)\n",
    "        \n",
    "        if best_config_timegpt is not None:\n",
    "            best_configs.append(best_config_timegpt)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_timegpt is not None:\n",
    "            all_results.extend(results_timegpt)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n",
    "\n",
    "df_all_results = convert_results_to_df(all_results) if all_results else pd.DataFrame()\n",
    "df_best_configs = pd.DataFrame(best_configs) if best_configs else pd.DataFrame()\n",
    "\n",
    "df_all_results.to_csv(\"timegpt_best_configs.csv\", index=False)\n",
    "df_best_configs.to_csv(\"timegpt_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    if not results:\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio se results estiver vazio\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Itera sobre cada dicionário dentro da lista de resultados\n",
    "    for result in results:\n",
    "        product_id = result.get('product_id', None)\n",
    "        winsorize = result.get('winsorize', None)\n",
    "        \n",
    "        # Verifica se 'features' existe e se não está vazio\n",
    "        features = ', '.join(result.get('features', [])) if result.get('features') else 'all'\n",
    "        \n",
    "        metrics = result.get('metrics', {})\n",
    "\n",
    "        validation_predictions = result.get('validation_predictions', [])\n",
    "\n",
    "        # Adiciona uma linha para cada previsão de validação\n",
    "        for pred in validation_predictions:\n",
    "            data.append({\n",
    "                'product_id': product_id,\n",
    "                'winsorize': winsorize,\n",
    "                'features': features,\n",
    "                'RMSE': metrics.get('RMSE', np.nan),\n",
    "                'MAPE': metrics.get('MAPE', np.nan),\n",
    "                'Overfit_Score': metrics.get('Overfit Score', np.nan),\n",
    "                'validation_prediction': pred\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timegpt = convert_results_to_df(best_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timegpt.to_csv('best_configs_timegpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Google's TimesFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load TimesFM model\n",
    "model = AutoModel.from_pretrained(\"google/timesfm\")\n",
    "\n",
    "# Load tokenizer (if applicable)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/timesfm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_timesfm(product_id, df_train, target_col='Sales', \n",
    "                              winsorize=False, feature_set=None, horizon=10):\n",
    "    \"\"\"\n",
    "    Function to prepare data, run TimesFM forecast, and evaluate performance.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    data = df_train[product_id].copy()\n",
    "    y = data[target_col]\n",
    "    X = data.drop(columns=[target_col])\n",
    "\n",
    "    # Select features if specified\n",
    "    if feature_set is not None:\n",
    "        X = X[feature_set]\n",
    "\n",
    "    # Time-based train/test split\n",
    "    X_train, X_val, y_train, y_val = time_series_train_test_split(X, y, test_size=horizon)\n",
    "\n",
    "    # Preprocessing\n",
    "    train, val = preprocessing_pipeline(X_train, X_val, test=None, outlier_treatment=winsorize)\n",
    "\n",
    "    # Scaling\n",
    "    def scale(data_to_fit, data, method=\"standard\"):\n",
    "        return scaler_function(\n",
    "            method,\n",
    "            fit_data=data_to_fit, \n",
    "            df=data,\n",
    "            numerical_columns=data_to_fit.columns)\n",
    "\n",
    "    train = scale(train, train)\n",
    "    val = scale(train, val)\n",
    "\n",
    "    # Memory reduction\n",
    "    train = reduce_memory_usage(train, train.columns)\n",
    "    val = reduce_memory_usage(val, val.columns)\n",
    "\n",
    "    # Select features\n",
    "    train = train[feature_set]\n",
    "    val = val[feature_set]\n",
    "\n",
    "    # ---- TimesFM Processing ----\n",
    "    # Convert to tensor format (assuming TimesFM expects PyTorch tensors)\n",
    "    timeseries_tensor = torch.tensor(train.values, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # Run prediction using TimesFM\n",
    "    with torch.no_grad():\n",
    "        forecast_output = model(timeseries_tensor)\n",
    "\n",
    "    # Extract predictions\n",
    "    y_pred = forecast_output.squeeze(0).numpy()\n",
    "\n",
    "    # Evaluate model\n",
    "    metrics = calculate_metrics(y_val, y_pred)\n",
    "\n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set if feature_set else 'all',\n",
    "        'metrics': metrics,\n",
    "        'predictions': y_pred}\n",
    "\n",
    "def find_best_timesfm_config(product_id, df_train, target_col='Sales'):\n",
    "    \"\"\"\n",
    "    Find the best TimesFM configuration by testing:\n",
    "    - With/without winsorization\n",
    "    - Different feature combinations\n",
    "    \"\"\"\n",
    "    data = df_train[product_id].copy()\n",
    "    available_features = [col for col in data.columns if col != target_col]\n",
    "\n",
    "    # Generate all possible feature combinations (max 3 features at a time)\n",
    "    feature_combinations = generate_feature_combinations(available_features, max_features=3)\n",
    "\n",
    "    # Test all configurations\n",
    "    results = []\n",
    "    for winsorize in [True, False]:\n",
    "        for features in feature_combinations:\n",
    "            print(f\"\\nTesting config - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "            result = choose_parameters_timesfm(\n",
    "                product_id=product_id,\n",
    "                df_train=df_train,\n",
    "                target_col=target_col,\n",
    "                winsorize=winsorize,\n",
    "                feature_set=features\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "    # Find best configuration (lowest RMSE)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_idx = results_df['metrics'].apply(lambda x: x['RMSE']).idxmin()\n",
    "    best_config = results[best_idx]\n",
    "\n",
    "    print(f\"\\nBest configuration for product {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- R²: {best_config['metrics']['R2']:.4f}\")\n",
    "\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store results\n",
    "comparison_results = []\n",
    "\n",
    "# List of all product IDs (assuming df_train has product_id as a column)\n",
    "all_product_ids = df_train.columns.levels[0]  # Adjust this if product_id is structured differently\n",
    "\n",
    "# Apply TimesFM model for each product\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"\\n🔄 Processing Product ID: {product_id} ...\")\n",
    "\n",
    "    best_config = find_best_timesfm_config(\n",
    "        product_id=product_id,\n",
    "        df_train=df_train,\n",
    "        target_col='Sales')\n",
    "\n",
    "    # Save results in the comparison table\n",
    "    comparison_results.append({\n",
    "        'Product_ID': product_id,\n",
    "        'Model': 'TimesFM',\n",
    "        'Winsorize': best_config['winsorize'],\n",
    "        'Features': best_config['features'],\n",
    "        'RMSE': best_config['metrics']['RMSE'],\n",
    "        'R²': best_config['metrics']['R2']})\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "# Save as CSV for further analysis\n",
    "comparison_df.to_csv(\"timesfm_results.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n📊 TimesFM Model Performance Across Products:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_xgboost(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                            param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized XGBoost forecasting with proper walk-forward validation\"\"\"\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize)\n",
    "        # Verify temporal ordering\n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Optimized Parameter Grid for Small Data ---\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [2, 3],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.9, 1.0],\n",
    "            'colsample_bytree': [0.9, 1.0],\n",
    "            'min_child_weight': [1, 3]}\n",
    "\n",
    "    # --- Efficient Model Setup ---\n",
    "    base_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',random_state=42,n_jobs=1,tree_method='hist',  # Faster training\n",
    "        enable_categorical=False,gamma=0,reg_alpha=0,reg_lambda=1)\n",
    "\n",
    "    # --- Strict Temporal Validation ---\n",
    "    test_fold = np.array([-1] * len(X_train) + [0] * len(X_val))\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "    # --- Focused Grid Search ---\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,param_grid=param_grid,scoring='neg_root_mean_squared_error',cv=ps,n_jobs=n_jobs,verbose=1,refit=True)\n",
    "\n",
    "    # --- Training with Validation ---\n",
    "    X_combined = pd.concat([X_train, X_val])\n",
    "    y_combined = pd.concat([y_train, y_val])\n",
    "    \n",
    "    print(f\"Starting grid search for {product_id}...\")\n",
    "    try:\n",
    "        grid_search.fit(X_combined, y_combined)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Robust Walk-forward Validation ---\n",
    "    predictions = []\n",
    "    print(f\"Using walk-forward (n_train={len(X_train)}, horizon={horizon})\")\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            # Create expanding window\n",
    "            window_X = pd.concat([X_train, X_val.iloc[:i]])\n",
    "            window_y = pd.concat([y_train, y_val.iloc[:i]])\n",
    "            \n",
    "            # Correct XGBoost 2.1.3+ fitting\n",
    "            best_model.fit(window_X, window_y, eval_set=[(X_val.iloc[[i]], y_val.iloc[[i]])], verbose=0)\n",
    "    \n",
    "            pred = best_model.predict(X_val.iloc[[i]]).item()\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)} - using fallback\")\n",
    "            fallback = (predictions[-1] if len(predictions) > 0 else \n",
    "                       y_train.iloc[-1] if len(y_train) > 0 else np.nan)\n",
    "            predictions.append(fallback)\n",
    "\n",
    "    # --- Training Metrics ---\n",
    "    try:\n",
    "        train_predictions = best_model.predict(X_train).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Training predictions failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    # --- Comprehensive Metrics ---\n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_predictions': predictions,\n",
    "        'feature_importances': dict(zip(feature_set, best_model.feature_importances_))}\n",
    "\n",
    "\n",
    "def find_best_xgboost_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_xgboost(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_xgboost(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_xgboost_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_param_grids= ver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_ids ={'P14'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "all_results_xgboost = []  # List to store all results\n",
    "best_configs_xgboost = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_xgboost, results_xgboost = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_xgboost is not None:\n",
    "            best_configs_xgboost.append(best_config_xgboost)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_xgboost is not None:\n",
    "            all_results_xgboost.extend(results_xgboost)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_results_xgboost = convert_results_to_df(all_results_xgboost) if all_results_xgboost else pd.DataFrame()\n",
    "df_best_configs_xgboost = pd.DataFrame(best_configs_xgboost) if best_configs_xgboost else pd.DataFrame()\n",
    "\n",
    "df_all_results_xgboost.to_csv(\"xgboost_results.csv\", index=False)\n",
    "df_best_configs_xgboost.to_csv(\"xgboost_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_tcn(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                         param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized TCN forecasting with walk-forward validation\"\"\"\n",
    "    \n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize, scaling=True)\n",
    "        \n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Reshape data for TCN input (samples, timesteps, features)\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_val = y_val.values.reshape(-1, 1)\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'nb_filters': [16, 32],\n",
    "            'kernel_size': [2, 3],\n",
    "            'dilations': [[1, 2, 4], [1, 2, 4, 8]],\n",
    "            'dropout_rate': [0.1, 0.2],\n",
    "            'return_sequences': [False]  # single-step prediction\n",
    "}\n",
    "    \n",
    "    def build_tcn(nb_filters=32, kernel_size=2, dilations=[1, 2, 4], \n",
    "                 dropout_rate=0.1, return_sequences=False):\n",
    "        inputs = Input(shape=(1, X_train.shape[2]))\n",
    "        x = TCN(nb_filters=nb_filters, kernel_size=kernel_size, \n",
    "               dilations=dilations, dropout_rate=dropout_rate,\n",
    "               return_sequences=return_sequences)(inputs)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        return model\n",
    "    \n",
    "    best_params = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for filters in param_grid['nb_filters']:\n",
    "        for kernel in param_grid['kernel_size']:\n",
    "            for dilations in param_grid['dilations']:\n",
    "                for dropout in param_grid['dropout_rate']:\n",
    "                    for return_seq in param_grid['return_sequences']:\n",
    "                        model = build_tcn(filters, kernel, dilations, dropout, return_seq)\n",
    "                        try:\n",
    "                            model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "                            \n",
    "                            # Walk-forward validation (predict one step at a time)\n",
    "                            predictions = []\n",
    "                            history_X = X_train.copy()\n",
    "                            history_y = y_train.copy()\n",
    "                            \n",
    "                            for i in range(len(X_val)):\n",
    "                                # Predict next step\n",
    "                                X_next = X_val[i].reshape(1, 1, -1)\n",
    "                                pred = model.predict(X_next, verbose=0).flatten()[0]\n",
    "                                predictions.append(pred)\n",
    "                                \n",
    "                            # Calculate RMSE\n",
    "                            if len(y_val) == len(predictions):\n",
    "                                rmse = np.sqrt(np.mean((y_val.flatten() - predictions) ** 2))\n",
    "                            else:\n",
    "                                print(f\"Shape mismatch: y_val ({y_val.shape}), preds ({len(predictions)})\")\n",
    "                                continue\n",
    "                            \n",
    "                            if rmse < best_rmse:\n",
    "                                best_rmse = rmse\n",
    "                                best_params = {\n",
    "                                    'nb_filters': filters,\n",
    "                                    'kernel_size': kernel,\n",
    "                                    'dilations': dilations,\n",
    "                                    'dropout_rate': dropout,\n",
    "                                    'return_sequences': return_seq\n",
    "                                }\n",
    "                        except Exception as e:\n",
    "                            print(f\"Hyperparameter set failed: {str(e)}\")\n",
    "\n",
    "    if best_params is None:\n",
    "        print(f\"❌ No valid models for {product_id}\")\n",
    "        return None\n",
    "\n",
    "    # Final model training and walk-forward prediction\n",
    "    final_model = build_tcn(**best_params)\n",
    "    final_model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "    predictions = []\n",
    "\n",
    "    # FIXED: Create DataFrame with both features and target\n",
    "    history_data = np.column_stack([X_train[:, 0, :], y_train.flatten()])\n",
    "    history = pd.DataFrame(history_data, columns=feature_set + [target_col])\n",
    "    history.index = range(len(history))\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = X_val[i].reshape(1, 1, -1)\n",
    "            pred = final_model.predict(X_future_step, verbose=0).flatten()[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update training data\n",
    "            new_row = X_val[i, 0, :].tolist()\n",
    "            new_row.append(pred)\n",
    "            new_df = pd.DataFrame([new_row], columns=feature_set + [target_col])\n",
    "            history = pd.concat([history, new_df]).reset_index(drop=True)\n",
    "            \n",
    "            # Retrain on expanded window\n",
    "            rolling_window = 36 if len(history) > 36 else len(history)\n",
    "            train_subset = history.iloc[-rolling_window:]\n",
    "            \n",
    "            X_train_update = train_subset[feature_set].values.reshape(-1, 1, len(feature_set))\n",
    "            y_train_update = train_subset[target_col].values.reshape(-1, 1)\n",
    "            \n",
    "            final_model.fit(X_train_update, y_train_update, epochs=5, batch_size=16, verbose=0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(predictions[-1] if predictions else y_train[-1][0])\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val.flatten(), y_pred=predictions, y_train=y_train.flatten())\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': best_params,\n",
    "        'validation_predictions': predictions\n",
    "    }\n",
    "\n",
    "def find_best_tcn_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_tcn(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_tcn(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "        best_config, results = find_best_tcn_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_tcn = []  # List to store all results\n",
    "best_configs_tcn = []  # List to store best configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P1',\n",
       " 'P11',\n",
       " 'P12',\n",
       " 'P13',\n",
       " 'P14',\n",
       " 'P16',\n",
       " 'P20',\n",
       " 'P3',\n",
       " 'P36',\n",
       " 'P4',\n",
       " 'P5',\n",
       " 'P6',\n",
       " 'P8',\n",
       " 'P9',\n",
       " 'Sales_CPI'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_product_ids = set(list(product_dfs.keys()))\n",
    "all_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_product_ids = set(list(product_dfs.keys())[:1])\n",
    "\n",
    "# for product_id in all_product_ids:\n",
    "#     print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "#     try:\n",
    "#         best_config_tcn, results_tcn = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "#         if best_config_tcn is not None:\n",
    "#             best_configs_tcn.append(best_config_tcn)  # Store best config\n",
    "#         else:\n",
    "#             print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "#         if results_tcn is not None:\n",
    "#             all_results_tcn.extend(results_tcn)  # Store results\n",
    "#         else:\n",
    "#             print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product: P6\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "WARNING:tensorflow:From c:\\Users\\beatr\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\beatr\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success - RMSE: 0.87\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27276_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 1.08\n",
      "\n",
      "Testing - Winsorize: True, Features: ['RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 0.69\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27840_org', 'PRO27276_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 0.91\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27840_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 1.22\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27276_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 1.47\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -150.48, Upper = 984286.42\n",
      "✅ Success - RMSE: 0.68\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27840_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 0.92\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27276_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 0.94\n",
      "\n",
      "Testing - Winsorize: False, Features: ['RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 1.04\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27840_org', 'PRO27276_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 0.78\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27840_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 1.81\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27276_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 0.80\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 0.60\n",
      "\n",
      "✅ Best configuration for P6:\n",
      "- Winsorize: False\n",
      "- Features: ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org']\n",
      "- Best params: {'nb_filters': 16, 'kernel_size': 2, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}\n",
      "- RMSE: 0.60\n",
      "- Method: walkforward\n"
     ]
    }
   ],
   "source": [
    "product_ids = {'P6'}\n",
    "\n",
    "for product_id in product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_tcn, results_tcn = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_tcn is not None:\n",
    "            best_configs_tcn.append(best_config_tcn)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_tcn is not None:\n",
    "            all_results_tcn.extend(results_tcn)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'product_id': 'P6', 'winsorize': True, 'features': ['PRO27840_org'], 'metrics': {'RMSE': 0.8696939758457538, 'MAPE': 67.12610168662755, 'R2': -3.5894464825439396}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 2, 'dilations': [1, 2, 4], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [0.003561318, -0.19293699, -0.14849229, -0.16153687, -0.35302275, -0.27903166, -0.33987057]}, {'product_id': 'P6', 'winsorize': True, 'features': ['PRO27276_org'], 'metrics': {'RMSE': 1.0797312949623872, 'MAPE': 106.78444967713199, 'R2': -6.073897531347031}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.08723864, 0.121071674, -0.13749166, -0.25745684, -0.17254807, 1.0251366, -0.27287504]}, {'product_id': 'P6', 'winsorize': True, 'features': ['RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 0.68536182433094, 'MAPE': 73.38071097180048, 'R2': -1.8501466470060688}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 3, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [-0.6723993, -0.8840208, -0.5352734, -0.456475, -0.69018906, -1.4782369, -1.3175646]}, {'product_id': 'P6', 'winsorize': True, 'features': ['PRO27840_org', 'PRO27276_org'], 'metrics': {'RMSE': 0.9073997521096909, 'MAPE': 78.32188077265288, 'R2': -3.996026103407324}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 2, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [-0.56580895, -0.26857042, -0.15347596, -0.52618897, -0.7467756, 0.91443694, -0.67270464]}, {'product_id': 'P6', 'winsorize': True, 'features': ['PRO27840_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 1.2175036642611612, 'MAPE': 132.76706099556966, 'R2': -7.994311826507824}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 3, 'dilations': [1, 2, 4], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [0.045550097, -0.119880475, 0.090324715, 0.03475918, 0.31911573, 0.76735395, 0.43684223]}, {'product_id': 'P6', 'winsorize': True, 'features': ['PRO27276_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 1.4748792401295578, 'MAPE': 156.2582058326318, 'R2': -12.198978202224431}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 2, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [0.25094554, 0.6850795, -0.85123426, -1.018773, 0.5239646, 2.4155533, -0.8193068]}, {'product_id': 'P6', 'winsorize': True, 'features': ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 0.6757861395103592, 'MAPE': 84.48676114704945, 'R2': -1.77106010858388}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.919189, -0.13732535, -0.63464487, -0.93093604, -1.2467872, 0.3895409, -1.0937203]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27840_org'], 'metrics': {'RMSE': 0.922004671747753, 'MAPE': 88.04635900660206, 'R2': -4.197865562027666}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [0.079994775, 0.09745578, -0.03746833, -0.30626762, -0.104490176, -0.06524357, -0.35060298]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27276_org'], 'metrics': {'RMSE': 0.9365311046062008, 'MAPE': 97.99597753916375, 'R2': -4.362943374870463}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.20034161, 0.19319855, -0.24944334, -0.3492625, -0.06516671, 0.6134256, -0.41820386]}, {'product_id': 'P6', 'winsorize': False, 'features': ['RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 1.0421649661927468, 'MAPE': 103.83832173143229, 'R2': -5.6409733711329695}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 3, 'dilations': [1, 2, 4], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [0.11158751, 0.15100402, 0.102472074, -0.006622188, 0.034023426, -0.008187719, -0.17453733]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27840_org', 'PRO27276_org'], 'metrics': {'RMSE': 0.7847009755837182, 'MAPE': 70.82971221757786, 'R2': -2.7650191073402812}, 'method': 'walkforward', 'best_params': {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.7075303, -0.1777032, -0.21979769, -0.53215444, -0.6010353, 0.55656713, -0.47255933]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27840_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 1.8147847316144026, 'MAPE': 197.73381480929507, 'R2': -19.137661340983954}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 3, 'dilations': [1, 2, 4], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [-0.14512466, -0.063925624, 0.42862058, 0.8606015, 0.46421617, 1.6494534, 1.8905147]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27276_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 0.8003608609954594, 'MAPE': 83.90284890961145, 'R2': -2.9167917824472664}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 2, 'dilations': [1, 2, 4], 'dropout_rate': 0.2, 'return_sequences': False}, 'validation_predictions': [-0.33763894, -0.0832357, -0.90946084, -1.0925305, -0.36799723, 0.464701, -1.9192533]}, {'product_id': 'P6', 'winsorize': False, 'features': ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 0.5974510689768012, 'MAPE': 49.560644898547785, 'R2': -1.1825463955799926}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 2, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.7536576, -0.6319773, -0.3721999, -0.5463982, -0.6691273, -0.15547836, -0.7051902]}]\n",
      "[{'product_id': 'P6', 'winsorize': False, 'features': ['PRO27840_org', 'PRO27276_org', 'RohCRUDE_PETRO1000_org'], 'metrics': {'RMSE': 0.5974510689768012, 'MAPE': 49.560644898547785, 'R2': -1.1825463955799926}, 'method': 'walkforward', 'best_params': {'nb_filters': 16, 'kernel_size': 2, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}, 'validation_predictions': [-0.7536576, -0.6319773, -0.3721999, -0.5463982, -0.6691273, -0.15547836, -0.7051902]}]\n"
     ]
    }
   ],
   "source": [
    "print(all_results_tcn)\n",
    "print(best_configs_tcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    if not results:\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio se results estiver vazio\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Itera sobre cada dicionário dentro da lista de resultados\n",
    "    for result in results:\n",
    "        product_id = result.get('product_id', None)\n",
    "        winsorize = result.get('winsorize', None)\n",
    "        \n",
    "        # Verifica se 'features' existe e se não está vazio\n",
    "        features = ', '.join(result.get('features', [])) if result.get('features') else 'all'\n",
    "        \n",
    "        metrics = result.get('metrics', {})\n",
    "\n",
    "        validation_predictions = result.get('validation_predictions', [])\n",
    "\n",
    "        # Adiciona uma linha para cada previsão de validação\n",
    "        for pred in validation_predictions:\n",
    "            data.append({\n",
    "                'product_id': product_id,\n",
    "                'winsorize': winsorize,\n",
    "                'features': features,\n",
    "                'RMSE': metrics.get('RMSE', np.nan),\n",
    "                'MAPE': metrics.get('MAPE', np.nan),\n",
    "                'Overfit_Score': metrics.get('Overfit Score', np.nan),\n",
    "                'validation_prediction': pred\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed! Results saved.\n"
     ]
    }
   ],
   "source": [
    "df_all_results_tcn = convert_results_to_df(all_results_tcn) if all_results_tcn else pd.DataFrame()\n",
    "df_best_configs_tcn = pd.DataFrame(best_configs_tcn) if best_configs_tcn else pd.DataFrame()\n",
    "\n",
    "df_all_results_tcn.to_csv(\"tcn_results.csv\", index=False)\n",
    "df_best_configs_tcn.to_csv(\"tcn_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Ensure time-series data is sorted and complete\n",
    "df_p1_lags = df_p1_lags.sort_index()\n",
    "df_p1_lags = df_p1_lags.asfreq('MS').interpolate()  # Fill missing months\n",
    "\n",
    "# Fit Auto ARIMA Model (Using all data initially)\n",
    "auto_model = auto_arima(\n",
    "    df_p1_lags['P1'].dropna(),  \n",
    "    exogenous=df_p1_lags[['P1_lag_1', 'P1_lag_2', 'P1_lag_3']].dropna(),  \n",
    "    seasonal=False, \n",
    "    trace=True,\n",
    "    stepwise=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore',\n",
    "    max_p=5, \n",
    "    max_q=5,\n",
    "    d=None,  # Let ADF test decide differencing\n",
    "    test='adf',  \n",
    "    scoring='mse'\n",
    ")\n",
    "print(auto_model.summary())\n",
    "\n",
    "# Walk-Forward Forecasting with Expanding Window\n",
    "def walk_forward_forecast(model, df, horizon=10, initial_train_size=33):\n",
    "    \"\"\"\n",
    "    Walk-forward validation using an expanding window approach.\n",
    "    Predicts 'horizon' steps ahead at each iteration.\n",
    "    \"\"\"\n",
    "    predictions, true_values, lower_bounds, upper_bounds = [], [], [], []\n",
    "\n",
    "    for i in range(len(df) - initial_train_size - horizon + 1):\n",
    "        train = df.iloc[: i + initial_train_size]  # Expanding training set\n",
    "        test = df.iloc[i + initial_train_size : i + initial_train_size + horizon]\n",
    "\n",
    "        model.fit(train['P1'])\n",
    "        pred, conf_int = model.predict(n_periods=len(test), return_conf_int=True)\n",
    "\n",
    "        if len(pred) == len(test):  # Ensure consistent shape\n",
    "            predictions.append(pred)\n",
    "            true_values.append(test['P1'].values)\n",
    "            lower_bounds.append(conf_int[:, 0])\n",
    "            upper_bounds.append(conf_int[:, 1])\n",
    "\n",
    "    return np.array(true_values), np.array(predictions), np.array(lower_bounds), np.array(upper_bounds)\n",
    "\n",
    "# Perform Walk-Forward Forecasting\n",
    "true_values, predictions, lower_bounds, upper_bounds = walk_forward_forecast(auto_model, df_p1_lags, horizon=10)\n",
    "\n",
    "# Compute Errors\n",
    "def print_error(true, pred):\n",
    "    RMSE = mean_squared_error(true.flatten(), pred.flatten(), squared=False)\n",
    "    R2 = r2_score(true.flatten(), pred.flatten())\n",
    "    MAPE = mean_absolute_percentage_error(true.flatten(), pred.flatten())\n",
    "    print(\"RMSE:\", RMSE)\n",
    "    print(\"MAPE:\", round(MAPE * 100, 2), \"%\")\n",
    "    print(\"R2:\", R2)\n",
    "\n",
    "print_error(true_values, predictions)\n",
    "\n",
    "# ✅ Plot Full Dataset + Predictions + Confidence Intervals\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the real data (Training + Test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_p1_lags.index,\n",
    "    y=df_p1_lags['P1'],\n",
    "    mode='lines',\n",
    "    name='Real Data',\n",
    "    line=dict(color='black', width=2)\n",
    "))\n",
    "\n",
    "# Add predictions over time\n",
    "for i in range(len(predictions)):\n",
    "    forecast_index = df_p1_lags.index[i + 33 : i + 33 + 10]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_index, \n",
    "        y=predictions[i],\n",
    "        mode='lines',\n",
    "        name=f'Prediction {i+1}',\n",
    "        line=dict(dash='dash', width=1, color='blue')\n",
    "    ))\n",
    "\n",
    "    # Add confidence interval shading\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(forecast_index) + list(forecast_index[::-1]),\n",
    "        y=list(upper_bounds[i]) + list(lower_bounds[i][::-1]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(0,255,0,0.2)',\n",
    "        line=dict(color='rgba(255,255,255,0)'),\n",
    "        name='Confidence Interval',\n",
    "        showlegend=(i == 0)\n",
    "    ))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Walk-Forward Forecasting (10-month prediction horizon)\",\n",
    "    xaxis_title=\"Months\",\n",
    "    yaxis_title=\"P1 Values\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def choose_parameters_arima(product_id, df_train, target_col='Sales', winsorize=False, feature_set=None, horizon=10, seasonal=False, stationary=False):\n",
    "    \"\"\"\n",
    "    Optimized function to prepare data, run auto_arima, and evaluate performance.\n",
    "    \"\"\"\n",
    "    # Common Preprocessing\n",
    "    X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, product_id, target_col, feature_set, horizon, winsorize)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_train, y_val = y_train.values, y_val.values\n",
    "    X_train, X_val = X_train.values if not X_train.empty else None, X_val.values if not X_val.empty else None\n",
    "\n",
    "    # Auto ARIMA Model\n",
    "    model = auto_arima(y=y_train, X=X_train, stationary=stationary, seasonal=seasonal, suppress_warnings=True, stepwise=True, error_action='ignore', trace=False)\n",
    "\n",
    "    # Walk-forward Validation\n",
    "    predictions = []\n",
    "    for i in range(len(y_val)):\n",
    "        new_x = X_val[i].reshape(1, -1) if X_val is not None else None\n",
    "        model.update([y_val[i]], X=new_x)\n",
    "        pred = model.predict(n_periods=1, X=new_x)[0] if new_x is not None else model.predict(n_periods=1)[0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Evaluate Model\n",
    "    metrics = calculate_metrics(y_val, predictions)\n",
    "\n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set if feature_set else 'all',\n",
    "        'best_params': {'order': model.order, 'seasonal_order': model.seasonal_order if seasonal else None, 'aic': model.aic(), 'bic': model.bic()},\n",
    "        'metrics': metrics,\n",
    "        'model': model}\n",
    "\n",
    "\n",
    "def find_best_arima_config(product_id, df_train, target_col='Sales'):\n",
    "    \"\"\"\n",
    "    Find optimal ARIMA configuration matching original testing approach\n",
    "    \"\"\"\n",
    "    data = df_train[product_id].copy()\n",
    "    available_features = [col for col in data.columns if col != target_col]\n",
    "    \n",
    "    # Generate feature combinations (max 3 for ARIMAX stability)\n",
    "    feature_combinations = generate_feature_combinations(available_features, max_features=3)\n",
    "    \n",
    "    # Test all configurations - consistent with original approach\n",
    "    results = []\n",
    "    for winsorize in [True, False]:\n",
    "        for features in feature_combinations:\n",
    "            for seasonal in [False, True]:  # Test both seasonal and non-seasonal\n",
    "                print(f\"\\nTesting config - Winsorize: {winsorize}, Features: {features}, Seasonal: {seasonal}\")\n",
    "                \n",
    "                result = choose_parameters_arima(\n",
    "                    product_id=product_id,\n",
    "                    df_train=df_train,\n",
    "                    target_col=target_col,\n",
    "                    winsorize=winsorize,\n",
    "                    feature_set=features,\n",
    "                    seasonal=seasonal\n",
    "                )\n",
    "                results.append(result)\n",
    "    \n",
    "    # Find best configuration (lowest RMSE)\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "\n",
    "    print(f\"\\n✅ Best configuration for product {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Seasonal: {best_config['best_params']['seasonal_order'] is not None}\")\n",
    "    print(f\"- Order (p,d,q): {best_config['best_params']['order']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- R²: {best_config['metrics']['R2']:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def process_product_arima(product_id):\n",
    "    \"\"\"\n",
    "    Consistent wrapper function matching original format\n",
    "    \"\"\"\n",
    "    best_config = find_best_arima_config(\n",
    "        product_id=product_id,\n",
    "        df_train=df_train,\n",
    "        target_col='Sales'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'Product_ID': product_id,\n",
    "        'Model': 'ARIMA',\n",
    "        'Winsorize': best_config['winsorize'],\n",
    "        'Features': best_config['features'],\n",
    "        'Order': best_config['best_params']['order'],\n",
    "        'Seasonal_Order': best_config['best_params']['seasonal_order'],\n",
    "        'RMSE': best_config['metrics']['RMSE'],\n",
    "        'R²': best_config['metrics']['R2']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "def prepare_neuralprophet_data(df, target_col='Sales', features=None):\n",
    "    \"\"\"Convert dataframe to NeuralProphet format\"\"\"\n",
    "    df = df.reset_index()\n",
    "    if 'Date' not in df.columns and 'date' not in df.columns:\n",
    "        df['ds'] = pd.date_range(start='2020-01-01', periods=len(df), freq='D')\n",
    "    elif 'Date' in df.columns:\n",
    "        df['ds'] = pd.to_datetime(df['Date'])\n",
    "    elif 'date' in df.columns:\n",
    "        df['ds'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    df['y'] = df[target_col]\n",
    "    \n",
    "    if features:\n",
    "        for feature in features:\n",
    "            if feature not in ['ds', 'y']:\n",
    "                df[feature] = df[feature]\n",
    "    \n",
    "    return df[['ds', 'y'] + (features if features else [])]\n",
    "\n",
    "def choose_parameters_neuralprophet(product_id, df_train, target_col='Sales', winsorize=False, feature_set=None, horizon=10, n_lags=10, n_forecasts=10, epochs=50):\n",
    "    \"\"\"\n",
    "    Optimized NeuralProphet function with common preprocessing.\n",
    "    \"\"\"\n",
    "    # Common Preprocessing\n",
    "    X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, product_id, target_col, feature_set, horizon, winsorize)\n",
    "\n",
    "    # Convert to NeuralProphet format\n",
    "    train_df = prepare_neuralprophet_data(pd.concat([y_train, X_train], axis=1), target_col, feature_set)\n",
    "    val_df = prepare_neuralprophet_data(pd.concat([y_val, X_val], axis=1), target_col, feature_set)\n",
    "\n",
    "    # Initialize NeuralProphet\n",
    "    model = NeuralProphet(n_lags=n_lags, n_forecasts=n_forecasts, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, num_hidden_layers=2, d_hidden=32, learning_rate=0.01, epochs=epochs)\n",
    "\n",
    "    # Add exogenous variables if present\n",
    "    if feature_set:\n",
    "        for feature in feature_set:\n",
    "            model = model.add_lagged_regressor(feature)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(train_df, freq='D', validation_df=val_df)\n",
    "\n",
    "    # Make Predictions\n",
    "    predictions = []\n",
    "    for i in range(len(y_val)):\n",
    "        new_data = train_df.append(val_df.iloc[:i])  # Expand training set with new validation points\n",
    "        model.fit(new_data, freq='D')  # Retrain only when necessary\n",
    "\n",
    "        future = model.make_future_dataframe(new_data, periods=1)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        predictions.append(forecast['yhat1'].iloc[-1])  # Extract last prediction\n",
    "\n",
    "    # Evaluate Model\n",
    "    metrics = calculate_metrics(val_df['y'].values, predictions)\n",
    "\n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set if feature_set else 'all',\n",
    "        'best_params': {'n_lags': n_lags, 'n_forecasts': n_forecasts, 'epochs': epochs},\n",
    "        'metrics': metrics,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "def find_best_neuralprophet_config(product_id, df_train, target_col='Sales'):\n",
    "    \"\"\"\n",
    "    Find optimal NeuralProphet configuration matching original testing approach\n",
    "    \"\"\"\n",
    "    data = df_train[product_id].copy()\n",
    "    available_features = [col for col in data.columns if col != target_col]\n",
    "    \n",
    "    # Generate feature combinations\n",
    "    feature_combinations = generate_feature_combinations(available_features, max_features=3)\n",
    "    \n",
    "    # Test configurations\n",
    "    results = []\n",
    "    for winsorize in [True, False]:\n",
    "        for features in feature_combinations:\n",
    "            print(f\"\\nTesting config - Winsorize: {winsorize}, Features: {features}\")\n",
    "            \n",
    "            result = choose_parameters_neuralprophet(\n",
    "                product_id=product_id,\n",
    "                df_train=df_train,\n",
    "                target_col=target_col,\n",
    "                winsorize=winsorize,\n",
    "                feature_set=features\n",
    "            )\n",
    "            results.append(result)\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "\n",
    "    print(f\"\\n✅ Best NeuralProphet configuration for product {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- R²: {best_config['metrics']['R2']:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def process_product_neuralprophet(product_id):\n",
    "    \"\"\"\n",
    "    Consistent wrapper function matching original format\n",
    "    \"\"\"\n",
    "    best_config = find_best_neuralprophet_config(\n",
    "        product_id=product_id,\n",
    "        df_train=df_train,\n",
    "        target_col='Sales'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'Product_ID': product_id,\n",
    "        'Model': 'NeuralProphet',\n",
    "        'Winsorize': best_config['winsorize'],\n",
    "        'Features': best_config['features'],\n",
    "        'Parameters': best_config['best_params'],\n",
    "        'RMSE': best_config['metrics']['RMSE'],\n",
    "        'R²': best_config['metrics']['R2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for he Macro Features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to classify variables\n",
    "def classify_variable(series):\n",
    "    \"\"\"Classifies variable based on normality and stationarity tests.\"\"\"\n",
    "    \n",
    "    # Remove NaN values for testing\n",
    "    clean_series = series.dropna()\n",
    "\n",
    "    # Check for normality\n",
    "    if len(clean_series) > 3:\n",
    "        stat, p_value = shapiro(clean_series)\n",
    "        is_normal = p_value > 0.05  # p-value > 0.05 means normal\n",
    "    else:\n",
    "        is_normal = False  # Not enough data to test normality\n",
    "\n",
    "    # Check for stationarity\n",
    "    if len(clean_series) > 3:\n",
    "        adf_stat, adf_p_value, _, _, _, _ = adfuller(clean_series)\n",
    "        is_stationary = adf_p_value < 0.05  # p-value < 0.05 means stationary\n",
    "    else:\n",
    "        is_stationary = False  # Not enough data\n",
    "\n",
    "    return is_normal, is_stationary\n",
    "\n",
    "# Function to automatically fill missing values\n",
    "def auto_impute_missing_values(df_train, df_test):\n",
    "    \"\"\"Automatically selects the best imputation method for each missing variable.\"\"\"\n",
    "    \n",
    "    # Identify missing columns in test set\n",
    "    missing_columns = df_test.columns[df_test.isnull().any()]\n",
    "    \n",
    "    # Iterate through missing columns\n",
    "    for col in missing_columns:\n",
    "        print(f\"Processing: {col}\")\n",
    "\n",
    "        series = df_train[col]  # Use train data for imputation\n",
    "        is_normal, is_stationary = classify_variable(series)\n",
    "\n",
    "        if is_normal:\n",
    "            # Case 1: Normally distributed → Sample from normal distribution\n",
    "            print(f\" - {col} is normal → Using Mean & Std Sampling\")\n",
    "            mean_value, std_value = series.mean(), series.std()\n",
    "            num_missing = df_test[col].isnull().sum()\n",
    "            predictions = norm.rvs(loc=mean_value, scale=std_value, size=num_missing)\n",
    "        \n",
    "        elif is_stationary:\n",
    "            # Case 2: Stationary but non-normal → Simple Exponential Smoothing\n",
    "            print(f\" - {col} is stationary → Using Simple Exponential Smoothing\")\n",
    "            model = SimpleExpSmoothing(series.dropna()).fit()\n",
    "            predictions = model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        elif not is_stationary:\n",
    "            # Case 3: Non-Stationary → ARIMA\n",
    "            print(f\" - {col} is non-stationary → Using ARIMA\")\n",
    "            model = ARIMA(series.dropna(), order=(1, 1, 1))  # (p,d,q) chosen based on domain knowledge\n",
    "            fitted_model = model.fit()\n",
    "            predictions = fitted_model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        else:\n",
    "            # Case 4: If nothing works → Use XGBoost Regression\n",
    "            print(f\" - {col} is complex → Using XGBoost Regression\")\n",
    "            train_data = df_train.dropna(subset=[col])  # Drop missing values for training\n",
    "            X_train = train_data.drop(columns=[col])  # Exclude target column\n",
    "            y_train = train_data[col]  # Target column\n",
    "\n",
    "            X_test = df_test.loc[df_test[col].isnull(), X_train.columns]  # Only missing values\n",
    "\n",
    "            model = XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "        # Assign predictions\n",
    "        missing_indexes = df_test[df_test[col].isnull()].index\n",
    "        df_test.loc[missing_indexes, col] = predictions\n",
    "\n",
    "    return df_test\n",
    "\n",
    "# Example usage\n",
    "df_train = remerged_data[1]  # Use remerged train data\n",
    "df_test = test_1.copy()  # Copy test set\n",
    "\n",
    "# Apply automatic imputation\n",
    "df_test_filled = auto_impute_missing_values(df_train, df_test)\n",
    "\n",
    "# Check results\n",
    "print(df_test_filled.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
