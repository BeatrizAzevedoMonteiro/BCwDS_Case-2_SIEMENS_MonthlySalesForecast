{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install prophet == 1.1.6\n",
    "# pip install keras == 3.9.0 tensorflow == 2.19.0\n",
    "# pip install neuralprophet == 0.8.0\n",
    "# pytorch-forecasting-1.3.0\n",
    "#pip install nixtla == 0.6.6\n",
    "#pip install pmdarima == 2.0.4\n",
    "#pip install scikeras == 0.13.0\n",
    "#pip install keras-tcn == 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n",
      "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, TimeSeriesSplit\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#timesgpt\n",
    "from nixtla import NixtlaClient\n",
    "#xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "#lstm\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tcn import TCN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#Check if Y is stationary\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Modeling Pipeline (Feature selection, Scaling, Model testing)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from pytorch_forecasting.data.timeseries import TimeSeriesDataSet\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)       # Set display width\n",
    "pd.set_option('display.max_colwidth', 100) # Show full feature lists\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # 4 decimal places\n",
    "\n",
    "# If you want to force standard notation (no scientific):\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x if abs(x) > 1e-4 else '%.4e' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded P1 from product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from product_dfs_folder\\P13.pkl\n",
      "Loaded P14 from product_dfs_folder\\P14.pkl\n",
      "Loaded P16 from product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from product_dfs_folder\\P5.pkl\n",
      "Loaded P6 from product_dfs_folder\\P6.pkl\n",
      "Loaded P8 from product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from product_dfs_folder\\P9.pkl\n",
      "Loaded Sales_CPI from product_dfs_folder\\Sales_CPI.pkl\n",
      "Loaded P1 from lagged_product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from lagged_product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from lagged_product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from lagged_product_dfs_folder\\P13.pkl\n",
      "Loaded P16 from lagged_product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from lagged_product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from lagged_product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from lagged_product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from lagged_product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from lagged_product_dfs_folder\\P5.pkl\n",
      "Loaded P8 from lagged_product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from lagged_product_dfs_folder\\P9.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_dfs_from_folder(folder_path):\n",
    "    \"\"\"Loads DataFrames from files in a specified folder and returns a dictionary.\"\"\"\n",
    "    dfs = {}\n",
    "    # List all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pkl\"):\n",
    "            key = file_name.replace(\".pkl\", \"\")  # Extract key from the file name\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the dataframe from the pickle file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                dfs[key] = pickle.load(f)\n",
    "            print(f\"Loaded {key} from {file_path}\")\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# Load both product_dfs and lagged_product_dfs from their respective folders\n",
    "product_dfs = load_dfs_from_folder(\"product_dfs_folder\")\n",
    "lagged_product_dfs = load_dfs_from_folder(\"lagged_product_dfs_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_id in product_dfs.keys():\n",
    "    product_dfs[product_id] = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "\n",
    "for product_id in lagged_product_dfs.keys():\n",
    "    lagged_product_dfs[product_id] = lagged_product_dfs[product_id].rename(columns={product_id: \"Sales\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>MAB_ELE_SHP840</th>\n",
       "      <th>PRI27276_org</th>\n",
       "      <th>PRO27826_org</th>\n",
       "      <th>MAB_ELE_PRO276</th>\n",
       "      <th>MAB_ELE_SHP1100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>35774028.5209</td>\n",
       "      <td>127.8088</td>\n",
       "      <td>109.1196</td>\n",
       "      <td>118.6708</td>\n",
       "      <td>124.2279</td>\n",
       "      <td>130.9893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>5063648.6000</td>\n",
       "      <td>117.6759</td>\n",
       "      <td>109.2248</td>\n",
       "      <td>120.4670</td>\n",
       "      <td>127.4041</td>\n",
       "      <td>132.9341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>37321267.9382</td>\n",
       "      <td>123.2801</td>\n",
       "      <td>109.3301</td>\n",
       "      <td>105.3787</td>\n",
       "      <td>120.5186</td>\n",
       "      <td>131.2613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>27090400.9380</td>\n",
       "      <td>111.0438</td>\n",
       "      <td>109.7510</td>\n",
       "      <td>107.1749</td>\n",
       "      <td>104.7763</td>\n",
       "      <td>113.0576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>34132093.4229</td>\n",
       "      <td>116.7369</td>\n",
       "      <td>109.8562</td>\n",
       "      <td>110.6476</td>\n",
       "      <td>109.5970</td>\n",
       "      <td>117.7047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sales  MAB_ELE_SHP840  PRI27276_org  PRO27826_org  MAB_ELE_PRO276  MAB_ELE_SHP1100\n",
       "month_year                                                                                           \n",
       "2018-10-01 35774028.5209        127.8088      109.1196      118.6708        124.2279         130.9893\n",
       "2018-11-01  5063648.6000        117.6759      109.2248      120.4670        127.4041         132.9341\n",
       "2018-12-01 37321267.9382        123.2801      109.3301      105.3787        120.5186         131.2613\n",
       "2019-01-01 27090400.9380        111.0438      109.7510      107.1749        104.7763         113.0576\n",
       "2019-02-01 34132093.4229        116.7369      109.8562      110.6476        109.5970         117.7047"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dfs['P1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>PRI27840_org</th>\n",
       "      <th>RohCOPPER1000_org</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>P8_lag_1</th>\n",
       "      <th>P8_lag_2</th>\n",
       "      <th>P8_lag_5</th>\n",
       "      <th>P8_lag_6</th>\n",
       "      <th>P8_lag_10</th>\n",
       "      <th>P8_ma_1</th>\n",
       "      <th>P8_ma_2</th>\n",
       "      <th>P8_ma_5</th>\n",
       "      <th>P8_ma_6</th>\n",
       "      <th>P8_ma_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>398332.2744</td>\n",
       "      <td>110.6561</td>\n",
       "      <td>75.7745</td>\n",
       "      <td>10</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>580778.2653</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>383206.7376</td>\n",
       "      <td>380139.7221</td>\n",
       "      <td>413852.9409</td>\n",
       "      <td>400303.6854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>111.0503</td>\n",
       "      <td>76.4355</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>518398.3785</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>742593.7831</td>\n",
       "      <td>514233.4804</td>\n",
       "      <td>497925.6504</td>\n",
       "      <td>457149.3767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>112014.9227</td>\n",
       "      <td>111.0322</td>\n",
       "      <td>76.4097</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>267418.3494</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>599435.1073</td>\n",
       "      <td>477114.1424</td>\n",
       "      <td>447197.0541</td>\n",
       "      <td>441609.0341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>540208.8029</td>\n",
       "      <td>111.0354</td>\n",
       "      <td>77.7720</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>372627.9465</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>326111.8628</td>\n",
       "      <td>501098.4985</td>\n",
       "      <td>487629.9191</td>\n",
       "      <td>458367.1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>491332.7200</td>\n",
       "      <td>111.2866</td>\n",
       "      <td>80.6535</td>\n",
       "      <td>14</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>491332.7200</td>\n",
       "      <td>515770.7614</td>\n",
       "      <td>525748.8024</td>\n",
       "      <td>499470.8688</td>\n",
       "      <td>471352.9383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Sales  PRI27840_org  RohCOPPER1000_org  time_idx  year  month     P8_lag_1     P8_lag_2    P8_lag_5    P8_lag_6   P8_lag_10      P8_ma_1     P8_ma_2     P8_ma_5     P8_ma_6    P8_ma_10\n",
       "month_year                                                                                                                                                                                                \n",
       "2019-08-01  398332.2744      110.6561            75.7745        10  2019      8  368081.2008  420287.0223 582419.0346 361474.5342 580778.2653  398332.2744 383206.7376 380139.7221 413852.9409 400303.6854\n",
       "2019-09-01 1086855.2918      111.0503            76.4355        11  2019      9  398332.2744  368081.2008 416386.5006 582419.0346 518398.3785 1086855.2918 742593.7831 514233.4804 497925.6504 457149.3767\n",
       "2019-10-01  112014.9227      111.0322            76.4097        12  2019     10 1086855.2918  398332.2744 297611.6126 416386.5006 267418.3494  112014.9227 599435.1073 477114.1424 447197.0541 441609.0341\n",
       "2019-11-01  540208.8029      111.0354            77.7720        13  2019     11  112014.9227 1086855.2918 420287.0223 297611.6126 372627.9465  540208.8029 326111.8628 501098.4985 487629.9191 458367.1197\n",
       "2019-12-01  491332.7200      111.2866            80.6535        14  2019     12  540208.8029  112014.9227 368081.2008 420287.0223 361474.5342  491332.7200 515770.7614 525748.8024 499470.8688 471352.9383"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_product_dfs['P8'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(train, val=None, test=None, outlier_treatment=True):\n",
    "\n",
    "    def winsorize_function(df, cols, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        \"\"\"Apply Winsorization only to the product sales column\"\"\"\n",
    "        df = df.copy()\n",
    "        bounds = {}\n",
    "        for col in cols:\n",
    "            q1 = df[col].quantile(lower_quantile)\n",
    "            q3 = df[col].quantile(upper_quantile)\n",
    "            df[col] = df[col].clip(lower=q1, upper=q3)\n",
    "            bounds[col] = (q1, q3)  # Store actual percentile values\n",
    "            print(f\"{col}: Winsorized Bounds -> Lower = {q1:.2f}, Upper = {q3:.2f}\")\n",
    "\n",
    "        return df, bounds\n",
    "        \n",
    "    def process_dataset(df, cols, is_train=True, bounds=None):\n",
    "        df = df.copy() \n",
    "        if is_train:\n",
    "            if outlier_treatment:\n",
    "                df, bounds = winsorize_function(df, cols)\n",
    "            else:\n",
    "                bounds = {}  \n",
    "        else:\n",
    "            if outlier_treatment:\n",
    "                if bounds is None:\n",
    "                    raise ValueError(\"Bounds must be provided for validation and test datasets.\")\n",
    "                for col in cols: \n",
    "                    if col in bounds:\n",
    "                        lower, upper = bounds[col]\n",
    "                        df[col] = df[col].clip(lower, upper)  # Corrected clipping\n",
    "        return (df, bounds) if is_train else df\n",
    "\n",
    "    # Process the training dataset\n",
    "    train, bounds = process_dataset(train, cols = train.columns, is_train=True)\n",
    "\n",
    "    # Process validation and test datasets with correct bounds\n",
    "    if val is not None:\n",
    "        val = process_dataset(val, cols = val.columns, is_train=False, bounds=bounds)\n",
    "\n",
    "    if test is not None:\n",
    "        test = process_dataset(test, cols = test.columns, is_train=False, bounds=bounds)\n",
    "        \n",
    "    # Return the datasets \n",
    "    if test is not None and val is not None:\n",
    "        return train, val, test\n",
    "    elif val is not None:\n",
    "        return train, val\n",
    "    elif test is not None:\n",
    "        return train, test\n",
    "    else:\n",
    "        return train\n",
    "\n",
    "def time_series_train_test_split(X, y, test_size=10):\n",
    "    \"\"\"Split time series data maintaining temporal order\"\"\"\n",
    "    split_idx = len(X) - test_size\n",
    "    return (\n",
    "        X.iloc[:split_idx], X.iloc[split_idx:],\n",
    "        y.iloc[:split_idx], y.iloc[split_idx:])\n",
    "\n",
    "def prepare_time_series_data(df_train, feature_set, target_col=\"Sales\", horizon=10, winsorize=False, scaling=False):\n",
    "\n",
    "    # Extract product data\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    y = data[[target_col]]  \n",
    "    X = data.drop(columns=[target_col])\n",
    "    \n",
    "    # Select features - handle empty feature_set case\n",
    "    if feature_set:\n",
    "        try:\n",
    "            # Convert feature_set to list if it's not already\n",
    "            if not isinstance(feature_set, list):\n",
    "                feature_set = [feature_set]\n",
    "            # Select only columns that exist in X\n",
    "            available_features = [f for f in feature_set if f in X.columns]\n",
    "            X = X[available_features]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Some features not found in DataFrame: {e}\")\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_val, y_train, y_val = time_series_train_test_split(X, y, test_size=horizon)\n",
    "\n",
    "    # Apply preprocessing steps\n",
    "    y_train, y_val = preprocessing_pipeline(y_train, y_val, test=None, outlier_treatment=winsorize)\n",
    "\n",
    "    # Scaling\n",
    "    if scaling: \n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_X.transform(X_val)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n",
    "        y_train_scaled = pd.DataFrame(y_train_scaled, index=y_train.index, columns=y_train.columns)\n",
    "        y_val_scaled = pd.DataFrame(y_val_scaled, index=y_val.index, columns=y_val.columns)\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled\n",
    "    else:\n",
    "        return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combinations(features, max_features=None):\n",
    "    \"\"\"Generate all possible feature combinations\"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = len(features)\n",
    "    \n",
    "    all_combinations = []\n",
    "    for r in range(1, max_features + 1):\n",
    "        all_combinations.extend(combinations(features, r))\n",
    "    \n",
    "    return [list(comb) for comb in all_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    if not results:\n",
    "        return pd.DataFrame()  # Retorna um DataFrame vazio se results estiver vazio\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Itera sobre cada dicionário dentro da lista de resultados\n",
    "    for result in results:\n",
    "        product_id = result.get('product_id', None)\n",
    "        winsorize = result.get('winsorize', None)\n",
    "        \n",
    "        # Verifica se 'features' existe e se não está vazio\n",
    "        features = ', '.join(result.get('features', [])) if result.get('features') else 'all'\n",
    "        \n",
    "        metrics = result.get('metrics', {})\n",
    "\n",
    "        validation_predictions = result.get('validation_predictions', [])\n",
    "\n",
    "        # Adiciona uma linha para cada previsão de validação\n",
    "        for pred in validation_predictions:\n",
    "            data.append({\n",
    "                'product_id': product_id,\n",
    "                'winsorize': winsorize,\n",
    "                'features': features,\n",
    "                'RMSE': metrics.get('RMSE', np.nan),\n",
    "                'MAPE': metrics.get('MAPE', np.nan),\n",
    "                'Overfit_Score': metrics.get('Overfit Score', np.nan),\n",
    "                'validation_prediction': pred\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_train=None, y_train_pred=None, print_metrics=False):\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    }\n",
    "    \n",
    "    if y_train is not None and y_train_pred is not None:\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        metrics['Overfit Score'] = (metrics['RMSE'] - train_rmse) / max(train_rmse, 1e-10)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"\\n=== Metrics ===\")\n",
    "        print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "        print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        if 'Overfit Score' in metrics:\n",
    "            print(f\"Overfit Score: {metrics['Overfit Score']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def convert_results_to_df(results):\n",
    "    \"\"\"Convert results to DataFrame format\"\"\"\n",
    "    if results is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame([{\n",
    "        'product_id': results['product_id'],\n",
    "        'winsorize': results['winsorize'],\n",
    "        'features': ', '.join(results['features']) if results['features'] else 'all',\n",
    "        'RMSE': results['metrics']['RMSE'],\n",
    "        'MAPE': results['metrics']['MAPE'],\n",
    "        'Overfit_Score': results['metrics'].get('Overfit Score', np.nan),\n",
    "        'method': results['method']\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Modeling Pipeline (Feature selection, Scaling, Model testing):**\n",
    "\n",
    "#### 4.1. Train vs Validation Splitting Strategy\n",
    "\n",
    "Our pipeline is going to iterate over:\n",
    "- **Walk-Forward Validation:** One-step forecasting where the model predicts the next step, and the validation window moves forward one step at a time.\n",
    "\n",
    "\n",
    "\n",
    "#### 4.2. Feature Combinations\n",
    "\n",
    "- No macro/lags features (baseline model).\n",
    "- Single-feature models.\n",
    "- Multi-feature models.\n",
    "\n",
    "#### 4.4. Models\n",
    "\n",
    "We will tune hyperparameters for all the models for each product, with the best features.\n",
    "\n",
    "\n",
    "**Time Series Model Comparison Table**\n",
    "\n",
    "| Feature               | TimeGPT (Zero-Shot) | XGBoost            | TCN                | ARIMA              | Neural Prophet     |\n",
    "|-----------------------|---------------------|--------------------|--------------------|--------------------|--------------------|\n",
    "| **Scaling Needed?**   | ❌ No               | ❌ No              | ✅ Yes             | ❌ No (if stationary) | ✅ Yes            |\n",
    "| **Handles Trends**    | ✅ Excellent        | ✅ (With features) | ✅ Excellent       | ✅ (With differencing) | ✅ Excellent     |\n",
    "| **Handles Seasonality** | ✅ Automatic       | ❌ (Requires manual lags) | ✅ (With large receptive field) | ✅ (SARIMA) | ✅ Automatic      |\n",
    "| **Multivariate Support** | ✅ Yes            | ✅ Yes             | ✅ Yes             | ❌ Univariate only | ✅ Yes             |\n",
    "| **Training Speed**    | ⚡ Instant (pre-trained) | 🏎️ Fast          | 🐢 Slow (GPU helps) | 🏃 Moderate        | 🐢 Slow           |\n",
    "| **Interpretability**  | ❌ Black box        | ✅ Feature importances | ❌ Black box     | ✅ Model coefficients | ✅ Component plots |\n",
    "| **Best For**          | Zero-shot forecasting | Tabular data with temporal features | Long-range dependencies | Simple univariate series | Hybrid (AR + NN) patterns |\n",
    "| **Weakness**          | Limited control     | Poor with long sequences | Computationally heavy | Linear assumptions only | Overfit risk       |\n",
    "\n",
    "\n",
    "1. **TimeGPT** (by Nixtla):\n",
    "   - Pros: No training needed, handles any frequency, great for quick benchmarks.\n",
    "   - Cons: Proprietary, limited customization.\n",
    "\n",
    "2. **XGBoost**:\n",
    "   - Best when you have:\n",
    "     - Easy and fast. \n",
    "\n",
    "3. **TCN** (Temporal Convolutional Networks):\n",
    "   - Pros: Captures long-range patterns better than LSTMs.\n",
    "   - Cons: Needs careful tuning of and is more computationaly expensive.\n",
    "\n",
    "4. **ARIMA**:\n",
    "   - Important:\n",
    "     - Data is stationary (or differenced) \n",
    "     - Short-term forecasts (≤12 steps)\n",
    "   - Warning: Fails miserably with:\n",
    "     - Non-linear trends\n",
    "     - High-frequency data, wich is ok, bc we are dealing with months.\n",
    "\n",
    "5. **Neural Prophet**:\n",
    "   - Hybrid of AR terms and neural nets.\n",
    "   - Best when: needed interpretable components (trend, seasonality)\n",
    "\n",
    "\n",
    "Summary:\n",
    "\n",
    "- **We will Scale**: TCN, Neural Prophet\n",
    "- **Optional Scaling**: XGBoost (for extreme values)\n",
    "- **We will not Scale**: ARIMA (but difference if non-stationary!), TimeGPT\n",
    "\n",
    "1. We will start with **TimeGPT** for a baseline.\n",
    "2. Then test **XGBoost** with lag features.\n",
    "3. Then **TCN** to see he effect for more complex patterns.\n",
    "4. **Neural Prophet** and **ARIMA** for interpretability.\n",
    "\n",
    "#### 4.5. Evaluation Metrics: \n",
    "- RMSE → Measures error magnitude.\n",
    "- MAPE → Percentage-based error, useful for business impact.\n",
    "- R² → Measures how well the model explains variance.\n",
    "- Overfit Score →  \n",
    "\n",
    "$$\n",
    "\\text{Overfit Score} = \\frac{\\text{Test RMSE} - \\text{Train RMSE}}{\\text{Train RMSE}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "    - < 0.1 (Underfit)\n",
    "    - 0.1 - 0.5 (Good Fit)\n",
    "    - > 0.5 (Overfit Warning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(model, X_train, y_train, X_test, y_test, \n",
    "                          title=\"Model Performance\", show_confidence=True):\n",
    "    # Get predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = make_subplots(rows=2, cols=1, \n",
    "                        subplot_titles=(\"Training Set Performance\", \"Test Set Performance\"),\n",
    "                        vertical_spacing=0.15)\n",
    "    \n",
    "    # Training set plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train.index,\n",
    "        y=y_train,\n",
    "        mode='lines',\n",
    "        name='Actual (Train)',\n",
    "        line=dict(color='black', width=2)\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_train.index,\n",
    "        y=train_pred,\n",
    "        mode='lines',\n",
    "        name='Predicted (Train)',\n",
    "        line=dict(color='blue', width=1, dash='dash')\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Test set plot\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_test.index,\n",
    "        y=y_test,\n",
    "        mode='lines',\n",
    "        name='Actual (Test)',\n",
    "        line=dict(color='black', width=2),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_test.index,\n",
    "        y=test_pred,\n",
    "        mode='lines',\n",
    "        name='Predicted (Test)',\n",
    "        line=dict(color='red', width=1, dash='dash'),\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    # Add confidence intervals if available and requested\n",
    "    if show_confidence and hasattr(model, 'predict_interval'):\n",
    "        try:\n",
    "            _, train_lower, train_upper = model.predict_interval(X_train)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(X_train.index) + list(X_train.index[::-1]),\n",
    "                y=list(train_upper) + list(train_lower[::-1]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0,100,80,0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                name='95% Confidence',\n",
    "                showlegend=False\n",
    "            ), row=1, col=1)\n",
    "            \n",
    "            _, test_lower, test_upper = model.predict_interval(X_test)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(X_test.index) + list(X_test.index[::-1]),\n",
    "                y=list(test_upper) + list(test_lower[::-1]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(100,0,80,0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                name='95% Confidence',\n",
    "                showlegend=False\n",
    "            ), row=2, col=1)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Value\", row=2, col=1)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(history):\n",
    "    \"\"\"For models that provide training history (like neural networks)\"\"\"\n",
    "    if not hasattr(history, 'history'):\n",
    "        print(\"Model doesn't have training history to plot\")\n",
    "        return\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training loss\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.arange(len(history.history['loss'])),\n",
    "        y=history.history['loss'],\n",
    "        mode='lines',\n",
    "        name='Training Loss',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    # Plot validation loss if available\n",
    "    if 'val_loss' in history.history:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.arange(len(history.history['val_loss'])),\n",
    "            y=history.history['val_loss'],\n",
    "            mode='lines',\n",
    "            name='Validation Loss',\n",
    "            line=dict(color='orange')\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Training Progress\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error\n",
    ")\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all relevant metrics for time series forecasting\"\"\"\n",
    "    metrics = {\n",
    "        'RMSE': mean_squared_error(y_true, y_pred, squared=False),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,  # as percentage\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'Error_Std': np.std(y_true - y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Model Performance\"):\n",
    "    \"\"\"Print metrics in a consistent format\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "    print(f\"MAE: {metrics['MAE']:.3f}\")\n",
    "    print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    print(f\"R²: {metrics['R2']:.3f}\")\n",
    "    print(f\"Error Std: {metrics['Error_Std']:.3f}\")\n",
    "\n",
    "def plot_time_series_forecast(\n",
    "    train_series, \n",
    "    test_series, \n",
    "    predictions,\n",
    "    train_predictions=None,\n",
    "    confidence_intervals=None,\n",
    "    title=\"Forecast Performance\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Value\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create interactive forecast visualization with Plotly\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_series : pd.Series with training data\n",
    "    test_series : pd.Series with actual test values\n",
    "    predictions : array-like with predicted values\n",
    "    train_predictions : array-like with predictions on training set (optional)\n",
    "    confidence_intervals : tuple of (upper_bound, lower_bound) arrays\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train_series.index,\n",
    "        y=train_series,\n",
    "        mode='lines',\n",
    "        name='Training Data',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Training predictions if provided\n",
    "    if train_predictions is not None:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=train_series.index,\n",
    "            y=train_predictions,\n",
    "            mode='lines',\n",
    "            name='Training Predictions',\n",
    "            line=dict(color='#ff7f0e', width=1.5, dash='dot')\n",
    "        ))\n",
    "    \n",
    "    # Test data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_series.index,\n",
    "        y=test_series,\n",
    "        mode='lines',\n",
    "        name='Actual Values',\n",
    "        line=dict(color='#2ca02c', width=2)\n",
    "    ))\n",
    "    \n",
    "    # Predictions\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=test_series.index,\n",
    "        y=predictions,\n",
    "        mode='lines',\n",
    "        name='Predictions',\n",
    "        line=dict(color='#d62728', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    # Confidence intervals if provided\n",
    "    if confidence_intervals is not None:\n",
    "        upper, lower = confidence_intervals\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(test_series.index) + list(test_series.index[::-1]),\n",
    "            y=list(upper) + list(lower[::-1]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(173, 216, 230, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Confidence',\n",
    "            showlegend=True\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        margin=dict(l=20, r=20, t=40, b=20)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_training_history(\n",
    "    history,\n",
    "    metrics=['loss'],\n",
    "    val_metrics=None,\n",
    "    title=\"Training Progress\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot training history for models that provide it (like neural networks)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object or dict with training metrics\n",
    "    metrics : list of metrics to plot from training\n",
    "    val_metrics : list of corresponding validation metrics\n",
    "    \"\"\"\n",
    "    if not hasattr(history, 'history') and not isinstance(history, dict):\n",
    "        print(\"No training history available to plot\")\n",
    "        return None\n",
    "    \n",
    "    history_dict = history.history if hasattr(history, 'history') else history\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training metrics\n",
    "    for metric in metrics:\n",
    "        if metric in history_dict:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=np.arange(1, len(history_dict[metric])+1),\n",
    "                y=history_dict[metric],\n",
    "                mode='lines+markers',\n",
    "                name=f'Training {metric}',\n",
    "                line=dict(width=2)\n",
    "            ))\n",
    "    \n",
    "    # Plot validation metrics if available\n",
    "    if val_metrics:\n",
    "        for val_metric in val_metrics:\n",
    "            if val_metric in history_dict:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=np.arange(1, len(history_dict[val_metric])+1),\n",
    "                    y=history_dict[val_metric],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'Validation {val_metric}',\n",
    "                    line=dict(width=2, dash='dash')\n",
    "                ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Metric Value\",\n",
    "        template=\"plotly_white\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def evaluate_and_visualize(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    model_type='standard',\n",
    "    confidence_intervals=None,\n",
    "    training_history=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation and visualization wrapper\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fitted model\n",
    "    X_train, y_train : training data\n",
    "    X_test, y_test : test data\n",
    "    model_type : str - 'standard', 'sequential', 'prophet', etc.\n",
    "    confidence_intervals : tuple of (upper, lower) bounds\n",
    "    training_history : history object for models that track training\n",
    "    \"\"\"\n",
    "    # Get predictions based on model type\n",
    "    if model_type == 'sequential':  # For Keras models\n",
    "        train_pred = model.predict(X_train).flatten()\n",
    "        test_pred = model.predict(X_test).flatten()\n",
    "    elif model_type == 'prophet':\n",
    "        train_pred = model.predict(X_train)['yhat'].values\n",
    "        test_pred = model.predict(X_test)['yhat'].values\n",
    "    else:  # Standard sklearn-style models\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = calculate_metrics(y_train, train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, test_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print_metrics(train_metrics, \"Training Set Performance\")\n",
    "    print_metrics(test_metrics, \"Test Set Performance\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    forecast_fig = plot_time_series_forecast(\n",
    "        train_series=y_train,\n",
    "        test_series=y_test,\n",
    "        predictions=test_pred,\n",
    "        train_predictions=train_pred,\n",
    "        confidence_intervals=confidence_intervals,\n",
    "        title=\"Model Forecast Performance\"\n",
    "    )\n",
    "    \n",
    "    if training_history is not None:\n",
    "        history_fig = plot_training_history(training_history)\n",
    "        return {\n",
    "            'metrics': {'train': train_metrics, 'test': test_metrics},\n",
    "            'forecast_plot': forecast_fig,\n",
    "            'training_plot': history_fig\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'metrics': {'train': train_metrics, 'test': test_metrics},\n",
    "        'forecast_plot': forecast_fig\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. TimeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Happy Forecasting! :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = \"nixak-CIwSKQ0cRLIuFR1TYllLFVakTGx3WCY30YPEKfxG0lDQcE0akGo3GE4aMJO9XXbkKjdFaGDP5x6uSxQ6\"\n",
    ")\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_timegpt(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                              nixtla_client=None, winsorize=False, \n",
    "                              horizon=7):\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if nixtla_client is None:\n",
    "        raise ValueError(\"NixtlaClient instance is required.\")\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, feature_set, target_col, horizon, winsorize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    " \n",
    "    # --- Determine Feature Types ---\n",
    "    hist_exog = [f for f in feature_set if '_lag_' in f or '_ma_' in f] if feature_set else []\n",
    "    fut_exog = [f for f in feature_set if f not in hist_exog] if feature_set else []\n",
    "\n",
    "\n",
    "    # --- Prepare Future Exogenous Features ---\n",
    "    X_future = None\n",
    "    if fut_exog:\n",
    "        X_future = X_val[fut_exog].copy()\n",
    "        if not X_future.isnull().all().all():  \n",
    "            X_future.insert(0, 'month_year', X_val.index)\n",
    "        else:\n",
    "            X_future = None  # Avoid passing empty DataFrame\n",
    "\n",
    "    # --- Simple Forecast (12 ≤ obs < 36) ---\n",
    "    if len(X_train) < 36:\n",
    "        print(f\"Using simple forecast ({len(X_train)} < 36 obs)\")\n",
    "        try:\n",
    "            history = df_train.copy()\n",
    "            if feature_set:\n",
    "                history = history[feature_set + [target_col]]\n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history.reset_index(),\n",
    "                                              time_col=\"month_year\", target_col=target_col, h=horizon,\n",
    "                                              X_df=X_future,  # Always pass X_future if available\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)\n",
    "            \n",
    "            # Get training predictions\n",
    "            train_pred = nixtla_client.forecast(df=history.reset_index(),time_col=\"month_year\",\n",
    "                                                target_col=target_col,h=1,hist_exog_list=hist_exog if hist_exog else None\n",
    "                                                ).iloc[0][\"TimeGPT\"]\n",
    "            \n",
    "            train_predictions = [train_pred] * len(history)\n",
    "            \n",
    "            return {\n",
    "                'product_id': product_id,'winsorize': winsorize,'features': feature_set,\n",
    "                'metrics': calculate_metrics(y_true=y_val,y_pred=forecast[\"TimeGPT\"],\n",
    "                                             y_train=history[target_col],y_train_pred=train_predictions),\n",
    "                'method': 'simple','validation_predictions': forecast[\"TimeGPT\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"Simple forecast failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- Walk-Forward (≥36 obs) ---\n",
    "    print(f\"Using walk-forward ({len(X_train)} ≥ 36 obs)\")\n",
    "    predictions = []\n",
    "    history = X_train.copy()\n",
    "    history[target_col] = y_train.values\n",
    "    history = history.reset_index()\n",
    "    \n",
    "    # Get training predictions\n",
    "    try:\n",
    "        train_fit = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,h=1,\n",
    "                                           hist_exog_list=hist_exog if hist_exog else None)\n",
    "        train_predictions = [train_fit[\"TimeGPT\"].iloc[0]] * len(y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Training pred failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = None\n",
    "            if fut_exog:\n",
    "                X_future_step = X_val.iloc[[i]][fut_exog].copy()\n",
    "                if not X_future_step.isnull().all().all():  \n",
    "                    X_future_step.insert(0, 'month_year', X_val.index[i])\n",
    "                else:\n",
    "                    X_future_step = None  \n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,\n",
    "                                              h=1, X_df=X_future_step if fut_exog else None, # FUTURE features (macro)\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)  # HISTORICAL features (lags, moving averages)\n",
    "            \n",
    "            pred = forecast[\"TimeGPT\"].iloc[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            new_row = X_val.iloc[i].copy()\n",
    "            new_row['month_year'] = X_val.index[i]\n",
    "            new_row[target_col] = pred\n",
    "            history = pd.concat([history, pd.DataFrame([new_row])])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {'product_id': product_id,'winsorize': winsorize,'features': feature_set,'metrics': metrics,\n",
    "            'method': 'walkforward','validation_predictions': predictions}\n",
    "\n",
    "\n",
    "def find_best_timegpt_config(product_id, df_train, nixtla_client, lagged_df=None, target_col=\"Sales\"):\n",
    "\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    print(normal_feature_combinations)\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                result = choose_parameters_timegpt(\n",
    "                    product_id=product_id,df_train=data, feature_set=features, target_col=target_col,\n",
    "                    nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    result = choose_parameters_timegpt(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_timegpt_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,\n",
    "            nixtla_client=nixtla_client,target_col=\"Sales\")\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "\n",
    "all_results = []  # List to store all results\n",
    "best_configs = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_timegpt, results_timegpt = process_product_parallel(product_id)\n",
    "        \n",
    "        if best_config_timegpt is not None:\n",
    "            best_configs.append(best_config_timegpt)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_timegpt is not None:\n",
    "            all_results.extend(results_timegpt)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n",
    "\n",
    "df_all_results = convert_results_to_df(all_results) if all_results else pd.DataFrame()\n",
    "df_best_configs = pd.DataFrame(best_configs) if best_configs else pd.DataFrame()\n",
    "\n",
    "df_all_results.to_csv(\"timegpt_best_configs.csv\", index=False)\n",
    "df_best_configs.to_csv(\"timegpt_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_xgboost(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                            param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized XGBoost forecasting with proper walk-forward validation\"\"\"\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize)\n",
    "        # Verify temporal ordering\n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Optimized Parameter Grid for Small Data ---\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [2, 3],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.9, 1.0],\n",
    "            'colsample_bytree': [0.9, 1.0],\n",
    "            'min_child_weight': [1, 3]}\n",
    "\n",
    "    # --- Efficient Model Setup ---\n",
    "    base_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',random_state=42,n_jobs=1,tree_method='hist',  # Faster training\n",
    "        enable_categorical=False,gamma=0,reg_alpha=0,reg_lambda=1)\n",
    "\n",
    "    # --- Strict Temporal Validation ---\n",
    "    test_fold = np.array([-1] * len(X_train) + [0] * len(X_val))\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "    # --- Focused Grid Search ---\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,param_grid=param_grid,scoring='neg_root_mean_squared_error',cv=ps,n_jobs=n_jobs,verbose=1,refit=True)\n",
    "\n",
    "    # --- Training with Validation ---\n",
    "    X_combined = pd.concat([X_train, X_val])\n",
    "    y_combined = pd.concat([y_train, y_val])\n",
    "    \n",
    "    print(f\"Starting grid search for {product_id}...\")\n",
    "    try:\n",
    "        grid_search.fit(X_combined, y_combined)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Robust Walk-forward Validation ---\n",
    "    predictions = []\n",
    "    print(f\"Using walk-forward (n_train={len(X_train)}, horizon={horizon})\")\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            # Create expanding window\n",
    "            window_X = pd.concat([X_train, X_val.iloc[:i]])\n",
    "            window_y = pd.concat([y_train, y_val.iloc[:i]])\n",
    "            \n",
    "            # Correct XGBoost 2.1.3+ fitting\n",
    "            best_model.fit(window_X, window_y, eval_set=[(X_val.iloc[[i]], y_val.iloc[[i]])], verbose=0)\n",
    "    \n",
    "            pred = best_model.predict(X_val.iloc[[i]]).item()\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)} - using fallback\")\n",
    "            fallback = (predictions[-1] if len(predictions) > 0 else \n",
    "                       y_train.iloc[-1] if len(y_train) > 0 else np.nan)\n",
    "            predictions.append(fallback)\n",
    "\n",
    "    # --- Training Metrics ---\n",
    "    try:\n",
    "        train_predictions = best_model.predict(X_train).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Training predictions failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    # --- Comprehensive Metrics ---\n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_predictions': predictions,\n",
    "        'feature_importances': dict(zip(feature_set, best_model.feature_importances_))}\n",
    "\n",
    "\n",
    "def find_best_xgboost_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_xgboost(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_xgboost(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_xgboost_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product: P5\n",
      "\n",
      "Testing - Winsorize: True, Features: []\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2125718.32\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1413065.22\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_SHP840']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2430260.13\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO271000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2519839.97\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2349197.41\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'MAB_ELE_SHP840']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1487484.89\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2083144.18\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1939059.32\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_SHP840', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2364489.99\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_SHP840', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2498038.06\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2202237.91\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2302070.16\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2414259.29\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1999480.95\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2429448.41\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1789375.78, Upper = 19056280.70\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2125718.32\n",
      "\n",
      "Testing - Winsorize: False, Features: []\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1989788.48\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1331522.07\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_SHP840']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2427323.29\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO271000_org']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2503312.35\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2330456.14\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'MAB_ELE_SHP840']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1544903.42\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2055348.75\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1979618.19\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_SHP840', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2339493.68\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_SHP840', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2684772.67\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2212564.35\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2315832.61\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2411074.92\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1987395.79\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 2432920.35\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 1989788.48\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2772577.80\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 3166254.47\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 3088499.03\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 3091055.90\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2154560.90\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2184444.37\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2851427.79\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2859963.93\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2426779.53\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2057063.72\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2723322.68\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2680536.84\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2101560.20\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2296357.18\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2867357.77\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2969262.89\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1991603.98\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1929731.61\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2510958.43\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2630676.19\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2083799.62\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2234443.30\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2445898.95\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2593427.89\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1956127.77\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2222964.90\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2500900.49\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 1999525.90, Upper = 19615688.64\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2625596.39\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2749043.85\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2786017.94\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 3097674.94\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 3100674.16\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2136650.24\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2220352.03\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2819534.50\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2871958.15\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2458245.73\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2054463.06\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2713305.42\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2524479.25\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2152006.42\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2327747.33\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2915331.01\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2997109.58\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1971115.18\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1892132.52\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2451236.23\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2649317.24\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1892873.67\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2233152.05\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2426939.84\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2645321.46\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 1820690.75\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2272065.14\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2425085.56\n",
      "\n",
      "Testing - Lagged Features: ['P5_lag_17', 'P5_lag_7', 'P5_lag_10', 'PRO27826_org', 'MAB_ELE_SHP840', 'PRO271000_org', 'MAB_ELE_PRO156', 'P5_ma_17', 'P5_ma_7', 'P5_ma_10']\n",
      "Preparing data...\n",
      "Starting grid search for P5...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 2647351.78\n",
      "\n",
      "✅ Best configuration for P5:\n",
      "- Winsorize: False\n",
      "- Features: ['PRO27826_org']\n",
      "- Best params: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.9}\n",
      "- RMSE: 1331522.07\n",
      "- Method: walkforward\n",
      "Processing product: P13\n",
      "\n",
      "Testing - Winsorize: True, Features: []\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11514.92\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11791.21\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11230.33\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12099.10\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 14105.72\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11294.55\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 10534.60\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12006.89\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27756_org', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11333.74\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27756_org', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12351.98\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12493.59\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 10669.35\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12183.81\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 10460.80\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12493.72\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 2973.33, Upper = 66712.37\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11514.92\n",
      "\n",
      "Testing - Winsorize: False, Features: []\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11450.21\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11964.38\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27756_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11232.16\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12094.39\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 14058.62\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'PRO27756_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11285.33\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 10540.05\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11966.95\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27756_org', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11324.56\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27756_org', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12405.51\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 12473.88\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 10643.68\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11958.43\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11358.07\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11060.91\n",
      "\n",
      "Testing - Winsorize: False, Features: ['MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=36, horizon=7)\n",
      "✅ Success - RMSE: 11450.21\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 13613.74\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12624.49\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12338.69\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 11263.04\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_5']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 13207.27\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_5', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12356.42\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_5', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 11458.06\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_5', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 10529.02\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12299.93\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_8', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12480.28\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_8', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 10777.03\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_8', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 10529.02\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_lag_5']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 13287.42\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_lag_5', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12624.49\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_lag_5', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 12380.81\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_lag_5', 'MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO276', 'PRI27840_org', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n",
      "Using walk-forward (n_train=19, horizon=7)\n",
      "✅ Success - RMSE: 11288.28\n",
      "\n",
      "Testing - Lagged Features: ['P13_lag_17', 'P13_lag_8']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 3860.58, Upper = 59849.75\n",
      "Starting grid search for P13...\n",
      "Fitting 1 folds for each of 96 candidates, totalling 96 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing product: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     best_config_xgboost, results_xgboost \u001b[38;5;241m=\u001b[39m process_product_parallel(product_id, custom_param_grids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_config_xgboost \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         best_configs_xgboost\u001b[38;5;241m.\u001b[39mappend(best_config_xgboost)  \u001b[38;5;66;03m# Store best config\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 212\u001b[0m, in \u001b[0;36mprocess_product_parallel\u001b[1;34m(product_id, custom_param_grids)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Some index values could not be parsed into datetime!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m     lagged_df \u001b[38;5;241m=\u001b[39m lagged_product_dfs\u001b[38;5;241m.\u001b[39mget(product_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 212\u001b[0m     best_config, results \u001b[38;5;241m=\u001b[39m find_best_xgboost_config(product_id\u001b[38;5;241m=\u001b[39mproduct_id,df_train\u001b[38;5;241m=\u001b[39mdf_train,lagged_df\u001b[38;5;241m=\u001b[39mlagged_df,target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_param_grids\u001b[38;5;241m=\u001b[39mcustom_param_grids)\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_config, results\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[42], line 174\u001b[0m, in \u001b[0;36mfind_best_xgboost_config\u001b[1;34m(product_id, df_train, lagged_df, target_col, n_jobs, custom_param_grids)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_param_grids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(features) \u001b[38;5;129;01min\u001b[39;00m custom_param_grids:\n\u001b[0;32m    172\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m custom_param_grids[\u001b[38;5;28mtuple\u001b[39m(features)]\n\u001b[1;32m--> 174\u001b[0m result \u001b[38;5;241m=\u001b[39m choose_parameters_xgboost(product_id\u001b[38;5;241m=\u001b[39mproduct_id,df_train\u001b[38;5;241m=\u001b[39mlagged_data,feature_set\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    175\u001b[0m                                    target_col\u001b[38;5;241m=\u001b[39mtarget_col,winsorize\u001b[38;5;241m=\u001b[39mwinsorize,n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,param_grid\u001b[38;5;241m=\u001b[39mparam_grid)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Success - RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 50\u001b[0m, in \u001b[0;36mchoose_parameters_xgboost\u001b[1;34m(product_id, df_train, feature_set, target_col, param_grid, n_jobs, winsorize, horizon)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting grid search for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     grid_search\u001b[38;5;241m.\u001b[39mfit(X_combined, y_combined)\n\u001b[0;32m     51\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "all_results_xgboost = []  # List to store all results\n",
    "best_configs_xgboost = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_xgboost, results_xgboost = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_xgboost is not None:\n",
    "            best_configs_xgboost.append(best_config_xgboost)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_xgboost is not None:\n",
    "            all_results_xgboost.extend(results_xgboost)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed! Results saved.\n"
     ]
    }
   ],
   "source": [
    "df_all_results_xgboost = convert_results_to_df(all_results_xgboost) if all_results_xgboost else pd.DataFrame()\n",
    "df_best_configs_xgboost = pd.DataFrame(best_configs_xgboost) if best_configs_xgboost else pd.DataFrame()\n",
    "\n",
    "df_all_results_xgboost.to_csv(\"xgboost_results.csv\", index=False)\n",
    "df_best_configs_xgboost.to_csv(\"xgboost_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_tcn(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                         param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized TCN forecasting with walk-forward validation\"\"\"\n",
    "    \n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize, scaling=True)\n",
    "        \n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Reshape data for TCN input (samples, timesteps, features)\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_val = y_val.values.reshape(-1, 1)\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'nb_filters': [16, 32],\n",
    "            'kernel_size': [2, 3],\n",
    "            'dilations': [[1, 2, 4], [1, 2, 4, 8]],\n",
    "            'dropout_rate': [0.1, 0.2],\n",
    "            'return_sequences': [False]  # single-step prediction\n",
    "}\n",
    "    \n",
    "    def build_tcn(nb_filters=32, kernel_size=2, dilations=[1, 2, 4], \n",
    "                 dropout_rate=0.1, return_sequences=False):\n",
    "        inputs = Input(shape=(1, X_train.shape[2]))\n",
    "        x = TCN(nb_filters=nb_filters, kernel_size=kernel_size, \n",
    "               dilations=dilations, dropout_rate=dropout_rate,\n",
    "               return_sequences=return_sequences)(inputs)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        return model\n",
    "    \n",
    "    best_params = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for filters in param_grid['nb_filters']:\n",
    "        for kernel in param_grid['kernel_size']:\n",
    "            for dilations in param_grid['dilations']:\n",
    "                for dropout in param_grid['dropout_rate']:\n",
    "                    for return_seq in param_grid['return_sequences']:\n",
    "                        model = build_tcn(filters, kernel, dilations, dropout, return_seq)\n",
    "                        try:\n",
    "                            model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "                            \n",
    "                            # Walk-forward validation (predict one step at a time)\n",
    "                            predictions = []\n",
    "                            history_X = X_train.copy()\n",
    "                            history_y = y_train.copy()\n",
    "                            \n",
    "                            for i in range(len(X_val)):\n",
    "                                # Predict next step\n",
    "                                X_next = X_val[i].reshape(1, 1, -1)\n",
    "                                pred = model.predict(X_next, verbose=0).flatten()[0]\n",
    "                                predictions.append(pred)\n",
    "                                \n",
    "                            # Calculate RMSE\n",
    "                            if len(y_val) == len(predictions):\n",
    "                                rmse = np.sqrt(np.mean((y_val.flatten() - predictions) ** 2))\n",
    "                            else:\n",
    "                                print(f\"Shape mismatch: y_val ({y_val.shape}), preds ({len(predictions)})\")\n",
    "                                continue\n",
    "                            \n",
    "                            if rmse < best_rmse:\n",
    "                                best_rmse = rmse\n",
    "                                best_params = {\n",
    "                                    'nb_filters': filters,\n",
    "                                    'kernel_size': kernel,\n",
    "                                    'dilations': dilations,\n",
    "                                    'dropout_rate': dropout,\n",
    "                                    'return_sequences': return_seq\n",
    "                                }\n",
    "                        except Exception as e:\n",
    "                            print(f\"Hyperparameter set failed: {str(e)}\")\n",
    "\n",
    "    if best_params is None:\n",
    "        print(f\"❌ No valid models for {product_id}\")\n",
    "        return None\n",
    "\n",
    "    # Final model training and walk-forward prediction\n",
    "    final_model = build_tcn(**best_params)\n",
    "    final_model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "    predictions = []\n",
    "\n",
    "    # FIXED: Create DataFrame with both features and target\n",
    "    history_data = np.column_stack([X_train[:, 0, :], y_train.flatten()])\n",
    "    history = pd.DataFrame(history_data, columns=feature_set + [target_col])\n",
    "    history.index = range(len(history))\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = X_val[i].reshape(1, 1, -1)\n",
    "            pred = final_model.predict(X_future_step, verbose=0).flatten()[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update training data\n",
    "            new_row = X_val[i, 0, :].tolist()\n",
    "            new_row.append(pred)\n",
    "            new_df = pd.DataFrame([new_row], columns=feature_set + [target_col])\n",
    "            history = pd.concat([history, new_df]).reset_index(drop=True)\n",
    "            \n",
    "            # Retrain on expanded window\n",
    "            rolling_window = 36 if len(history) > 36 else len(history)\n",
    "            train_subset = history.iloc[-rolling_window:]\n",
    "            \n",
    "            X_train_update = train_subset[feature_set].values.reshape(-1, 1, len(feature_set))\n",
    "            y_train_update = train_subset[target_col].values.reshape(-1, 1)\n",
    "            \n",
    "            final_model.fit(X_train_update, y_train_update, epochs=5, batch_size=16, verbose=0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(predictions[-1] if predictions else y_train[-1][0])\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val.flatten(), y_pred=predictions, y_train=y_train.flatten())\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': best_params,\n",
    "        'validation_predictions': predictions\n",
    "    }\n",
    "\n",
    "def find_best_tcn_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_tcn(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_tcn(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "        best_config, results = find_best_tcn_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product: P14\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27392_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 14978.79\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO28380_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 15484.21\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 16197.10\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27392_org', 'PRO28380_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 15385.45\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27392_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 15505.44\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO28380_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 15158.13\n",
      "\n",
      "Testing - Winsorize: True, Features: ['PRO27392_org', 'PRO28380_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = -2064.76, Upper = 73222.25\n",
      "✅ Success - RMSE: 15369.66\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27392_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 15358.70\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO28380_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 15123.52\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27756_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 16028.73\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27392_org', 'PRO28380_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 14890.75\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27392_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 14700.50\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO28380_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 14857.72\n",
      "\n",
      "Testing - Winsorize: False, Features: ['PRO27392_org', 'PRO28380_org', 'PRO27756_org']\n",
      "Preparing data...\n",
      "✅ Success - RMSE: 15557.06\n",
      "\n",
      "✅ Best configuration for P14:\n",
      "- Winsorize: False\n",
      "- Features: ['PRO27392_org', 'PRO27756_org']\n",
      "- Best params: {'nb_filters': 32, 'kernel_size': 3, 'dilations': [1, 2, 4, 8], 'dropout_rate': 0.1, 'return_sequences': False}\n",
      "- RMSE: 14700.50\n",
      "- Method: walkforward\n"
     ]
    }
   ],
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "all_results_tcn = []  # List to store all results\n",
    "best_configs_tcn = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_tcn, results_tcn = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_tcn is not None:\n",
    "            best_configs_tcn.append(best_config_tcn)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_tcn is not None:\n",
    "            all_results_tcn.extend(results_tcn)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_p1_lags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score, mean_absolute_percentage_error\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure time-series data is sorted and complete\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df_p1_lags \u001b[38;5;241m=\u001b[39m df_p1_lags\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m      9\u001b[0m df_p1_lags \u001b[38;5;241m=\u001b[39m df_p1_lags\u001b[38;5;241m.\u001b[39masfreq(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minterpolate()  \u001b[38;5;66;03m# Fill missing months\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fit Auto ARIMA Model (Using all data initially)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_p1_lags' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Ensure time-series data is sorted and complete\n",
    "df_p1_lags = df_p1_lags.sort_index()\n",
    "df_p1_lags = df_p1_lags.asfreq('MS').interpolate()  # Fill missing months\n",
    "\n",
    "# Fit Auto ARIMA Model (Using all data initially)\n",
    "auto_model = auto_arima(\n",
    "    df_p1_lags['P1'].dropna(),  \n",
    "    exogenous=df_p1_lags[['P1_lag_1', 'P1_lag_2', 'P1_lag_3']].dropna(),  \n",
    "    seasonal=False, \n",
    "    trace=True,\n",
    "    stepwise=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore',\n",
    "    max_p=5, \n",
    "    max_q=5,\n",
    "    d=None,  # Let ADF test decide differencing\n",
    "    test='adf',  \n",
    "    scoring='mse'\n",
    ")\n",
    "print(auto_model.summary())\n",
    "\n",
    "# Walk-Forward Forecasting with Expanding Window\n",
    "def walk_forward_forecast(model, df, horizon=10, initial_train_size=33):\n",
    "    \"\"\"\n",
    "    Walk-forward validation using an expanding window approach.\n",
    "    Predicts 'horizon' steps ahead at each iteration.\n",
    "    \"\"\"\n",
    "    predictions, true_values, lower_bounds, upper_bounds = [], [], [], []\n",
    "\n",
    "    for i in range(len(df) - initial_train_size - horizon + 1):\n",
    "        train = df.iloc[: i + initial_train_size]  # Expanding training set\n",
    "        test = df.iloc[i + initial_train_size : i + initial_train_size + horizon]\n",
    "\n",
    "        model.fit(train['P1'])\n",
    "        pred, conf_int = model.predict(n_periods=len(test), return_conf_int=True)\n",
    "\n",
    "        if len(pred) == len(test):  # Ensure consistent shape\n",
    "            predictions.append(pred)\n",
    "            true_values.append(test['P1'].values)\n",
    "            lower_bounds.append(conf_int[:, 0])\n",
    "            upper_bounds.append(conf_int[:, 1])\n",
    "\n",
    "    return np.array(true_values), np.array(predictions), np.array(lower_bounds), np.array(upper_bounds)\n",
    "\n",
    "# Perform Walk-Forward Forecasting\n",
    "true_values, predictions, lower_bounds, upper_bounds = walk_forward_forecast(auto_model, df_p1_lags, horizon=10)\n",
    "\n",
    "# Compute Errors\n",
    "def print_error(true, pred):\n",
    "    RMSE = mean_squared_error(true.flatten(), pred.flatten(), squared=False)\n",
    "    R2 = r2_score(true.flatten(), pred.flatten())\n",
    "    MAPE = mean_absolute_percentage_error(true.flatten(), pred.flatten())\n",
    "    print(\"RMSE:\", RMSE)\n",
    "    print(\"MAPE:\", round(MAPE * 100, 2), \"%\")\n",
    "    print(\"R2:\", R2)\n",
    "\n",
    "print_error(true_values, predictions)\n",
    "\n",
    "# ✅ Plot Full Dataset + Predictions + Confidence Intervals\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the real data (Training + Test)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_p1_lags.index,\n",
    "    y=df_p1_lags['P1'],\n",
    "    mode='lines',\n",
    "    name='Real Data',\n",
    "    line=dict(color='black', width=2)\n",
    "))\n",
    "\n",
    "# Add predictions over time\n",
    "for i in range(len(predictions)):\n",
    "    forecast_index = df_p1_lags.index[i + 33 : i + 33 + 10]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=forecast_index, \n",
    "        y=predictions[i],\n",
    "        mode='lines',\n",
    "        name=f'Prediction {i+1}',\n",
    "        line=dict(dash='dash', width=1, color='blue')\n",
    "    ))\n",
    "\n",
    "    # Add confidence interval shading\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(forecast_index) + list(forecast_index[::-1]),\n",
    "        y=list(upper_bounds[i]) + list(lower_bounds[i][::-1]),\n",
    "        fill='toself',\n",
    "        fillcolor='rgba(0,255,0,0.2)',\n",
    "        line=dict(color='rgba(255,255,255,0)'),\n",
    "        name='Confidence Interval',\n",
    "        showlegend=(i == 0)\n",
    "    ))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Walk-Forward Forecasting (10-month prediction horizon)\",\n",
    "    xaxis_title=\"Months\",\n",
    "    yaxis_title=\"P1 Values\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def choose_parameters_arima(product_id, df_train, target_col='Sales', winsorize=False, feature_set=None, horizon=10, seasonal=False, stationary=False):\n",
    "    \"\"\"\n",
    "    Optimized function to prepare data, run auto_arima, and evaluate performance.\n",
    "    \"\"\"\n",
    "    # Common Preprocessing\n",
    "    X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, product_id, target_col, feature_set, horizon, winsorize)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_train, y_val = y_train.values, y_val.values\n",
    "    X_train, X_val = X_train.values if not X_train.empty else None, X_val.values if not X_val.empty else None\n",
    "\n",
    "    # Auto ARIMA Model\n",
    "    model = auto_arima(y=y_train, X=X_train, stationary=stationary, seasonal=seasonal, suppress_warnings=True, stepwise=True, error_action='ignore', trace=False)\n",
    "\n",
    "    # Walk-forward Validation\n",
    "    predictions = []\n",
    "    for i in range(len(y_val)):\n",
    "        new_x = X_val[i].reshape(1, -1) if X_val is not None else None\n",
    "        model.update([y_val[i]], X=new_x)\n",
    "        pred = model.predict(n_periods=1, X=new_x)[0] if new_x is not None else model.predict(n_periods=1)[0]\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Evaluate Model\n",
    "    metrics = calculate_metrics(y_val, predictions)\n",
    "\n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set if feature_set else 'all',\n",
    "        'best_params': {'order': model.order, 'seasonal_order': model.seasonal_order if seasonal else None, 'aic': model.aic(), 'bic': model.bic()},\n",
    "        'metrics': metrics,\n",
    "        'model': model}\n",
    "\n",
    "\n",
    "def find_best_arima_config(product_id, df_train, target_col='Sales'):\n",
    "    \"\"\"\n",
    "    Find optimal ARIMA configuration matching original testing approach\n",
    "    \"\"\"\n",
    "    data = df_train[product_id].copy()\n",
    "    available_features = [col for col in data.columns if col != target_col]\n",
    "    \n",
    "    # Generate feature combinations (max 3 for ARIMAX stability)\n",
    "    feature_combinations = generate_feature_combinations(available_features, max_features=3)\n",
    "    \n",
    "    # Test all configurations - consistent with original approach\n",
    "    results = []\n",
    "    for winsorize in [True, False]:\n",
    "        for features in feature_combinations:\n",
    "            for seasonal in [False, True]:  # Test both seasonal and non-seasonal\n",
    "                print(f\"\\nTesting config - Winsorize: {winsorize}, Features: {features}, Seasonal: {seasonal}\")\n",
    "                \n",
    "                result = choose_parameters_arima(\n",
    "                    product_id=product_id,\n",
    "                    df_train=df_train,\n",
    "                    target_col=target_col,\n",
    "                    winsorize=winsorize,\n",
    "                    feature_set=features,\n",
    "                    seasonal=seasonal\n",
    "                )\n",
    "                results.append(result)\n",
    "    \n",
    "    # Find best configuration (lowest RMSE)\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "\n",
    "    print(f\"\\n✅ Best configuration for product {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Seasonal: {best_config['best_params']['seasonal_order'] is not None}\")\n",
    "    print(f\"- Order (p,d,q): {best_config['best_params']['order']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- R²: {best_config['metrics']['R2']:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "def process_product_arima(product_id):\n",
    "    \"\"\"\n",
    "    Consistent wrapper function matching original format\n",
    "    \"\"\"\n",
    "    best_config = find_best_arima_config(\n",
    "        product_id=product_id,\n",
    "        df_train=df_train,\n",
    "        target_col='Sales'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'Product_ID': product_id,\n",
    "        'Model': 'ARIMA',\n",
    "        'Winsorize': best_config['winsorize'],\n",
    "        'Features': best_config['features'],\n",
    "        'Order': best_config['best_params']['order'],\n",
    "        'Seasonal_Order': best_config['best_params']['seasonal_order'],\n",
    "        'RMSE': best_config['metrics']['RMSE'],\n",
    "        'R²': best_config['metrics']['R2']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. NeuralProphet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_neuralprophet(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                                  param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized NeuralProphet forecasting with walk-forward validation\"\"\"\n",
    "    \n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize)\n",
    "        \n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "        \n",
    "        # Prepare data for NeuralProphet (requires 'ds' and 'y' columns)\n",
    "        train_df = pd.DataFrame({\n",
    "            'ds': X_train.index,\n",
    "            'y': y_train\n",
    "        })\n",
    "        \n",
    "        val_df = pd.DataFrame({\n",
    "            'ds': X_val.index,\n",
    "            'y': y_val\n",
    "        })\n",
    "        \n",
    "        # Add features if present\n",
    "        if feature_set:\n",
    "            for feature in feature_set:\n",
    "                train_df[feature] = X_train[feature].values\n",
    "                val_df[feature] = X_val[feature].values\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_lags': [3, 7, 14],\n",
    "            'n_forecasts': [horizon],\n",
    "            'changepoints_range': [0.8, 0.95],\n",
    "            'num_hidden_layers': [0, 1],\n",
    "            'd_hidden': [32, 64],\n",
    "            'learning_rate': [0.01, 0.001],\n",
    "            'batch_size': [16, 32],\n",
    "            'epochs': [50, 100]\n",
    "        }\n",
    "    \n",
    "    best_params = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    # Grid search over parameter combinations\n",
    "    for n_lags in param_grid['n_lags']:\n",
    "        for n_forecasts in param_grid['n_forecasts']:\n",
    "            for changepoints_range in param_grid['changepoints_range']:\n",
    "                for num_hidden_layers in param_grid['num_hidden_layers']:\n",
    "                    for d_hidden in param_grid['d_hidden']:\n",
    "                        for learning_rate in param_grid['learning_rate']:\n",
    "                            for batch_size in param_grid['batch_size']:\n",
    "                                for epochs in param_grid['epochs']:\n",
    "                                    try:\n",
    "                                        # Initialize model with current parameters\n",
    "                                        model = NeuralProphet(\n",
    "                                            n_lags=n_lags,n_forecasts=n_forecasts,changepoints_range=changepoints_range,num_hidden_layers=num_hidden_layers,d_hidden=d_hidden,\n",
    "                                            learning_rate=learning_rate,normalize='soft')\n",
    "                                        \n",
    "                                        # Add features if present\n",
    "                                        if feature_set:\n",
    "                                            for feature in feature_set:\n",
    "                                                model = model.add_lagged_regressor(feature)\n",
    "                                        \n",
    "                                        # Train model\n",
    "                                        metrics = model.fit(train_df, freq='D', \n",
    "                                                          batch_size=batch_size, \n",
    "                                                          epochs=epochs,\n",
    "                                                          silent=True)\n",
    "                                        \n",
    "                                        # Walk-forward validation\n",
    "                                        predictions = []\n",
    "                                        history = train_df.copy()\n",
    "                                        \n",
    "                                        for i in range(len(val_df)):\n",
    "                                            # Create future dataframe with known regressors\n",
    "                                            future = model.make_future_dataframe(\n",
    "                                                history, \n",
    "                                                periods=1,\n",
    "                                                n_historic=len(history))\n",
    "                                            \n",
    "                                            if feature_set:\n",
    "                                                # Add future regressor values\n",
    "                                                for feature in feature_set:\n",
    "                                                    future[feature] = val_df[feature].iloc[i]\n",
    "                                            \n",
    "                                            # Predict next step\n",
    "                                            forecast = model.predict(future)\n",
    "                                            pred = forecast['yhat1'].iloc[0]\n",
    "                                            predictions.append(pred)\n",
    "                                            \n",
    "                                            # Update history with actual observation\n",
    "                                            new_row = val_df.iloc[i].to_frame().T\n",
    "                                            history = pd.concat([history, new_row])\n",
    "                                        \n",
    "                                        # Calculate RMSE\n",
    "                                        if len(y_val) == len(predictions):\n",
    "                                            rmse = np.sqrt(np.mean((y_val - predictions) ** 2))\n",
    "                                        else:\n",
    "                                            print(f\"Shape mismatch: y_val ({len(y_val)}), preds ({len(predictions)})\")\n",
    "                                            continue\n",
    "                                        \n",
    "                                        if rmse < best_rmse:\n",
    "                                            best_rmse = rmse\n",
    "                                            best_params = {\n",
    "                                                'n_lags': n_lags,\n",
    "                                                'n_forecasts': n_forecasts,\n",
    "                                                'changepoints_range': changepoints_range,\n",
    "                                                'num_hidden_layers': num_hidden_layers,\n",
    "                                                'd_hidden': d_hidden,\n",
    "                                                'learning_rate': learning_rate,\n",
    "                                                'batch_size': batch_size,\n",
    "                                                'epochs': epochs\n",
    "                                            }\n",
    "                                        \n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Hyperparameter set failed: {str(e)}\")\n",
    "                                        continue\n",
    "\n",
    "    if best_params is None:\n",
    "        print(f\"❌ No valid models for {product_id}\")\n",
    "        return None\n",
    "\n",
    "    # Train final model with best parameters\n",
    "    final_model = NeuralProphet(\n",
    "        n_lags=best_params['n_lags'],\n",
    "        n_forecasts=best_params['n_forecasts'],\n",
    "        changepoints_range=best_params['changepoints_range'],\n",
    "        num_hidden_layers=best_params['num_hidden_layers'],\n",
    "        d_hidden=best_params['d_hidden'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        normalize='soft'\n",
    "    )\n",
    "    \n",
    "    if feature_set:\n",
    "        for feature in feature_set:\n",
    "            final_model = final_model.add_lagged_regressor(feature)\n",
    "    \n",
    "    # Train on full training data\n",
    "    final_model.fit(train_df, freq='D', \n",
    "                   batch_size=best_params['batch_size'],\n",
    "                   epochs=best_params['epochs'],\n",
    "                   silent=True)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    predictions = []\n",
    "    history = train_df.copy()\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            # Create future dataframe\n",
    "            future = final_model.make_future_dataframe(\n",
    "                history, \n",
    "                periods=1,\n",
    "                n_historic=len(history)\n",
    "            \n",
    "            if feature_set:\n",
    "                # Add future regressor values if available\n",
    "                if i < len(val_df):\n",
    "                    for feature in feature_set:\n",
    "                        future[feature] = val_df[feature].iloc[i]\n",
    "                else:\n",
    "                    # If beyond validation data, use last known values\n",
    "                    for feature in feature_set:\n",
    "                        future[feature] = history[feature].iloc[-1]\n",
    "            \n",
    "            # Predict next step\n",
    "            forecast = final_model.predict(future)\n",
    "            pred = forecast['yhat1'].iloc[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update history with predicted observation\n",
    "            if i < len(val_df):\n",
    "                new_row = val_df.iloc[i].to_frame().T\n",
    "            else:\n",
    "                # If beyond validation data, create synthetic row\n",
    "                new_row = pd.DataFrame({\n",
    "                    'ds': [history['ds'].iloc[-1] + pd.Timedelta(days=1)],\n",
    "                    'y': pred\n",
    "                })\n",
    "                if feature_set:\n",
    "                    for feature in feature_set:\n",
    "                        new_row[feature] = history[feature].iloc[-1]\n",
    "            \n",
    "            history = pd.concat([history, new_row])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(predictions[-1] if predictions else train_df['y'].iloc[-1])\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val, y_pred=predictions[:len(y_val)], \n",
    "                              y_train=train_df['y'].values)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': best_params,\n",
    "        'validation_predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for he Macro Features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to classify variables\n",
    "def classify_variable(series):\n",
    "    \"\"\"Classifies variable based on normality and stationarity tests.\"\"\"\n",
    "    \n",
    "    # Remove NaN values for testing\n",
    "    clean_series = series.dropna()\n",
    "\n",
    "    # Check for normality\n",
    "    if len(clean_series) > 3:\n",
    "        stat, p_value = shapiro(clean_series)\n",
    "        is_normal = p_value > 0.05  # p-value > 0.05 means normal\n",
    "    else:\n",
    "        is_normal = False  # Not enough data to test normality\n",
    "\n",
    "    # Check for stationarity\n",
    "    if len(clean_series) > 3:\n",
    "        adf_stat, adf_p_value, _, _, _, _ = adfuller(clean_series)\n",
    "        is_stationary = adf_p_value < 0.05  # p-value < 0.05 means stationary\n",
    "    else:\n",
    "        is_stationary = False  # Not enough data\n",
    "\n",
    "    return is_normal, is_stationary\n",
    "\n",
    "# Function to automatically fill missing values\n",
    "def auto_impute_missing_values(df_train, df_test):\n",
    "    \"\"\"Automatically selects the best imputation method for each missing variable.\"\"\"\n",
    "    \n",
    "    # Identify missing columns in test set\n",
    "    missing_columns = df_test.columns[df_test.isnull().any()]\n",
    "    \n",
    "    # Iterate through missing columns\n",
    "    for col in missing_columns:\n",
    "        print(f\"Processing: {col}\")\n",
    "\n",
    "        series = df_train[col]  # Use train data for imputation\n",
    "        is_normal, is_stationary = classify_variable(series)\n",
    "\n",
    "        if is_normal:\n",
    "            # Case 1: Normally distributed → Sample from normal distribution\n",
    "            print(f\" - {col} is normal → Using Mean & Std Sampling\")\n",
    "            mean_value, std_value = series.mean(), series.std()\n",
    "            num_missing = df_test[col].isnull().sum()\n",
    "            predictions = norm.rvs(loc=mean_value, scale=std_value, size=num_missing)\n",
    "        \n",
    "        elif is_stationary:\n",
    "            # Case 2: Stationary but non-normal → Simple Exponential Smoothing\n",
    "            print(f\" - {col} is stationary → Using Simple Exponential Smoothing\")\n",
    "            model = SimpleExpSmoothing(series.dropna()).fit()\n",
    "            predictions = model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        elif not is_stationary:\n",
    "            # Case 3: Non-Stationary → ARIMA\n",
    "            print(f\" - {col} is non-stationary → Using ARIMA\")\n",
    "            model = ARIMA(series.dropna(), order=(1, 1, 1))  # (p,d,q) chosen based on domain knowledge\n",
    "            fitted_model = model.fit()\n",
    "            predictions = fitted_model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        else:\n",
    "            # Case 4: If nothing works → Use XGBoost Regression\n",
    "            print(f\" - {col} is complex → Using XGBoost Regression\")\n",
    "            train_data = df_train.dropna(subset=[col])  # Drop missing values for training\n",
    "            X_train = train_data.drop(columns=[col])  # Exclude target column\n",
    "            y_train = train_data[col]  # Target column\n",
    "\n",
    "            X_test = df_test.loc[df_test[col].isnull(), X_train.columns]  # Only missing values\n",
    "\n",
    "            model = XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "        # Assign predictions\n",
    "        missing_indexes = df_test[df_test[col].isnull()].index\n",
    "        df_test.loc[missing_indexes, col] = predictions\n",
    "\n",
    "    return df_test\n",
    "\n",
    "# Example usage\n",
    "df_train = remerged_data[1]  # Use remerged train data\n",
    "df_test = test_1.copy()  # Copy test set\n",
    "\n",
    "# Apply automatic imputation\n",
    "df_test_filled = auto_impute_missing_values(df_train, df_test)\n",
    "\n",
    "# Check results\n",
    "print(df_test_filled.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
