{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"header.png\" width=\"100%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <strong style=\"display: block; margin-bottom: 10px;\">Group P</strong> \n",
    "    <table style=\"margin: 0 auto; border-collapse: collapse; border: 1px solid black;\">\n",
    "        <tr>\n",
    "            <th style=\"border: 1px solid white; padding: 8px;\">Name</th>\n",
    "            <th style=\"border: 1px solid white; padding: 8px;\">Student ID</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Beatriz Monteiro</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20240591</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Catarina Nunes</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20230083</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Margarida Raposo</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20241020</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">Teresa Menezes</td>\n",
    "            <td style=\"border: 1px solid white; padding: 8px;\">20240333</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔗 Table of Contents <a id='table-of-contents'></a>\n",
    "1. [Introduction](#introduction): Previous Notebook\n",
    "2. [Business Understanding](#business-understanding): Previous Notebook  \n",
    "3. [Data Understanding](#data-understanding): Previous Notebook\n",
    "4. [Data Preparation](#data-preparation): Previous Notebook + In this one we have the preparation pipeline  \n",
    "5. [Modeling](#modeling): \n",
    "6. [Evaluation](#evaluation)\n",
    "7. [Conclusion](#conclusion) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\"> 📌 Introduction <a id='introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project follows the **CRISP-DM** methodology to conduct a monthly sales forecast of the smart infrastructure business unit of Siemens. \n",
    "\n",
    "<b style=\"background-color:#A9A9A9; padding:5px; border-radius:5px; display: inline-block;\">CRISP-DM</b>\n",
    "\n",
    "<ul style=\"margin-bottom: 30px;\">\n",
    "    <li><u>Business Understanding</u>: Defining objectives, assessing resources, and project planning.</li>\n",
    "    <li><u>Data Understanding</u>: Collecting, exploring, and verifying data quality.</li>\n",
    "    <li><u>Data Preparation</u>: Selecting, cleaning, constructing, integrating, and formatting data to ensure it is ready for analysis.</li>\n",
    "    <li><u>Modeling</u>: Selecting and applying various modeling techniques while calibrating their parameters to optimal values.</li>\n",
    "    <li><u>Evaluation</u>: Select the models which are the best performers and evaluate thoroughly if they align with the business objectives. </li>\n",
    "    <li><u>Deployment</u>: Bridge between data mining goals and the business application of the finalized model.</li>\n",
    "</ul>\n",
    "\n",
    "---\n",
    "<b style=\"background-color:#A9A9A9; padding:5px; border-radius:5px; display: inline-block;\">Montlhy Sales Forecast</b>\n",
    "\n",
    "<p style=\"margin-bottom: 50px;\"> This case study focuses on ... </p>\n",
    "\n",
    "---\n",
    "\n",
    "<b style=\"background-color:#A9A9A9; padding:5px; border-radius:5px; display: inline-block;\">Market Data</b><br>\n",
    "(https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Thematic_glossaries)<br>\n",
    "Due to the lack of information about the macroeconomic indicatores used, our analysis was based on the assumption that the following definitions are correct.\n",
    "\n",
    "\n",
    "- The **Production index** measures changes in value added at factor cost of industry and construction over a month. It does this by measuring changes in the volume of output and activity monthly.\n",
    "- The **Shipment index** is a measure of the volume of goods shipped by manufacturers monthly. It is an indicator of economic trends, since it reflects production, supply chain dynamics and overall industrial performance. \n",
    "- The **World prices of materials** is monthly average global market prices for key industrial inputs, such as energy, heavy materials, etc. These are likely to fluctuate with changes in supply/demand dynamics and economic conditions. \n",
    "- The **United States: EUR in LCU** is the monthly exchange rate of the Euro, expressed in local currency units from the United States (US dollar), representing the number of US dollars per euro. It indicates currency fluctuation, which often impacts trade.\n",
    "- The **Producer prices** measures the monthly change in the trading price of products and related services, from the seller's perspective. It does not only serve as an early indicator of inflationary pressures in the economy before it reaches the consumer, but it can also record the evolution of prices over longer time periods.\n",
    "\n",
    "<br>\n",
    "\n",
    "| **Features**                                    | **Feature Description**                                                                                             |\n",
    "|-------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| *MAB_ELE_PRO156*                                | **Production index** for machinery and electrical equipment in China.                                                     |\n",
    "| *MAB_ELE_SHP156*                                | **Shipments index** for machinery and electrical equipment in China.                                                     |\n",
    "| *MAB_ELE_PRO250*                                | **Production index** for machinery and electrical equipment in France.                                                    |\n",
    "| *MAB_ELE_SHP250*                                | **Shipments index** for machinery and electrical equipment in France.                                                    |\n",
    "| *MAB_ELE_PRO276*                                | **Production index** for machinery and electrical equipment in Germany.                                                  |\n",
    "| *MAB_ELE_SHP276*                                | **Shipments index** for machinery and electrical equipment in Germany.                                                  |\n",
    "| *MAB_ELE_PRO380*                                | **Production index** for machinery and electrical equipment in Italy.                                                    |\n",
    "| *MAB_ELE_SHP380*                                | **Shipments index** for machinery and electrical equipment in Italy.                                                    |\n",
    "| *MAB_ELE_PRO392*                                | **Production index** for machinery and electrical equipment in Japan.                                                    |\n",
    "| *MAB_ELE_SHP392*                                | **Shipments index** for machinery and electrical equipment in Japan.                                                    |\n",
    "| *MAB_ELE_PRO756*                                | **Production index** for machinery and electrical equipment in Switzerland.                                              |\n",
    "| *MAB_ELE_SHP756*                                | **Shipments index** for machinery and electrical equipment in Switzerland.                                              |\n",
    "| *MAB_ELE_PRO826*                                | **Production index** for machinery and electrical equipment in United Kingdom.                                           |\n",
    "| *MAB_ELE_SHP826*                                | **Shipments index** for machinery and electrical equipment in United Kingdom.                                           |\n",
    "| *MAB_ELE_PRO840*                                | **Production index** for machinery and electrical equipment in the United States.                                        |\n",
    "| *MAB_ELE_SHP840*                                | **Shipments index** for machinery and electrical equipment in the United States.                                        |\n",
    "| *MAB_ELE_PRO1100*                               | **Production index** for machinery and electrical equipment in Europe.                                                   |\n",
    "| *MAB_ELE_SHP1100*                               | **Shipments index** for machinery and electrical equipment in Europe.                                                   |\n",
    "| *RohiBASEMET1000_org*                           | **World price** of base metals.                                                                                          |\n",
    "| *RohiENERGY1000_org*                            | **World price** of energy.                                                                                               |\n",
    "| *RohiMETMIN1000_org*                            | **World price** of metals and minerals.                                                                                   |\n",
    "| *RohiNATGAS1000_org*                            | **World price** of natural gas index.                                                                                     |\n",
    "| *RohCRUDE_PETRO1000_org*                        | **World price** of crude oil (average).                                                                                   |\n",
    "| *RohCOPPER1000_org*                             | **World price** of copper.                                                                                                |\n",
    "| *WKLWEUR840_org*                                | United States: **Euros in local currency units** (LCU).                                                                    |\n",
    "| *PRI27840_org*                                  | United States: Electrical equipment (**Producer Prices**).                                                                |\n",
    "| *PRI27826_org*                                  | United Kingdom: Electrical equipment (**Producer Prices**).                                                               |\n",
    "| *PRI27380_org*                                  | Italy: Electrical equipment (**Producer Prices**).                                                                        |\n",
    "| *PRI27250_org*                                  | France: Electrical equipment (**Producer Prices**).                                                                       |\n",
    "| *PRI27276_org*                                  | Germany: Electrical equipment (**Producer Prices**).                                                                      |\n",
    "| *PRI27156_org*                                  | China: Electrical equipment (**Producer Prices**).                                                                       |\n",
    "| *PRO28840_org*                                  | United States: Machinery and equipment n.e.c. (**Production index**).                                                      |\n",
    "| *PRO281000_org*                                 | World: Machinery and equipment n.e.c. (**Production index**).                                                             |\n",
    "| *PRO28756_org*                                  | Switzerland: Machinery and equipment n.e.c. (**Production index**).                                                       |\n",
    "| *PRO28826_org*                                  | United Kingdom: Machinery and equipment n.e.c. (**Production index**).                                                    |\n",
    "| *PRO28380_org*                                  | Italy: Machinery and equipment n.e.c. (**Production index**).                                                             |\n",
    "| *PRO28392_org*                                  | Japan: Machinery and equipment n.e.c. (**Production index**).                                                             |\n",
    "| *PRO28250_org*                                  | France: Machinery and equipment n.e.c. (**Production index**).                                                            |\n",
    "| *PRO28276_org*                                  | Germany: Machinery and equipment n.e.c. (**Production index**).                                                           |\n",
    "| *PRO27840_org*                                  | United States: Electrical equipment (**Production index**).                                                               |\n",
    "| *PRO271000_org*                                 | World: Electrical equipment (**Production index**).                                                                      |\n",
    "| *PRO27756_org*                                  | Switzerland: Electrical equipment (**Production index**).                                                                 |\n",
    "| *PRO27826_org*                                  | United Kingdom: Electrical equipment (**Production index**).                                                             |\n",
    "| *PRO27380_org*                                  | Italy: Electrical equipment (**Production index**).                                                                      |\n",
    "| *PRO27392_org*                                  | Japan: Electrical equipment (**Production index**).                                                                      |\n",
    "| *PRO27250_org*                                  | France: Electrical equipment (**Production index**).                                                                     |\n",
    "| *PRO27276_org*                                  | Germany: Electrical equipment (**Production index**).                                                                    |\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<b style=\"background-color:#A9A9A9; padding:5px; border-radius:5px; display: inline-block;\">Sales Data</b>\n",
    "| Features                                      | Feature Description |\n",
    "|-----------------------------------------------|---------------------|\n",
    "| *DATE*                                       | Day at which a sale was made, in the format *dd-mm-yyyy* |\n",
    "| *Mapped_GCK*                                 | Product group of the product sold |\n",
    "| *Sales_EUR*                                  | Monetary amount made from the sale, in Euros  |\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install prophet == 1.1.6\n",
    "# pip install keras == 3.9.0 tensorflow == 2.19.0\n",
    "# pip install neuralprophet == 0.8.0\n",
    "# pytorch-forecasting-1.3.0\n",
    "#pip install nixtla == 0.6.6\n",
    "#pip install pmdarima == 2.0.4\n",
    "#pip install scikeras == 0.13.0\n",
    "#pip install keras-tcn == 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n",
      "ERROR:NP.plotly:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, TimeSeriesSplit\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#timesgpt\n",
    "from nixtla import NixtlaClient\n",
    "#xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "#tcn\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tcn import TCN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "#arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "#NeuralProphet\n",
    "from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error,\n",
    "    mean_absolute_error,\n",
    "    root_mean_squared_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "#Check if Y is stationary\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Modeling Pipeline (Feature selection, Scaling, Model testing)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from pytorch_forecasting.data.timeseries import TimeSeriesDataSet\n",
    "import torch\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)       # Set display width\n",
    "pd.set_option('display.max_colwidth', 100) # Show full feature lists\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # 4 decimal places\n",
    "\n",
    "# If you want to force standard notation (no scientific):\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x if abs(x) > 1e-4 else '%.4e' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_excel_sheets_side_by_side(excel_path, tablespacing=5, max_height=\"500px\"):\n",
    "    \"\"\"\n",
    "    Displays all sheets from an Excel file as tables in a scrollable horizontal div.\n",
    "    \n",
    "    Args:\n",
    "        excel_path (str): Path to the Excel file.\n",
    "        tablespacing (int): Spacing between tables (in pixels).\n",
    "        max_height (str): Maximum height of the table container (e.g., \"500px\").\n",
    "    \"\"\"\n",
    "    # Read all sheets from the Excel file\n",
    "    sheets_dict = pd.read_excel(excel_path, sheet_name=None)\n",
    "    \n",
    "    # Generate HTML for each sheet\n",
    "    html = \"\"\"\n",
    "    <div style='overflow-x: auto; white-space: nowrap; padding-bottom: 10px; border-bottom: 2px solid #ddd;'>\n",
    "    \"\"\"\n",
    "    \n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        # Convert DataFrame to HTML and style it\n",
    "        table_html = df.to_html(\n",
    "            index=False, \n",
    "            classes='dataframe', \n",
    "            border=0,\n",
    "            max_rows=10  # Limit rows for display (optional)\n",
    "        )\n",
    "        \n",
    "        # Wrap each table in a scrollable div\n",
    "        html += f\"\"\"\n",
    "        <div style='display:inline-block; margin-right:{tablespacing * 5}px; vertical-align:top;'>\n",
    "            <h4 style='text-align:center;'>{sheet_name}</h4>\n",
    "            <div style='max-height:{max_height}; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
    "                {table_html}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\"> 📌 Business Understanding <a id='business-understanding'></a>\n",
    "\n",
    "##### Click [here](#table-of-contents) ⬆️ to return to the Index.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Business Understanding** phase of the project entails the comprehension of the background leading to the project, as well as the business goals and requirements to be achieved. \n",
    "<br><br><br>\n",
    "Siemens is a global technology company that provides multi-industry solutions, with a strong focus on automation, digitalization, and sustainability. \n",
    "\n",
    "**Smart Infrastructure** division at Siemens\n",
    "\n",
    "(https://www.siemens.com/global/en/company/about/businesses/smart-infrastructure.html\n",
    "https://www.siemens.com/global/en/company/about/history/company/2007-2018.html)\n",
    "\n",
    "In October 2018, Siemens launched its Smart Infrastructure division, as part of the implementation of an optimized corporate structure. This new division emerged from the merger of former divisions, specifically the power distribution segment of the Energy Management division, the Building Technologies’ business, and the control products business of the Digital Factory division.\n",
    "Smart Infrastructure helps customers through their digital transformation journey by leveraging cutting-edge technology that enables clients to adapt to evolving environments and to seize new opportunities. It mostly provides B2B services, focusing on energy efficiency and resource optimization, which in turn improves clients’ asset performance, availability, reliability, and operational efficiency. All in all, this division future-proofs infrastructures, fostering collaborative ecosystems.\n",
    "\n",
    "<br><br>\n",
    "**Economic Environment 2018**\n",
    "\n",
    "(Siemens. (2018). Annual report 2018. Siemens AG. https://www.siemens.com/global/en/company/investor-relations/events-publications-ad-hoc/annualreports.html)\n",
    "\n",
    "`Overall economic conditions`\n",
    "In 2018, advanced countries saw GDP growth of 2.3%, mainly due to the U.S. economy benefiting from tax cuts. In contrast, emerging countries' GDP growth fell slightly from 4.7% to 4.6%, affected by capital outflows and currency depreciation, notably in Argentina and Turkey. This decline was worsened by U.S. monetary tightening and fears of a global trade war from new tariffs. Political tensions increased in the Middle East, particularly with U.S. sanctions on Iran. In Europe, uncertainties around the U.K.'s exit from the EU and worries about Italy’s budget led to negative investment sentiment. All in all, the economic acceleration at the beginning of 2018 lost steam towards the end of the year due to these adverse effects.\n",
    "\n",
    "`Energy Management`\n",
    "The Energy Management Division offers a wide spectrum of software, products, systems, solutions, and services for transmitting, distributing and managing electrical power and for providing intelligent power infrastructure.\n",
    "This sector faces competition from large multinational companies and fast-growing firms in emerging countries. It generally benefits from major trends, in particular decarbonization, decentralization and digitalization. Although Energy Management reported a double-digit order decline, due to a sharply lower volume of large orders in the transmission solutions business, revenue grew in most Division’s businesses, led by the power distribution businesses. For the latter, on a geographic basis, higher revenue in the region Asia, Australia was partially offset by a decline in the Americas strongly influenced by currency headwinds, while revenue in the region Europe, C. I. S., Africa, Middle East was flat year-over-year.\n",
    "Under the new organizational structure, transmission solutions will be assigned to the Operating Company Gas and Power, while the businesses for power distribution will be assigned to the Operating Company Smart Infrastructure.\n",
    "\n",
    "\n",
    "`Building Technologies`\n",
    "The Building Technologies Division is a leading provider of automation technologies and digital services for safe, secure and efficient buildings and infrastructures. The Division offers products, solutions, services and software for fire safety, security, building automation and energy management.\n",
    "The Division competes with multinational companies, system integrators, local firms, and IT startups. Strong competition is causing price pressure, especially in the solutions business. Economic changes affect the Division's activities with delay, and particularly in non-residential construction markets, there is a time lag of two to four quarters. Key trends include the demand for energy efficiency and smart space solutions to improve comfort and productivity. In fiscal 2018, the Building Technologies acquired three startups to strengthen its portfolio and expertise. Revenue increased, with the Division’s revenue growing faster than the market on increases in all reporting regions, with strongest market growth contributions came from the Asia, Australia region, particularly including India. Orders grew in the region Europe, C. I. S., Africa, Middle East, driven by double-digit growth in Germany including multiple multi-year service contracts. The Division won major orders in the U. S. in both periods under review. Profit and profitability remained strong, supported by economies of scale and improved productivity. The markets are expected to grow solidly in fiscal 2019, although U.S. growth may slow slightly.\n",
    "\n",
    "\n",
    "`Digital Factory`\n",
    "The Digital Factory Division provides a wide range of automation products and solutions for manufacturing, including software, control systems, motors, and integrated systems. It also offers lifecycle services and software for design and testing. Changes in customer demand are strongly driven by macroeconomic cycles, leading to variations in profitability. Competition is mainly from large multinational firms offering a relatively broad portfolio, and companies that are only active in certain geographic or product markets. Key trends include rising interest rates, which will limit investment but at the same time make it more focused on improving efficiency, a shift towards regionalization solutions, either to protect local economies or to better adapt solutions to local needs, and an increased focus on digitalization for competitiveness. The Division reported year-over-year growth in orders and revenue, especially in software. Growth was strongest in the Americas and Asia, notably China, and customer investments grew faster than their production, underlining a clear growth momentum relating to automation and digitalization. While the market is expected to continue growing, slower demand is anticipated for 2019 due to the high investment levels of 2018 and rising global trade tensions.\n",
    "\n",
    "<br><br>\n",
    "**Economic Environment 2019**\n",
    "\n",
    "(Siemens. (2019). Annual report 2019. Siemens AG. https://www.siemens.com/global/en/company/investor-relations/events-publications-ad-hoc/annualreports.html)\n",
    "\n",
    "`Overall economic conditions`\n",
    "After GDP growth of 3.2% in 2018, it is expected to drop to 2.6% in 2019, as to the U.S.-China trade dispute and geopolitical tensions in the Middle East contributed to uncertainty, which weighed on capital investments. Global exchange of goods started to decline from October 2018, leading to near-stagnation of industrial production afterwards. The decline affected regions reliant on trade and industry, particularly Germany and Italy in Europe, Mexico and Canada in the Americas, and India in Asia. In addition, European economies suffered from continued uncertainties regarding the U. K. leaving the European Union (Brexit) and the budget clash between the European Commission and Italy’s government, both weighing especially on business investment environment. Countries dependent on commodities and raw material exporting, notably Chile, Brazil and Argentina, saw declines in commodity prices in addition to other adverse factors including domestic political and financial instability. Advanced countries are expected to grow by 1.6%, 0.7 percentage points lower than 2018, while emerging countries will see a decline from 4.6% to 4.1%.\n",
    "\n",
    "`Smart Infrastructure`\n",
    "Smart Infrastructure brings together energy supply – from intelligent control across the grid and low- and medium-voltage electrification and control products – with building technology, targeting high-growth areas like energy storage, electric vehicle infrastructure, and microgrids at the grid edge. It reaches customers through multiple channels, including global sales, distributors, OEMs, resellers, installers, and direct sales via regional branch offices. Its diverse customer base encompasses infrastructure developers, construction firms, public and commercial buildings, utilities, and heavy industries. Smart Infrastructure faces competition from large multinationals, smaller manufacturers, local integrators, and facility management firms. Economic changes impact customer demand differently, with discrete manufacturing reacting quickly and strongly with macroeconomic cycles, while other sectors - infrastructure, construction, heavy industries and the utilities – have a slower response to economic cycles. Key trends include rising population and urbanization, the need for safe and sustainable environments, and decarbonization, growing decentralization and prosumers’ influence. Orders and revenue increased across all business areas and regions in fiscal 2019.  \n",
    "\n",
    "<br><br>\n",
    "**Economic Environment 2020**\n",
    "\n",
    "(Siemens. (2020). Annual report 2020. Siemens AG. https://www.siemens.com/global/en/company/investor-relations/events-publications-ad-hoc/annualreports.html)\n",
    "\n",
    "`Overall economic conditions`\n",
    "The fiscal year 2020 was heavily affected by the coronavirus pandemic and subsequent recession, the deepest since World War II. Global GDP is expected to fall by 4.5% in 2020, following a growth of 2.6% in 2019. Economic activity was already decelerating due to the U.S.-China trade conflict, but economic sentiment indicators were starting to improve in response to calming of the conflict ( “phase one deal”). Then COVID-19 emerged and started to spread globally, resulting in strict social distancing measures that limited operations, especially in contact-heavy sectors. Many other industries were directly affected by supply chain problems or indirectly by insufficient demand and stopped production.\n",
    "\n",
    "Governments and central banks responded with strong fiscal and monetary policies to support businesses and households, leading to a brief economic rebound in the summer. However, renewed virus outbreaks hindered full recovery and restrictions in high contact industries made the economy operate only at about 90% of its full potential, with advanced economies projected to see a 5.5% decline in GDP. Siemens experienced mixed impacts across its sectors, benefiting from growth in technology-related areas, while facing challenges in customer access and sales. While the pandemic significantly slowed our sales and service activities, this also resulted in cost reductions such as lower travel and marketing expenses. Furthermore, we were able to keep our production largely stable due to the use of our own technology in our factories and our diversified value chain.\n",
    "\n",
    "\n",
    "`Smart Infrastructure`\n",
    "Smart Infrastructure’s businesses are impacted by changes in the overall economic environment to varying degrees, depending on customer segment. Particularly in its solutions and service business, it is affected by changes in the non-residential building construction markets with a time lag of two to four quarters.\n",
    "Smart Infrastructure is experiencing a decline in orders, particularly in the solutions and services business, which previously saw a higher volume of orders. The products business showed only a slight decrease, while orders in the systems and software business stayed stable. Overall, revenue for the solutions and services business and for the systems and software business remained close to the prior- year levels. The drop in orders and revenue primarily came from Europe, Africa, the Middle East, and parts of Asia and Australia, but the Americas remained stable.\n",
    "Demand in Smart Infrastructure markets declined moderately due to COVID-19, especially in the automotive, oil and gas, and machine-building sectors. However, energy performance services and the data center market grew due to the increased focus on energy efficiency, digital services and remote work. \n",
    "Market development will be affected by lower demand from building construction, leading to a slight decline in volume of markets served by Smart Infrastructure year-over-year. Asia and Australia are expected return to normal growth, while Europe and the U.S. may see declines.\n",
    "\n",
    "<br><br>\n",
    "**Economic Environment 2021**\n",
    "\n",
    "(Siemens. (2021). Annual report 2021. Siemens AG. https://www.siemens.com/global/en/company/investor-relations/events-publications-ad-hoc/annualreports.html)\n",
    "\n",
    "`Overall economic conditions`\n",
    "In fiscal 2021, the global economy was heavily influenced by the COVID-19 pandemic and its many repercussions. After a GDP contraction in 2020, a strong recovery is expected in 2021. The economy grew rapidly in the third quarter of 2020 after the first wave of COVID-19. Despite fears of a new recession due to subsequent COVID-19 waves, economic activity had already adapted, supported by significant stimulus measures in Europe and the U.S.\n",
    "Central banks gave support with expansionary measures, while short-term interest rates were at or near zero. Accordingly, the global economy continued to expand also in the fourth quarter of calendar 2020 and the first quarter of calendar 2021, despite renewed outbreaks and lockdowns. In December 2020, the first countries approved new COVID-19 vaccines.\n",
    "However, momentum weakened in early 2021 due to rising infections and the more contagious Delta variant. Vaccination efforts lagged, especially in emerging countries, while supply disruptions affected many sectors, leading to logistics bottlenecks that disrupted value chains from raw materials to high-tech goods, especially semiconductors, and caused extraordinary disruptions in global logistics systems. High demand for goods, driven by consumer savings, contributed to increased producer and energy prices,  resulting in elevated rates of inflation. \n",
    "The Chinese economy – with the world’s largest manufacturing sector –  particularly benefited from global demand. However, tensions in the property sector and energy shortages weighed on economic activity in the second half of calendar 2021. \n",
    "Overall, major economies experienced strong rebounds, and GDP is expected to grow strongly.\n",
    "\n",
    "`Smart Infrastructure`\n",
    "Smart Infrastructure is benefitting from several positive trends, including urbanization, demographic changes, climate change, and digitalization. Urbanization and demographic shifts create a demand for smarter buildings, while climate change pushes for decarbonization and the development of flexible energy infrastructures. Digitalization enables the improvement of building efficiency and electricity management with more renewable energy sources.\n",
    "Orders for Smart Infrastructure improved across all sectors, with notable growth in the products and systems businesses, driven by demand from industrial customers and contracts from U.S. semiconductor manufacturers. Revenue also grew, although the solutions and services business saw a slight decline due to negative currency translation effects. Meanwhile, supply chain challenges were managed effectively, preventing major disruptions.\n",
    "Overall, markets served by Smart Infrastructure grew moderately in fiscal 2021, experiencing a recovery from COVID-19-related effects that had a strong impact on most customer industries a year earlier.\n",
    "Smart Infrastructure also experienced a number of supply chain constraints, especially in the areas of base metals (copper, aluminum, steel), plastics, semiconductors and transportation services. Whereas the management of these constraints required additional effort, Smart Infrastructure’s supply chains have proven to be resilient, so that major interruptions could be avoided, and delivery ability was maintained.\n",
    "Market growth is expected to accelerate, primarily fueled by demand in the pharmaceutical sector, data centers, and utilities. Growth will be strongest in Asia and Australia. Growth in the region Europe, C.I.S., Africa, and the Middle East is expected to accelerate and markets in the region Americas are expected to return to growth.\n",
    "\n",
    "<br><br>\n",
    "**Economic Environment 2022**\n",
    "\n",
    "(Siemens. (2022). Annual report 2022. Siemens AG. https://www.siemens.com/global/en/company/investor-relations/events-publications-ad-hoc/annualreports.html)\n",
    "\n",
    "`Overall economic conditions`\n",
    "In fiscal 2022, global economic growth faced major disruptions from the war in Ukraine, the effects of COVID-19, and the economic slowdown in China. After a strong recovery in 2021, GDP growth is expected to decline as the economic rebound ended abruptly. In late 2021, increased vaccinations and reduced restrictions boosted consumer spending, triggering inflationary pressures, especially in the U.S. and Europe. Supply chain issues, labor shortages, and heightened energy prices contributed to this inflation, while large stimulus packages and high household savings fueled it. \n",
    "The war in Ukraine further strained economies in early 2022, causing another significant rise in energy prices, which heavily impacted Europe and industrial sectors, especially energy-intensive industries such as chemicals. The war in Ukraine put further pressure on developing economies, especially in the Middle East, Africa, and Turkey, as both Russia and Ukraine were major exporters of grain and fertilizer before the war.\n",
    "China’s zero-COVID strategy became even more strict with the emergence of the Delta variant and the highly infectious Omicron variant, which hindered economic activity, along with regulations affecting growing sectors and a recession in real estate. Consequently, China's growth was predicted to slow. \n",
    "In response to rising inflation, the U.S. Federal Reserve raised interest rates and tightened monetary policy, followed by other central banks. This led to economic disruptions, resulting in slower-than-expected GDP growth for 2022.\n",
    "\n",
    "`Smart Infrastructure`\n",
    "In fiscal 2022, Smart Infrastructure acquired Brightly Software Inc., improving its position in software for asset and maintenance management. Smart Infrastructure’s businesses are impacted by changes in the overall economic environment to varying degrees. Especially, demand for service offerings shows only limited influence from macroeconomic cycles.\n",
    "Orders and revenue grew significantly across all businesses, especially in electrical products and electrification, driven by industrial customers, data centers, and proactive purchasing. Smart Infrastructure faced supply chain challenges but avoided major disruptions. The Americas region saw the strongest growth, while Asia and Australia were hindered by COVID-19 impacts, particularly in China. Both order and revenue development included positive currency translation effects. \n",
    "Overall, markets served by Smart Infrastructure grew clearly in fiscal 2022. Market growth is anticipated to be slightly slower due to ongoing inflation and supply chain issues, with solid demand expected in data centers and power distribution despite challenges like price increases and geopolitical tensions.\n",
    "Market development is expected to continue to be influenced by supply chain constraints and effects from the war in Ukraine, including on energy prices. Further impacts could arise from potential lockdown measures in China and geopolitical tensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\"> 📌 Modeling <a id='modeling'></a>\n",
    "\n",
    "[1. Data Loading](#data-loading-and-description)\n",
    "\n",
    "[2. Usefull Functions](#functions)\n",
    "\n",
    "[3. Model Selection per Product](#model-selection)\n",
    "- [3.1. TimeGPT](#timegpt)\n",
    "- [3.2. XGBoost](#xgboost)\n",
    "- [3.3. TCN](#tcn) \n",
    "- [3.4. Prophet](#prophet) \n",
    "- [3.5. ARIMA](#arima)\n",
    "- [3.6. Results Comparison](#compare) \n",
    "\n",
    "[4. Exogenous Variables Prediction](#market)\n",
    "\n",
    "[5. Final Modeling](#final)\n",
    "\n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**1. Data Loading**</span> <a id='data-loading-and-description'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded P1 from product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from product_dfs_folder\\P13.pkl\n",
      "Loaded P14 from product_dfs_folder\\P14.pkl\n",
      "Loaded P16 from product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from product_dfs_folder\\P5.pkl\n",
      "Loaded P6 from product_dfs_folder\\P6.pkl\n",
      "Loaded P8 from product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from product_dfs_folder\\P9.pkl\n",
      "Loaded Sales_CPI from product_dfs_folder\\Sales_CPI.pkl\n",
      "Loaded P1 from lagged_product_dfs_folder\\P1.pkl\n",
      "Loaded P11 from lagged_product_dfs_folder\\P11.pkl\n",
      "Loaded P12 from lagged_product_dfs_folder\\P12.pkl\n",
      "Loaded P13 from lagged_product_dfs_folder\\P13.pkl\n",
      "Loaded P16 from lagged_product_dfs_folder\\P16.pkl\n",
      "Loaded P20 from lagged_product_dfs_folder\\P20.pkl\n",
      "Loaded P3 from lagged_product_dfs_folder\\P3.pkl\n",
      "Loaded P36 from lagged_product_dfs_folder\\P36.pkl\n",
      "Loaded P4 from lagged_product_dfs_folder\\P4.pkl\n",
      "Loaded P5 from lagged_product_dfs_folder\\P5.pkl\n",
      "Loaded P8 from lagged_product_dfs_folder\\P8.pkl\n",
      "Loaded P9 from lagged_product_dfs_folder\\P9.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_dfs_from_folder(folder_path):\n",
    "    \"\"\"Loads DataFrames from files in a specified folder and returns a dictionary.\"\"\"\n",
    "    dfs = {}\n",
    "    # List all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pkl\"):\n",
    "            key = file_name.replace(\".pkl\", \"\")  # Extract key from the file name\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Load the dataframe from the pickle file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                dfs[key] = pickle.load(f)\n",
    "            print(f\"Loaded {key} from {file_path}\")\n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# Load both product_dfs and lagged_product_dfs from their respective folders\n",
    "product_dfs = load_dfs_from_folder(\"product_dfs_folder\")\n",
    "lagged_product_dfs = load_dfs_from_folder(\"lagged_product_dfs_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_id in product_dfs.keys():\n",
    "    product_dfs[product_id] = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "\n",
    "for product_id in lagged_product_dfs.keys():\n",
    "    lagged_product_dfs[product_id] = lagged_product_dfs[product_id].rename(columns={product_id: \"Sales\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>MAB_ELE_SHP840</th>\n",
       "      <th>PRI27276_org</th>\n",
       "      <th>PRO27826_org</th>\n",
       "      <th>MAB_ELE_PRO276</th>\n",
       "      <th>MAB_ELE_SHP1100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>35774028.5209</td>\n",
       "      <td>127.8088</td>\n",
       "      <td>109.1196</td>\n",
       "      <td>118.6708</td>\n",
       "      <td>124.2279</td>\n",
       "      <td>130.9893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>5063648.6000</td>\n",
       "      <td>117.6759</td>\n",
       "      <td>109.2248</td>\n",
       "      <td>120.4670</td>\n",
       "      <td>127.4041</td>\n",
       "      <td>132.9341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>37321267.9382</td>\n",
       "      <td>123.2801</td>\n",
       "      <td>109.3301</td>\n",
       "      <td>105.3787</td>\n",
       "      <td>120.5186</td>\n",
       "      <td>131.2613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>27090400.9380</td>\n",
       "      <td>111.0438</td>\n",
       "      <td>109.7510</td>\n",
       "      <td>107.1749</td>\n",
       "      <td>104.7763</td>\n",
       "      <td>113.0576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>34132093.4229</td>\n",
       "      <td>116.7369</td>\n",
       "      <td>109.8562</td>\n",
       "      <td>110.6476</td>\n",
       "      <td>109.5970</td>\n",
       "      <td>117.7047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sales  MAB_ELE_SHP840  PRI27276_org  PRO27826_org  MAB_ELE_PRO276  MAB_ELE_SHP1100\n",
       "month_year                                                                                           \n",
       "2018-10-01 35774028.5209        127.8088      109.1196      118.6708        124.2279         130.9893\n",
       "2018-11-01  5063648.6000        117.6759      109.2248      120.4670        127.4041         132.9341\n",
       "2018-12-01 37321267.9382        123.2801      109.3301      105.3787        120.5186         131.2613\n",
       "2019-01-01 27090400.9380        111.0438      109.7510      107.1749        104.7763         113.0576\n",
       "2019-02-01 34132093.4229        116.7369      109.8562      110.6476        109.5970         117.7047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_dfs['P1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>PRI27840_org</th>\n",
       "      <th>RohCOPPER1000_org</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>P8_lag_1</th>\n",
       "      <th>P8_lag_2</th>\n",
       "      <th>P8_lag_5</th>\n",
       "      <th>P8_lag_6</th>\n",
       "      <th>P8_lag_10</th>\n",
       "      <th>P8_ma_1</th>\n",
       "      <th>P8_ma_2</th>\n",
       "      <th>P8_ma_5</th>\n",
       "      <th>P8_ma_6</th>\n",
       "      <th>P8_ma_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>398332.2744</td>\n",
       "      <td>110.6561</td>\n",
       "      <td>75.7745</td>\n",
       "      <td>10</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>580778.2653</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>383206.7376</td>\n",
       "      <td>380139.7221</td>\n",
       "      <td>413852.9409</td>\n",
       "      <td>400303.6854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>111.0503</td>\n",
       "      <td>76.4355</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>582419.0346</td>\n",
       "      <td>518398.3785</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>742593.7831</td>\n",
       "      <td>514233.4804</td>\n",
       "      <td>497925.6504</td>\n",
       "      <td>457149.3767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>112014.9227</td>\n",
       "      <td>111.0322</td>\n",
       "      <td>76.4097</td>\n",
       "      <td>12</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>398332.2744</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>416386.5006</td>\n",
       "      <td>267418.3494</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>599435.1073</td>\n",
       "      <td>477114.1424</td>\n",
       "      <td>447197.0541</td>\n",
       "      <td>441609.0341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>540208.8029</td>\n",
       "      <td>111.0354</td>\n",
       "      <td>77.7720</td>\n",
       "      <td>13</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>1086855.2918</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>297611.6126</td>\n",
       "      <td>372627.9465</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>326111.8628</td>\n",
       "      <td>501098.4985</td>\n",
       "      <td>487629.9191</td>\n",
       "      <td>458367.1197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>491332.7200</td>\n",
       "      <td>111.2866</td>\n",
       "      <td>80.6535</td>\n",
       "      <td>14</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>540208.8029</td>\n",
       "      <td>112014.9227</td>\n",
       "      <td>368081.2008</td>\n",
       "      <td>420287.0223</td>\n",
       "      <td>361474.5342</td>\n",
       "      <td>491332.7200</td>\n",
       "      <td>515770.7614</td>\n",
       "      <td>525748.8024</td>\n",
       "      <td>499470.8688</td>\n",
       "      <td>471352.9383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Sales  PRI27840_org  RohCOPPER1000_org  time_idx  year  month     P8_lag_1     P8_lag_2    P8_lag_5    P8_lag_6   P8_lag_10      P8_ma_1     P8_ma_2     P8_ma_5     P8_ma_6    P8_ma_10\n",
       "month_year                                                                                                                                                                                                \n",
       "2019-08-01  398332.2744      110.6561            75.7745        10  2019      8  368081.2008  420287.0223 582419.0346 361474.5342 580778.2653  398332.2744 383206.7376 380139.7221 413852.9409 400303.6854\n",
       "2019-09-01 1086855.2918      111.0503            76.4355        11  2019      9  398332.2744  368081.2008 416386.5006 582419.0346 518398.3785 1086855.2918 742593.7831 514233.4804 497925.6504 457149.3767\n",
       "2019-10-01  112014.9227      111.0322            76.4097        12  2019     10 1086855.2918  398332.2744 297611.6126 416386.5006 267418.3494  112014.9227 599435.1073 477114.1424 447197.0541 441609.0341\n",
       "2019-11-01  540208.8029      111.0354            77.7720        13  2019     11  112014.9227 1086855.2918 420287.0223 297611.6126 372627.9465  540208.8029 326111.8628 501098.4985 487629.9191 458367.1197\n",
       "2019-12-01  491332.7200      111.2866            80.6535        14  2019     12  540208.8029  112014.9227 368081.2008 420287.0223 361474.5342  491332.7200 515770.7614 525748.8024 499470.8688 471352.9383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lagged_product_dfs['P8'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**2. Usefull Functions**</span> <a id='functions'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(train, val=None, test=None, outlier_treatment=True):\n",
    "\n",
    "    def winsorize_function(df, cols, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        \"\"\"Apply Winsorization only to the product sales column\"\"\"\n",
    "        df = df.copy()\n",
    "        bounds = {}\n",
    "        for col in cols:\n",
    "            q1 = df[col].quantile(lower_quantile)\n",
    "            q3 = df[col].quantile(upper_quantile)\n",
    "            df[col] = df[col].clip(lower=q1, upper=q3)\n",
    "            bounds[col] = (q1, q3)  # Store actual percentile values\n",
    "            print(f\"{col}: Winsorized Bounds -> Lower = {q1:.2f}, Upper = {q3:.2f}\")\n",
    "\n",
    "        return df, bounds\n",
    "        \n",
    "    def process_dataset(df, cols, is_train=True, bounds=None):\n",
    "        df = df.copy() \n",
    "        if is_train:\n",
    "            if outlier_treatment:\n",
    "                df, bounds = winsorize_function(df, cols)\n",
    "            else:\n",
    "                bounds = {}  \n",
    "        else:\n",
    "            if outlier_treatment:\n",
    "                if bounds is None:\n",
    "                    raise ValueError(\"Bounds must be provided for validation and test datasets.\")\n",
    "                for col in cols: \n",
    "                    if col in bounds:\n",
    "                        lower, upper = bounds[col]\n",
    "                        df[col] = df[col].clip(lower, upper)  # Corrected clipping\n",
    "        return (df, bounds) if is_train else df\n",
    "\n",
    "    # Process the training dataset\n",
    "    train, bounds = process_dataset(train, cols = train.columns, is_train=True)\n",
    "\n",
    "    # Process validation and test datasets with correct bounds\n",
    "    if val is not None:\n",
    "        val = process_dataset(val, cols = val.columns, is_train=False, bounds=bounds)\n",
    "\n",
    "    if test is not None:\n",
    "        test = process_dataset(test, cols = test.columns, is_train=False, bounds=bounds)\n",
    "        \n",
    "    # Return the datasets \n",
    "    if test is not None and val is not None:\n",
    "        return train, val, test\n",
    "    elif val is not None:\n",
    "        return train, val\n",
    "    elif test is not None:\n",
    "        return train, test\n",
    "    else:\n",
    "        return train\n",
    "\n",
    "def time_series_train_test_split(X, y, test_size=10):\n",
    "    \"\"\"Split time series data maintaining temporal order\"\"\"\n",
    "    split_idx = len(X) - test_size\n",
    "    return (\n",
    "        X.iloc[:split_idx], X.iloc[split_idx:],\n",
    "        y.iloc[:split_idx], y.iloc[split_idx:])\n",
    "\n",
    "def prepare_time_series_data(df_train, feature_set, target_col=\"Sales\", horizon=10, winsorize=False, scaling=False):\n",
    "\n",
    "    # Extract product data\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    y = data[[target_col]]  \n",
    "    X = data.drop(columns=[target_col])\n",
    "    \n",
    "    # Select features - handle empty feature_set case\n",
    "    if feature_set:\n",
    "        try:\n",
    "            # Convert feature_set to list if it's not already\n",
    "            if not isinstance(feature_set, list):\n",
    "                feature_set = [feature_set]\n",
    "            # Select only columns that exist in X\n",
    "            available_features = [f for f in feature_set if f in X.columns]\n",
    "            X = X[available_features]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Some features not found in DataFrame: {e}\")\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_val, y_train, y_val = time_series_train_test_split(X, y, test_size=horizon)\n",
    "\n",
    "    # Apply preprocessing steps\n",
    "    y_train, y_val = preprocessing_pipeline(y_train, y_val, test=None, outlier_treatment=winsorize)\n",
    "\n",
    "    # Scaling\n",
    "    if scaling: \n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        X_val_scaled = scaler_X.transform(X_val)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "        y_val_scaled = scaler_y.transform(y_val)\n",
    "        \n",
    "        # Convert back to DataFrame for easier handling\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n",
    "        y_train_scaled = pd.DataFrame(y_train_scaled, index=y_train.index, columns=y_train.columns)\n",
    "        y_val_scaled = pd.DataFrame(y_val_scaled, index=y_val.index, columns=y_val.columns)\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled\n",
    "    else:\n",
    "        return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combinations(features, max_features=None):\n",
    "    \"\"\"Generate all possible feature combinations\"\"\"\n",
    "    if max_features is None:\n",
    "        max_features = len(features)\n",
    "    \n",
    "    all_combinations = []\n",
    "    for r in range(1, max_features + 1):\n",
    "        all_combinations.extend(combinations(features, r))\n",
    "    \n",
    "    return [list(comb) for comb in all_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_df(results):\n",
    "    if results is None or len(results) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = []\n",
    "    for result in results:\n",
    "        # Assuming each item in the list is a dictionary\n",
    "        data.append({\n",
    "            'product_id': result['product_id'],\n",
    "            'winsorize': result['winsorize'],\n",
    "            'features': ', '.join(result['features']) if result['features'] else 'all',\n",
    "            'RMSE': result['metrics']['RMSE'],\n",
    "            'MAPE': result['metrics']['MAPE'],\n",
    "            'Overfit_Score': result['metrics'].get('Overfit Score', np.nan),\n",
    "            'method': result['method']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_train=None, y_train_pred=None, print_metrics=False):\n",
    "    metrics = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    }\n",
    "    \n",
    "    if y_train is not None and y_train_pred is not None:\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        metrics['Overfit Score'] = (metrics['RMSE'] - train_rmse) / max(train_rmse, 1e-10)\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"\\n=== Metrics ===\")\n",
    "        print(f\"RMSE: {metrics['RMSE']:.3f}\")\n",
    "        print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        if 'Overfit Score' in metrics:\n",
    "            print(f\"Overfit Score: {metrics['Overfit Score']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def convert_results_to_df(results):\n",
    "    \"\"\"Convert results to DataFrame format\"\"\"\n",
    "    if results is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame([{\n",
    "        'product_id': results['product_id'],\n",
    "        'winsorize': results['winsorize'],\n",
    "        'features': ', '.join(results['features']) if results['features'] else 'all',\n",
    "        'RMSE': results['metrics']['RMSE'],\n",
    "        'MAPE': results['metrics']['MAPE'],\n",
    "        'Overfit_Score': results['metrics'].get('Overfit Score', np.nan),\n",
    "        'method': results['method']\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3. Model Selection per Product**</span> <a id='model-selection'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Our pipeline is going to use:\n",
    "\n",
    "- **Walk-Forward Validation:** One-step forecasting where the model predicts the next step, and the validation window moves forward one step at a time.\n",
    "- In the cases we needed to use **GridSearch**, we use a PredefinedSplit method to ensure no cross validation and prevent messing up with the train/validation timeline.\n",
    "\n",
    "➡️ For Feature Combinations we will iterate over:\n",
    "\n",
    "- No macro/lags features (baseline model).\n",
    "- Single-feature models.\n",
    "- Multi-feature models.\n",
    "\n",
    "`IMPORTANT:` For feature selection, we must be extremely careful because our dataset is limited—we only have 43 monthly data points per product. This small sample size restricts our modeling options. For initial validation, we’ll use a 7-month forecast horizon because TimeGPT requires at least 36 data points to perform well in a walk-forward validation setup. To ensure fair comparisons across all models, we’ll maintain this 7-month horizon, even though our final goal is to predict 10 months ahead. Given the scarcity of data, we must also limit the number of features to avoid overfitting. Including too many variables could cause the model to capture noise rather than true patterns, reducing its accuracy on unseen data.\n",
    "\n",
    "➡️ Models\n",
    "\n",
    "We will tune hyperparameters for all the models for each product, with the best features.\n",
    "\n",
    "\n",
    "**Time Series Model Comparison Table**\n",
    "\n",
    "| Feature               | TimeGPT (Zero-Shot) | XGBoost            | TCN                | ARIMA              | Prophet (Facebook)     |\n",
    "|-----------------------|---------------------|--------------------|--------------------|--------------------|--------------------|\n",
    "| **Scaling Needed?**   | ❌ No               | ❌ No              | ✅ Yes             | ❌ No (if stationary) | ❌ No (logistic/linear trends handle scaling internally)            |\n",
    "| **Handles Trends**    | ✅ Excellent        | ✅ (With features) | ✅ Excellent       | ✅ (With differencing) | ✅ Piecewise linear/logistic     |\n",
    "| **Handles Seasonality** | ✅ Automatic       | ❌ (Requires manual lags) | ✅ (With large receptive field) | ✅ (SARIMA) | ✅ Fourier terms (fixed but interpretable)      |\n",
    "| **Multivariate Support** | ✅ Yes            | ✅ Yes             | ✅ Yes             | ❌ Univariate only | Limited            |\n",
    "| **Training Speed**    | ⚡ Instant (pre-trained) | 🏎️ Fast          | 🐢 Slow (GPU helps) | 🏃 Moderate        |  🏎️ Fast          |\n",
    "| **Interpretability**  | ❌ Black box        | ✅ Feature importances | ❌ Black box     | ✅ Model coefficients | ✅ Best-in-class (clear trend/seasonality plots) |\n",
    "| **Best For**          | Zero-shot forecasting | Tabular data with temporal features | Long-range dependencies | Simple univariate series | Business-friendly forecasts with explainability |\n",
    "| **Weakness**          | Limited control     | Poor with long sequences | Computationally heavy | Linear assumptions only | Struggles with short-term dependencies (no AR terms)     |\n",
    "\n",
    "\n",
    "1. **TimeGPT** (by Nixtla):\n",
    "   - Pros: No training needed, handles any frequency, great for quick benchmarks.\n",
    "   - Cons: Proprietary, limited customization.\n",
    "\n",
    "2. **XGBoost**:\n",
    "   - Best when you have:\n",
    "     - Easy and fast. \n",
    "\n",
    "3. **TCN** (Temporal Convolutional Networks):\n",
    "   - Pros: Captures long-range patterns better than LSTMs.\n",
    "   - Cons: Needs careful tuning of and is more computationaly expensive.\n",
    "\n",
    "4. **ARIMA**:\n",
    "   - Important:\n",
    "     - Data is stationary (or differenced) \n",
    "     - Short-term forecasts (≤12 steps)\n",
    "   - Warning: Fails miserably with:\n",
    "     - Non-linear trends\n",
    "     - High-frequency data, wich is ok, bc we are dealing with months.\n",
    "\n",
    "5. **Prophet**:\n",
    "   - Faster and more interpretable than NeuralProphet.\n",
    "   - No auto-regression (unlike NeuralProphet’s AR-Net).\n",
    "   -Less flexible seasonality (fixed Fourier terms vs. NN-backed).\n",
    "\n",
    "\n",
    "➡️ Summary:\n",
    "\n",
    "- **We will Scale**: TCN, Neural Prophet\n",
    "- **Optional Scaling**: XGBoost (for extreme values)\n",
    "- **We will not Scale**: ARIMA (but difference if non-stationary!), TimeGPT\n",
    "\n",
    "1. We will start with **TimeGPT** for a baseline.\n",
    "2. Then test **XGBoost** with lag features.\n",
    "3. Then **TCN** to see he effect for more complex patterns.\n",
    "4. **Prophet** and **ARIMA** for interpretability.\n",
    "\n",
    "➡️ Evaluation Metrics: \n",
    "- RMSE → Measures error magnitude.\n",
    "- MAPE → Percentage-based error, useful for business impact.\n",
    "- R² → Measures how well the model explains variance.\n",
    "- Overfit Score →  \n",
    "\n",
    "$$\n",
    "\\text{Overfit Score} = \\frac{\\text{Test RMSE} - \\text{Train RMSE}}{\\text{Train RMSE}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "    - < 0.1 (Underfit)\n",
    "    - 0.1 - 0.5 (Good Fit)\n",
    "    - > 0.5 (Overfit Warning!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.1. TimeGPT**</span> <a id='timegpt'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Happy Forecasting! :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nixtla_client = NixtlaClient(\n",
    "    api_key = \"nixak-CIwSKQ0cRLIuFR1TYllLFVakTGx3WCY30YPEKfxG0lDQcE0akGo3GE4aMJO9XXbkKjdFaGDP5x6uSxQ6\"\n",
    ")\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_timegpt(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                              nixtla_client=None, winsorize=False, \n",
    "                              horizon=7):\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if nixtla_client is None:\n",
    "        raise ValueError(\"NixtlaClient instance is required.\")\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, feature_set, target_col, horizon, winsorize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    " \n",
    "    # --- Determine Feature Types ---\n",
    "    hist_exog = [f for f in feature_set if '_lag_' in f or '_ma_' in f] if feature_set else []\n",
    "    fut_exog = [f for f in feature_set if f not in hist_exog] if feature_set else []\n",
    "\n",
    "\n",
    "    # --- Prepare Future Exogenous Features ---\n",
    "    X_future = None\n",
    "    if fut_exog:\n",
    "        X_future = X_val[fut_exog].copy()\n",
    "        if not X_future.isnull().all().all():  \n",
    "            X_future.insert(0, 'month_year', X_val.index)\n",
    "        else:\n",
    "            X_future = None  # Avoid passing empty DataFrame\n",
    "\n",
    "    # --- Simple Forecast (12 ≤ obs < 36) ---\n",
    "    if len(X_train) < 36:\n",
    "        print(f\"Using simple forecast ({len(X_train)} < 36 obs)\")\n",
    "        try:\n",
    "            history = df_train.copy()\n",
    "            if feature_set:\n",
    "                history = history[feature_set + [target_col]]\n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history.reset_index(),\n",
    "                                              time_col=\"month_year\", target_col=target_col, h=horizon,\n",
    "                                              X_df=X_future,  # Always pass X_future if available\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)\n",
    "            \n",
    "            # Get training predictions\n",
    "            train_pred = nixtla_client.forecast(df=history.reset_index(),time_col=\"month_year\",\n",
    "                                                target_col=target_col,h=1,hist_exog_list=hist_exog if hist_exog else None\n",
    "                                                ).iloc[0][\"TimeGPT\"]\n",
    "            \n",
    "            train_predictions = [train_pred] * len(history)\n",
    "            \n",
    "            return {\n",
    "                'product_id': product_id,'winsorize': winsorize,'features': feature_set,\n",
    "                'metrics': calculate_metrics(y_true=y_val,y_pred=forecast[\"TimeGPT\"],\n",
    "                                             y_train=history[target_col],y_train_pred=train_predictions),\n",
    "                'method': 'simple','validation_predictions': forecast[\"TimeGPT\"]}\n",
    "        except Exception as e:\n",
    "            print(f\"Simple forecast failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # --- Walk-Forward (≥36 obs) ---\n",
    "    print(f\"Using walk-forward ({len(X_train)} ≥ 36 obs)\")\n",
    "    predictions = []\n",
    "    history = X_train.copy()\n",
    "    history[target_col] = y_train.values\n",
    "    history = history.reset_index()\n",
    "    \n",
    "    # Get training predictions\n",
    "    try:\n",
    "        train_fit = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,h=1,\n",
    "                                           hist_exog_list=hist_exog if hist_exog else None)\n",
    "        train_predictions = [train_fit[\"TimeGPT\"].iloc[0]] * len(y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Training pred failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = None\n",
    "            if fut_exog:\n",
    "                X_future_step = X_val.iloc[[i]][fut_exog].copy()\n",
    "                if not X_future_step.isnull().all().all():  \n",
    "                    X_future_step.insert(0, 'month_year', X_val.index[i])\n",
    "                else:\n",
    "                    X_future_step = None  \n",
    "\n",
    "            forecast = nixtla_client.forecast(df=history,time_col='month_year',target_col=target_col,\n",
    "                                              h=1, X_df=X_future_step if fut_exog else None, # FUTURE features (macro)\n",
    "                                              hist_exog_list=hist_exog if hist_exog else None)  # HISTORICAL features (lags, moving averages)\n",
    "            \n",
    "            pred = forecast[\"TimeGPT\"].iloc[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            new_row = X_val.iloc[i].copy()\n",
    "            new_row['month_year'] = X_val.index[i]\n",
    "            new_row[target_col] = pred\n",
    "            history = pd.concat([history, pd.DataFrame([new_row])])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {'product_id': product_id,'winsorize': winsorize,'features': feature_set,'metrics': metrics,\n",
    "            'method': 'walkforward','validation_predictions': predictions}\n",
    "\n",
    "\n",
    "def find_best_timegpt_config(product_id, df_train, nixtla_client, lagged_df=None, target_col=\"Sales\"):\n",
    "\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    print(normal_feature_combinations)\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                result = choose_parameters_timegpt(\n",
    "                    product_id=product_id,df_train=data, feature_set=features, target_col=target_col,\n",
    "                    nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    result = choose_parameters_timegpt(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,nixtla_client=nixtla_client,winsorize=winsorize)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_timegpt_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,\n",
    "            nixtla_client=nixtla_client,target_col=\"Sales\")\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "\n",
    "all_results = []  # List to store all results\n",
    "best_configs = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_timegpt, results_timegpt = process_product_parallel(product_id)\n",
    "        \n",
    "        if best_config_timegpt is not None:\n",
    "            best_configs.append(best_config_timegpt)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_timegpt is not None:\n",
    "            all_results.extend(results_timegpt)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n",
    "\n",
    "df_all_results = convert_results_to_df(all_results) if all_results else pd.DataFrame()\n",
    "df_best_configs = pd.DataFrame(best_configs) if best_configs else pd.DataFrame()\n",
    "\n",
    "df_all_results.to_csv(\"timegpt_best_configs.csv\", index=False)\n",
    "df_best_configs.to_csv(\"timegpt_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.2. XGBoost**</span> <a id='xgboost'></a>  \n",
    "\n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_xgboost(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                            param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized XGBoost forecasting with proper walk-forward validation\"\"\"\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize)\n",
    "        # Verify temporal ordering\n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Optimized Parameter Grid for Small Data ---\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [2, 3],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'subsample': [0.9, 1.0],\n",
    "            'colsample_bytree': [0.9, 1.0],\n",
    "            'min_child_weight': [1, 3]}\n",
    "\n",
    "    # --- Efficient Model Setup ---\n",
    "    base_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',random_state=42,n_jobs=1,tree_method='hist',  # Faster training\n",
    "        enable_categorical=False,gamma=0,reg_alpha=0,reg_lambda=1)\n",
    "\n",
    "    # --- Strict Temporal Validation ---\n",
    "    test_fold = np.array([-1] * len(X_train) + [0] * len(X_val))\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "\n",
    "    # --- Focused Grid Search ---\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,param_grid=param_grid,scoring='neg_root_mean_squared_error',cv=ps,n_jobs=n_jobs,verbose=1,refit=True)\n",
    "\n",
    "    # --- Training with Validation ---\n",
    "    X_combined = pd.concat([X_train, X_val])\n",
    "    y_combined = pd.concat([y_train, y_val])\n",
    "    \n",
    "    print(f\"Starting grid search for {product_id}...\")\n",
    "    try:\n",
    "        grid_search.fit(X_combined, y_combined)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        print(f\"Grid search failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # --- Robust Walk-forward Validation ---\n",
    "    predictions = []\n",
    "    print(f\"Using walk-forward (n_train={len(X_train)}, horizon={horizon})\")\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            # Create expanding window\n",
    "            window_X = pd.concat([X_train, X_val.iloc[:i]])\n",
    "            window_y = pd.concat([y_train, y_val.iloc[:i]])\n",
    "            \n",
    "            # Correct XGBoost 2.1.3+ fitting\n",
    "            best_model.fit(window_X, window_y, eval_set=[(X_val.iloc[[i]], y_val.iloc[[i]])], verbose=0)\n",
    "    \n",
    "            pred = best_model.predict(X_val.iloc[[i]]).item()\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)} - using fallback\")\n",
    "            fallback = (predictions[-1] if len(predictions) > 0 else \n",
    "                       y_train.iloc[-1] if len(y_train) > 0 else np.nan)\n",
    "            predictions.append(fallback)\n",
    "\n",
    "    # --- Training Metrics ---\n",
    "    try:\n",
    "        train_predictions = best_model.predict(X_train).tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Training predictions failed: {str(e)} - using last value\")\n",
    "        train_predictions = [y_train.iloc[-1]] * len(y_train)\n",
    "\n",
    "    # --- Comprehensive Metrics ---\n",
    "    metrics = calculate_metrics(y_true=y_val,y_pred=predictions,y_train=y_train,y_train_pred=train_predictions)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'validation_predictions': predictions,\n",
    "        'feature_importances': dict(zip(feature_set, best_model.feature_importances_))}\n",
    "\n",
    "\n",
    "def find_best_xgboost_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = [[]] + normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_xgboost(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_xgboost(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "\n",
    "        best_config, results = find_best_xgboost_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "all_results_xgboost = []  # List to store all results\n",
    "best_configs_xgboost = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_xgboost, results_xgboost = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_xgboost is not None:\n",
    "            best_configs_xgboost.append(best_config_xgboost)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_xgboost is not None:\n",
    "            all_results_xgboost.extend(results_xgboost)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_all_results_xgboost = convert_results_to_df(all_results_xgboost) if all_results_xgboost else pd.DataFrame()\n",
    "df_best_configs_xgboost = pd.DataFrame(best_configs_xgboost) if best_configs_xgboost else pd.DataFrame()\n",
    "\n",
    "df_all_results_xgboost.to_csv(\"xgboost_results.csv\", index=False)\n",
    "df_best_configs_xgboost.to_csv(\"xgboost_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.3. TCN**</span> <a id='tcn'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_tcn(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                         param_grid=None, n_jobs=-1, winsorize=False, horizon=7):\n",
    "    \"\"\"Optimized TCN forecasting with walk-forward validation\"\"\"\n",
    "    \n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize, scaling=True)\n",
    "        \n",
    "        assert X_train.index[-1] < X_val.index[0], \"Validation data must be after training data\"\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Reshape data for TCN input (samples, timesteps, features)\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    y_train = y_train.values.reshape(-1, 1)\n",
    "    y_val = y_val.values.reshape(-1, 1)\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'nb_filters': [16, 32],\n",
    "            'kernel_size': [2, 3],\n",
    "            'dilations': [[1, 2, 4], [1, 2, 4, 8]],\n",
    "            'dropout_rate': [0.1, 0.2],\n",
    "            'return_sequences': [False]  # single-step prediction\n",
    "}\n",
    "    \n",
    "    def build_tcn(nb_filters=32, kernel_size=2, dilations=[1, 2, 4], \n",
    "                 dropout_rate=0.1, return_sequences=False):\n",
    "        inputs = Input(shape=(1, X_train.shape[2]))\n",
    "        x = TCN(nb_filters=nb_filters, kernel_size=kernel_size, \n",
    "               dilations=dilations, dropout_rate=dropout_rate,\n",
    "               return_sequences=return_sequences)(inputs)\n",
    "        outputs = Dense(1)(x)\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        return model\n",
    "    \n",
    "    best_params = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for filters in param_grid['nb_filters']:\n",
    "        for kernel in param_grid['kernel_size']:\n",
    "            for dilations in param_grid['dilations']:\n",
    "                for dropout in param_grid['dropout_rate']:\n",
    "                    for return_seq in param_grid['return_sequences']:\n",
    "                        model = build_tcn(filters, kernel, dilations, dropout, return_seq)\n",
    "                        try:\n",
    "                            model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "                            \n",
    "                            # Walk-forward validation (predict one step at a time)\n",
    "                            predictions = []\n",
    "                            history_X = X_train.copy()\n",
    "                            history_y = y_train.copy()\n",
    "                            \n",
    "                            for i in range(len(X_val)):\n",
    "                                # Predict next step\n",
    "                                X_next = X_val[i].reshape(1, 1, -1)\n",
    "                                pred = model.predict(X_next, verbose=0).flatten()[0]\n",
    "                                predictions.append(pred)\n",
    "                                \n",
    "                            # Calculate RMSE\n",
    "                            if len(y_val) == len(predictions):\n",
    "                                rmse = np.sqrt(np.mean((y_val.flatten() - predictions) ** 2))\n",
    "                            else:\n",
    "                                print(f\"Shape mismatch: y_val ({y_val.shape}), preds ({len(predictions)})\")\n",
    "                                continue\n",
    "                            \n",
    "                            if rmse < best_rmse:\n",
    "                                best_rmse = rmse\n",
    "                                best_params = {\n",
    "                                    'nb_filters': filters,\n",
    "                                    'kernel_size': kernel,\n",
    "                                    'dilations': dilations,\n",
    "                                    'dropout_rate': dropout,\n",
    "                                    'return_sequences': return_seq\n",
    "                                }\n",
    "                        except Exception as e:\n",
    "                            print(f\"Hyperparameter set failed: {str(e)}\")\n",
    "\n",
    "    if best_params is None:\n",
    "        print(f\"❌ No valid models for {product_id}\")\n",
    "        return None\n",
    "\n",
    "    # Final model training and walk-forward prediction\n",
    "    final_model = build_tcn(**best_params)\n",
    "    final_model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=0)\n",
    "    predictions = []\n",
    "\n",
    "    # FIXED: Create DataFrame with both features and target\n",
    "    history_data = np.column_stack([X_train[:, 0, :], y_train.flatten()])\n",
    "    history = pd.DataFrame(history_data, columns=feature_set + [target_col])\n",
    "    history.index = range(len(history))\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            X_future_step = X_val[i].reshape(1, 1, -1)\n",
    "            pred = final_model.predict(X_future_step, verbose=0).flatten()[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update training data\n",
    "            new_row = X_val[i, 0, :].tolist()\n",
    "            new_row.append(pred)\n",
    "            new_df = pd.DataFrame([new_row], columns=feature_set + [target_col])\n",
    "            history = pd.concat([history, new_df]).reset_index(drop=True)\n",
    "            \n",
    "            # Retrain on expanded window\n",
    "            rolling_window = 36 if len(history) > 36 else len(history)\n",
    "            train_subset = history.iloc[-rolling_window:]\n",
    "            \n",
    "            X_train_update = train_subset[feature_set].values.reshape(-1, 1, len(feature_set))\n",
    "            y_train_update = train_subset[target_col].values.reshape(-1, 1)\n",
    "            \n",
    "            final_model.fit(X_train_update, y_train_update, epochs=5, batch_size=16, verbose=0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(predictions[-1] if predictions else y_train[-1][0])\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val.flatten(), y_pred=predictions, y_train=y_train.flatten())\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id,\n",
    "        'winsorize': winsorize,\n",
    "        'features': feature_set,\n",
    "        'metrics': metrics,\n",
    "        'method': 'walkforward',\n",
    "        'best_params': best_params,\n",
    "        'validation_predictions': predictions\n",
    "    }\n",
    "\n",
    "def find_best_tcn_config(product_id, df_train, lagged_df=None, target_col=\"Sales\", \n",
    "                            n_jobs=-1, custom_param_grids=None):\n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    normal_feature_combinations = normal_feature_combinations\n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                # Use custom param grid if provided for this feature set\n",
    "                param_grid = None\n",
    "                if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                    param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                result = choose_parameters_tcn(product_id=product_id,df_train=data,feature_set=features,\n",
    "                    target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)  # Just lags\n",
    "            full_combinations.append(lag_combo + macro_features)  # Lags + macros\n",
    "            full_combinations.append(lag_combo + ma_features)  # Lags + MAs\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)  # All\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    # Use custom param grid if provided for this feature set\n",
    "                    param_grid = None\n",
    "                    if custom_param_grids and tuple(features) in custom_param_grids:\n",
    "                        param_grid = custom_param_grids[tuple(features)]\n",
    "\n",
    "                    result = choose_parameters_tcn(product_id=product_id,df_train=lagged_data,feature_set=features,\n",
    "                                                       target_col=target_col,winsorize=winsorize,n_jobs=n_jobs,param_grid=param_grid)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Best params: {best_config['best_params']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_parallel(product_id, custom_param_grids=None):\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "        best_config, results = find_best_tcn_config(product_id=product_id,df_train=df_train,lagged_df=lagged_df,target_col=\"Sales\", custom_param_grids=custom_param_grids)\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing product: P1\n",
      "\n",
      "Testing - Winsorize: True, Features: ['MAB_ELE_SHP840']\n",
      "Preparing data...\n",
      "Sales: Winsorized Bounds -> Lower = 12773011.92, Upper = 41955264.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing product: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     best_config_tcn, results_tcn \u001b[38;5;241m=\u001b[39m process_product_parallel(product_id, custom_param_grids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_config_tcn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m         best_configs_tcn\u001b[38;5;241m.\u001b[39mappend(best_config_tcn)  \u001b[38;5;66;03m# Store best config\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 249\u001b[0m, in \u001b[0;36mprocess_product_parallel\u001b[1;34m(product_id, custom_param_grids)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Some index values could not be parsed into datetime!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    248\u001b[0m     lagged_df \u001b[38;5;241m=\u001b[39m lagged_product_dfs\u001b[38;5;241m.\u001b[39mget(product_id, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 249\u001b[0m     best_config, results \u001b[38;5;241m=\u001b[39m find_best_tcn_config(product_id\u001b[38;5;241m=\u001b[39mproduct_id,df_train\u001b[38;5;241m=\u001b[39mdf_train,lagged_df\u001b[38;5;241m=\u001b[39mlagged_df,target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSales\u001b[39m\u001b[38;5;124m\"\u001b[39m, custom_param_grids\u001b[38;5;241m=\u001b[39mcustom_param_grids)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_config, results\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[15], line 165\u001b[0m, in \u001b[0;36mfind_best_tcn_config\u001b[1;34m(product_id, df_train, lagged_df, target_col, n_jobs, custom_param_grids)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_param_grids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(features) \u001b[38;5;129;01min\u001b[39;00m custom_param_grids:\n\u001b[0;32m    163\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m custom_param_grids[\u001b[38;5;28mtuple\u001b[39m(features)]\n\u001b[1;32m--> 165\u001b[0m result \u001b[38;5;241m=\u001b[39m choose_parameters_tcn(product_id\u001b[38;5;241m=\u001b[39mproduct_id,df_train\u001b[38;5;241m=\u001b[39mdata,feature_set\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    166\u001b[0m     target_col\u001b[38;5;241m=\u001b[39mtarget_col,winsorize\u001b[38;5;241m=\u001b[39mwinsorize,n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,param_grid\u001b[38;5;241m=\u001b[39mparam_grid)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Success - RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 65\u001b[0m, in \u001b[0;36mchoose_parameters_tcn\u001b[1;34m(product_id, df_train, feature_set, target_col, param_grid, n_jobs, winsorize, horizon)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_val)):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Predict next step\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     X_next \u001b[38;5;241m=\u001b[39m X_val[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_next, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     66\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate RMSE\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:499\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    497\u001b[0m ):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    500\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    501\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    502\u001b[0m         steps_per_epoch\u001b[38;5;241m=\u001b[39msteps,\n\u001b[0;32m    503\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    504\u001b[0m         distribute_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    505\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:720\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 720\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_adapter\u001b[38;5;241m.\u001b[39mget_tf_dataset()\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    722\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    723\u001b[0m         dataset\n\u001b[0;32m    724\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    233\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[1;32m--> 235\u001b[0m dataset \u001b[38;5;241m=\u001b[39m slice_inputs(indices_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs)\n\u001b[0;32m    237\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[0;32m    238\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    239\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:214\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[1;34m(indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtraverse(grab_one, data)\n\u001b[1;32m--> 214\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    215\u001b[0m     grab_batch, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization & deserialization\u001b[39;00m\n\u001b[0;32m    220\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m map_op\u001b[38;5;241m.\u001b[39m_map_v2(\n\u001b[0;32m   2342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2343\u001b[0m     map_func,\n\u001b[0;32m   2344\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m   2345\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m   2346\u001b[0m     synchronous\u001b[38;5;241m=\u001b[39msynchronous,\n\u001b[0;32m   2347\u001b[0m     use_unbounded_threadpool\u001b[38;5;241m=\u001b[39muse_unbounded_threadpool,\n\u001b[0;32m   2348\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   2349\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:57\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`synchronous` is not supported with `num_parallel_calls`, but\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `num_parallel_calls` was set to \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m       num_parallel_calls,\n\u001b[0;32m     56\u001b[0m   )\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     58\u001b[0m     input_dataset,\n\u001b[0;32m     59\u001b[0m     map_func,\n\u001b[0;32m     60\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mnum_parallel_calls,\n\u001b[0;32m     61\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[0;32m     62\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     63\u001b[0m     use_unbounded_threadpool\u001b[38;5;241m=\u001b[39muse_unbounded_threadpool,\n\u001b[0;32m     64\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:218\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_unbounded_threadpool \u001b[38;5;241m=\u001b[39m use_unbounded_threadpool\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m--> 218\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m    219\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m    221\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    222\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m    223\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic,\n\u001b[0;32m    224\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m    225\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m    226\u001b[0m     use_unbounded_threadpool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_unbounded_threadpool,\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\catar\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5866\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, use_unbounded_threadpool, metadata, name)\u001b[0m\n\u001b[0;32m   5864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   5865\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5866\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   5867\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParallelMapDatasetV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, input_dataset, other_arguments,\n\u001b[0;32m   5868\u001b[0m       num_parallel_calls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_types\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_types,\n\u001b[0;32m   5869\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_shapes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_inter_op_parallelism\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5870\u001b[0m       use_inter_op_parallelism, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeterministic\u001b[39m\u001b[38;5;124m\"\u001b[39m, deterministic,\n\u001b[0;32m   5871\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_cardinality\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_cardinality,\n\u001b[0;32m   5872\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_unbounded_threadpool\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_unbounded_threadpool, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5873\u001b[0m       metadata)\n\u001b[0;32m   5874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   5875\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "all_results_tcn = []  # List to store all results\n",
    "best_configs_tcn = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_tcn, results_tcn = process_product_parallel(product_id, custom_param_grids=None)\n",
    "        \n",
    "        if best_config_tcn is not None:\n",
    "            best_configs_tcn.append(best_config_tcn)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_tcn is not None:\n",
    "            all_results_tcn.extend(results_tcn)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.4. Prophet**</span> <a id='prophet'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_specific_config(product_id):\n",
    "    \"\"\"Returns Prophet configuration tailored to each product's time series characteristics\"\"\"\n",
    "    product_configs = {\n",
    "        'P1': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.05,  \n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'extra_regressors': []},\n",
    "        'P3': {\n",
    "            'seasonality_mode': 'multiplicative',\n",
    "            'yearly_seasonality': True,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 15.0, \n",
    "            'extra_regressors': []},\n",
    "        'P4': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonity': False,\n",
    "            'changepoint_prior_scale': 0.05,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'semiannual',\n",
    "                'period': 6,\n",
    "                'fourier_order': 3}]},\n",
    "        'P5': {\n",
    "            'seasonality_mode': 'multiplicative',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.05,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'semiannual',\n",
    "                'period': 6,\n",
    "                'fourier_order': 3}]},\n",
    "        'P6': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.05,  # gradual changes\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'extra_regressors': []},\n",
    "        'P8': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.2,\n",
    "            'seasonality_prior_scale': 15.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'quarterly',\n",
    "                'period': 3,\n",
    "                'fourier_order': 3}]},\n",
    "        'P9': {\n",
    "            'seasonality_mode': 'multiplicative',\n",
    "            'yearly_seasonality': True,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 15.0,  # Strong yearly anti-correlation\n",
    "            'extra_regressors': []},\n",
    "        'P11': {\n",
    "            'seasonality_mode': 'multiplicative',\n",
    "            'yearly_seasonality': True,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.15,\n",
    "            'seasonality_prior_scale': 20.0,  # Very strong yearly dependency\n",
    "            'extra_regressors': []},\n",
    "        'P12': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.3,  # For strong short-term dependencies\n",
    "            'seasonality_prior_scale': 5.0,\n",
    "            'extra_regressors': []},\n",
    "        'P13': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 15.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'custom_17',\n",
    "                'period': 17,\n",
    "                'fourier_order': 3}]},\n",
    "        'P14': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.05,\n",
    "            'seasonality_prior_scale': 10.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'semiannual',\n",
    "                'period': 6,\n",
    "                'fourier_order': 3}]},\n",
    "        'P16': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.3,  # For strong ARMA(3,3) patterns\n",
    "            'seasonality_prior_scale': 5.0,\n",
    "            'extra_regressors': []},\n",
    "        'P20': {\n",
    "            'seasonality_mode': 'additive',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 15.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'custom_13',\n",
    "                'period': 13,\n",
    "                'fourier_order': 3}]},\n",
    "        'P36': {\n",
    "            'seasonality_mode': 'multiplicative',\n",
    "            'yearly_seasonality': False,\n",
    "            'weekly_seasonality': False,\n",
    "            'daily_seasonality': False,\n",
    "            'changepoint_prior_scale': 0.1,\n",
    "            'seasonality_prior_scale': 15.0,\n",
    "            'extra_seasonalities': [{\n",
    "                'name': 'custom_10',\n",
    "                'period': 10,\n",
    "                'fourier_order': 3}]}}\n",
    "    \n",
    "    # Default configuration \n",
    "    default_config = {\n",
    "        'seasonality_mode': 'additive',\n",
    "        'yearly_seasonality': 'auto',\n",
    "        'weekly_seasonality': False,\n",
    "        'daily_seasonality': False,\n",
    "        'changepoint_prior_scale': 0.05,\n",
    "        'seasonality_prior_scale': 10.0,\n",
    "        'extra_regressors': []}\n",
    "    \n",
    "    return product_configs.get(product_id, default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_prophet(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                             winsorize=False, horizon=7):\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(\n",
    "            df_train, feature_set, target_col, horizon, winsorize\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Get product-specific configuration\n",
    "    config = get_product_specific_config(product_id)\n",
    "    \n",
    "    # Prepare Prophet-compatible DataFrames\n",
    "    train_df = pd.DataFrame({\n",
    "        'ds': X_train.index.to_numpy(),\n",
    "        'y': y_train.values.flatten()\n",
    "    })\n",
    "    \n",
    "    # Add regressors if any\n",
    "    regressors = []\n",
    "    if feature_set:\n",
    "        for feature in feature_set:\n",
    "            if feature in X_train.columns:\n",
    "                train_df[feature] = X_train[feature].values.reshape(-1)\n",
    "                regressors.append(feature)\n",
    "    \n",
    "    # --- Model Configuration Function ---\n",
    "    def create_prophet_model():\n",
    "        \"\"\"Helper to create a fresh Prophet instance with current config\"\"\"\n",
    "        model = Prophet(seasonality_mode=config['seasonality_mode'], yearly_seasonality=config['yearly_seasonality'], weekly_seasonality=config['weekly_seasonality'],\n",
    "            daily_seasonality=config['daily_seasonality'], changepoint_prior_scale=config['changepoint_prior_scale'], seasonality_prior_scale=config['seasonality_prior_scale'])\n",
    "        \n",
    "        # custom seasonalities\n",
    "        if 'extra_seasonalities' in config:\n",
    "            for seasonality in config['extra_seasonalities']:\n",
    "                model.add_seasonality(name=seasonality['name'], period=seasonality['period'], fourier_order=seasonality['fourier_order'])\n",
    "        \n",
    "        # Add regressors\n",
    "        for reg in regressors:\n",
    "            model.add_regressor(reg)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # --- Simple Forecast (12 ≤ obs < 36) ---\n",
    "    if len(X_train) < 36:\n",
    "        print(f\"Using simple forecast ({len(X_train)} < 36 obs)\")\n",
    "        try:\n",
    "            model = create_prophet_model()\n",
    "            model.fit(train_df)\n",
    "            \n",
    "            future = model.make_future_dataframe(periods=horizon, freq='M')\n",
    "            \n",
    "            if regressors and len(X_val) >= horizon:\n",
    "                for reg in regressors:\n",
    "                    future[reg] = X_val[reg].iloc[:horizon].values.reshape(-1)\n",
    "            \n",
    "            forecast = model.predict(future)\n",
    "            val_predictions = forecast['yhat'].values[-horizon:].flatten()\n",
    "            train_predictions = forecast['yhat'].values[:-horizon].flatten() if horizon > 0 else forecast['yhat'].values.flatten()\n",
    "            \n",
    "            return {\n",
    "                'product_id': product_id,\n",
    "                'winsorize': winsorize,\n",
    "                'features': feature_set,\n",
    "                'metrics': calculate_metrics(y_true=y_val.values.flatten(), y_pred=val_predictions, y_train=y_train.values.flatten(), y_train_pred=train_predictions),\n",
    "                'method': 'simple',\n",
    "                'validation_predictions': val_predictions,\n",
    "                'model_config': config}\n",
    "        except Exception as e:\n",
    "            print(f\"Simple forecast failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # --- Walk-Forward (≥36 obs) ---\n",
    "    print(f\"Using walk-forward ({len(X_train)} ≥ 36 obs)\")\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    history = train_df.copy()\n",
    "    history = history.convert_dtypes()\n",
    "\n",
    "    # Get training predictions\n",
    "    try:\n",
    "        train_model = create_prophet_model()\n",
    "        train_model.fit(history)\n",
    "        future_train = train_model.make_future_dataframe(periods=0, freq='M')\n",
    "        if regressors:\n",
    "            for reg in regressors:\n",
    "                future_train[reg] = history[reg].values.astype(float)  # Explicit float conversion\n",
    "        train_forecast = train_model.predict(future_train)\n",
    "        train_predictions = train_forecast['yhat'].values.flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Training pred failed: {str(e)} - using last value\")\n",
    "        train_predictions = np.full(len(y_train), y_train.iloc[-1])\n",
    "\n",
    "    # Walk-forward validation\n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            # Create fresh model\n",
    "            step_model = create_prophet_model()\n",
    "            \n",
    "            # Ensure numeric types\n",
    "            history = history.infer_objects()\n",
    "            \n",
    "            # Fit model\n",
    "            step_model.fit(history)\n",
    "            \n",
    "            # Prepare one-step forecast\n",
    "            future_step = step_model.make_future_dataframe(periods=1, include_history=False, freq='M')\n",
    "            \n",
    "            # Add regressors with type checking\n",
    "            if regressors and i < len(X_val):\n",
    "                for reg in regressors:\n",
    "                    if not np.issubdtype(X_val[reg].dtype, np.number):\n",
    "                        raise TypeError(f\"Regressor {reg} contains non-numeric values\")\n",
    "                    future_step[reg] = float(X_val[reg].iloc[i])  # Explicit float conversion\n",
    "            \n",
    "            # Make prediction\n",
    "            forecast_step = step_model.predict(future_step)\n",
    "            pred = float(forecast_step['yhat'].iloc[0])  # Explicit float conversion\n",
    "            predictions.append(pred)\n",
    "            actual = float(y_val.iloc[i]) if i < len(y_val) else np.nan\n",
    "            actuals.append(actual)\n",
    "            \n",
    "            # Update history \n",
    "\n",
    "            new_row = {'ds': X_val.index[i], 'y': actual}\n",
    "            if regressors and i < len(X_val):\n",
    "                for reg in regressors:\n",
    "                    new_row[reg] = float(X_val[reg].iloc[i])\n",
    "            \n",
    "            history = pd.concat([\n",
    "                history, \n",
    "                pd.DataFrame([new_row]).infer_objects()], ignore_index=True).convert_dtypes()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(float(y_train.iloc[-1]))\n",
    "            actuals.append(float(y_val.iloc[i]) if i < len(y_val) else np.nan)\n",
    "    \n",
    "    # Calculate metrics using actual observed values\n",
    "    metrics = calculate_metrics(\n",
    "        y_true=np.array(actuals).flatten(),\n",
    "        y_pred=np.array(predictions).flatten(),\n",
    "        y_train=y_train.values.flatten(),\n",
    "        y_train_pred=train_predictions)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id, 'winsorize': winsorize, 'features': feature_set, 'metrics': metrics, 'method': 'walkforward', 'validation_predictions': predictions, 'model_config': config}\n",
    "\n",
    "def find_best_prophet_config(product_id, df_train, lagged_df=None, target_col=\"Sales\"):\n",
    "    \"\"\"Find best Prophet configuration with improved error handling\"\"\"\n",
    "    \n",
    "    # Validate input data\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    if df_train.index.isnull().any():\n",
    "        raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # Feature groups for original data\n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    normal_feature_combinations = generate_feature_combinations(normal_features, max_features=6)  \n",
    "    \n",
    "    # Test original data configurations\n",
    "    for winsorize in [True, False]:\n",
    "        for features in normal_feature_combinations:\n",
    "            try:\n",
    "                print(f\"\\nTesting - Winsorize: {winsorize}, Features: {features}\")\n",
    "\n",
    "                result = choose_parameters_prophet(product_id=product_id, df_train=data, feature_set=features, target_col=target_col, winsorize=winsorize)\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Configuration failed: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Test lagged data configurations if available\n",
    "    if lagged_df is not None:\n",
    "        lagged_data = lagged_df.copy()\n",
    "        lag_features = [col for col in lagged_data.columns if 'lag' in col]\n",
    "        ma_features = [col for col in lagged_data.columns if '_ma_' in col]\n",
    "        macro_features = [col for col in data.columns if col != target_col]\n",
    "        \n",
    "        # Generate lag feature combinations\n",
    "        lag_feature_combinations = generate_feature_combinations(lag_features, max_features=5)\n",
    "        \n",
    "        # Create combined feature sets\n",
    "        full_combinations = []\n",
    "        for lag_combo in lag_feature_combinations:\n",
    "            full_combinations.append(lag_combo)\n",
    "            full_combinations.append(lag_combo + macro_features)\n",
    "            full_combinations.append(lag_combo + ma_features)\n",
    "            full_combinations.append(lag_combo + macro_features + ma_features)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen = set()\n",
    "        unique_combinations = []\n",
    "        for combo in full_combinations:\n",
    "            combo_tuple = tuple(sorted(combo))\n",
    "            if combo_tuple not in seen:\n",
    "                seen.add(combo_tuple)\n",
    "                unique_combinations.append(combo)\n",
    "        \n",
    "        # Test all unique combinations\n",
    "        for winsorize in [True, False]:\n",
    "            for features in unique_combinations:\n",
    "                try:\n",
    "                    print(f\"\\nTesting - Lagged Features: {features}\")\n",
    "\n",
    "                    result = choose_parameters_prophet(product_id=product_id, df_train=lagged_data, feature_set=features, target_col=target_col, winsorize=winsorize)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"✅ Success - RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Lagged configuration failed: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration by RMSE\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best configuration for {product_id}:\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    print(f\"- Method: {best_config.get('method', 'walkforward')}\")\n",
    "    \n",
    "    return best_config, results\n",
    "\n",
    "def process_product_prophet(product_id):\n",
    "    \"\"\"Parallel processing wrapper for NeuralProphet\"\"\"\n",
    "    try:\n",
    "        df_train = product_dfs[product_id].rename(columns={product_id: \"Sales\"})\n",
    "        \n",
    "        # Ensure DatetimeIndex\n",
    "        if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "            df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "        \n",
    "        if df_train.index.isnull().any():\n",
    "            raise ValueError(\"❌ Some index values could not be parsed into datetime!\")\n",
    "\n",
    "        lagged_df = lagged_product_dfs.get(product_id, None)\n",
    "        \n",
    "        best_config, results = find_best_prophet_config(\n",
    "            product_id=product_id,\n",
    "            df_train=df_train,\n",
    "            lagged_df=lagged_df,\n",
    "            target_col=\"Sales\")\n",
    "        \n",
    "        return best_config, results\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {product_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_product_ids = (product_dfs.keys())\n",
    "\n",
    "all_results_prophet = []  # List to store all results\n",
    "best_configs_prophet = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_prophet, results_prophet = process_product_prophet(product_id)\n",
    "        \n",
    "        if best_config_prophet is not None:\n",
    "            best_configs_prophet.append(best_config_prophet)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_prophet is not None:\n",
    "            all_results_prophet.extend(results_prophet)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")\n",
    "\n",
    "df_all_results_prophet = convert_results_to_df(all_results_prophet) if all_results_prophet else pd.DataFrame()\n",
    "df_best_configs_prophet = pd.DataFrame(best_configs_prophet) if best_configs_prophet else pd.DataFrame()\n",
    "\n",
    "df_all_results_prophet.to_csv(\"prophet_best_configs.csv\", index=False)\n",
    "df_best_configs_prophet.to_csv(\"prophet_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.5. ARIMA**</span> <a id='arima'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_parameters_arima(product_id, df_train, feature_set, target_col=\"Sales\", \n",
    "                          winsorize=False, horizon=7, seasonal_period=None,\n",
    "                          manual_order=None, manual_seasonal_order=None):\n",
    "    \"\"\"\n",
    "    ARIMA/SARIMAX with:\n",
    "    - Support for both manual and auto-ARIMA\n",
    "    - Proper non-stationary handling\n",
    "    - Fixed dimensionality issues\n",
    "    \"\"\"\n",
    "    # --- Input Validation ---\n",
    "    if len(df_train) < 12:\n",
    "        print(f\"⚠️ Skipping {product_id} - only {len(df_train)} observations\")\n",
    "        return None\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    print('Preparing data...')\n",
    "    try:\n",
    "        X_train, X_val, y_train, y_val = prepare_time_series_data(df_train, feature_set, target_col, horizon, winsorize)\n",
    "        \n",
    "        # Convert to numpy arrays and ensure correct shapes\n",
    "        y_train = np.array(y_train).ravel()\n",
    "        y_val = np.array(y_val).ravel()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Data preparation failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # --- Prepare Exogenous Data ---\n",
    "    exog_train = np.array(X_train[feature_set]) if feature_set else None\n",
    "    exog_val = np.array(X_val[feature_set]) if feature_set else None\n",
    "    \n",
    "    # --- Walk-Forward Validation ---\n",
    "    print(f\"Using walk-forward validation ({len(X_train)} observations)\")\n",
    "    predictions = []\n",
    "    history = y_train.copy()\n",
    "    exog_history = exog_train.copy() if exog_train is not None else None\n",
    "    \n",
    "    # Get training predictions\n",
    "    try:\n",
    "        if manual_order:  # Manual configuration path\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                model = SARIMAX(history, exog=exog_history, order=manual_order,\n",
    "                              seasonal_order=manual_seasonal_order if manual_seasonal_order else (0,0,0,0),\n",
    "                              enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "                \n",
    "                train_predictions = model.get_prediction().predicted_mean\n",
    "\n",
    "        else:  # Auto-ARIMA path\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                train_model = auto_arima(history, X=exog_history, seasonal=seasonal_period is not None,\n",
    "                                      m=seasonal_period if seasonal_period else 1, \n",
    "                                      suppress_warnings=True, error_action='ignore',\n",
    "                                      test='adf') # Auto-detect differencing needs\n",
    "                \n",
    "                train_predictions = train_model.predict_in_sample(X=exog_history)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Training pred failed: {str(e)} - using last value\")\n",
    "        train_predictions = [history[-1]] * len(history)\n",
    "    \n",
    "    for i in range(horizon):\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                current_exog = exog_val[[i]] if exog_val is not None else None\n",
    "                \n",
    "                if manual_order:  # Manual fit\n",
    "                    model = SARIMAX(history, exog=exog_history, order=manual_order,\n",
    "                                  seasonal_order=manual_seasonal_order if manual_seasonal_order else (0,0,0,0),\n",
    "                                  enforce_stationarity=False).fit(disp=False)\n",
    "                    \n",
    "                    pred = model.forecast(steps=1, exog=current_exog)[0]\n",
    "\n",
    "                else:  # Auto-ARIMA\n",
    "                    model = auto_arima(history, X=exog_history, seasonal=seasonal_period is not None,\n",
    "                                     m=seasonal_period if seasonal_period else 1, \n",
    "                                     suppress_warnings=True, error_action='ignore')\n",
    "                    \n",
    "                    pred = model.predict(n_periods=1, X=current_exog)[0]\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                # Update history maintaining 1D array\n",
    "                history = np.concatenate([history, [pred]])\n",
    "                if exog_history is not None:\n",
    "                    exog_history = np.vstack([exog_history, exog_val[i]])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Iteration {i+1} failed: {str(e)}\")\n",
    "            predictions.append(history[-1])\n",
    "            continue\n",
    "    \n",
    "    metrics = calculate_metrics(y_true=y_val, y_pred=predictions, y_train=y_train, y_train_pred=train_predictions)\n",
    "    \n",
    "    return {\n",
    "        'product_id': product_id, 'winsorize': winsorize, 'features': feature_set,  \n",
    "        'metrics': metrics, 'method': 'manual' if manual_order else 'auto',\n",
    "        'validation_predictions': predictions, 'model_order': manual_order if manual_order else model.order,\n",
    "        'seasonal_order': manual_seasonal_order if manual_seasonal_order else (model.seasonal_order if hasattr(model, 'seasonal_order') else (0,0,0,0))}\n",
    "\n",
    "def find_best_arima_config(product_id, df_train, target_col=\"Sales\"):\n",
    "    \"\"\"\n",
    "    Tests both manual (from ACF/PACF) and auto-ARIMA configurations\n",
    "    \"\"\"\n",
    "        \n",
    "    #  DatetimeIndex\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        df_train.index = pd.to_datetime(df_train.index, errors=\"coerce\")\n",
    "\n",
    "    # Validate input\n",
    "    if not isinstance(df_train.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"❌ df_train index is not a DatetimeIndex!\")\n",
    "    \n",
    "    results = []\n",
    "    data = df_train.copy()\n",
    "    \n",
    "    # --- Manual Configurations from ACF/PACF Analysis ---\n",
    "    manual_configs = {\n",
    "        'P1': [{'order': (1,0,0)}], # AR(1)\n",
    "        'P3': [{'order': (1,0,0), 'seasonal_order': (1,0,0,12)}], # SARIMA(1,0,0)(1,0,0,12)\n",
    "        'P4': [{'order': (0,0,0), 'seasonal_order': (1,0,0,6)}], # SARIMA(0,0,0)(1,0,0,6)\n",
    "        'P5': [{'order': (0,0,0), 'seasonal_order': (1,0,1,6)}], # SARIMA(0,0,0)(1,0,1,6)\n",
    "        'P6': [{'order': (1,0,0)}], # AR(1)\n",
    "        'P8': [  # Non-stationary series\n",
    "            {'order': (3,1,3)},  # ARIMA(3,1,3) - with differencing\n",
    "            {'order': (3,0,3)}], # ARIMA(3,0,3) - without differencing for comparison\n",
    "        'P9': [{'order': (0,0,0), 'seasonal_order': (0,0,1,12)}], # SARIMA(0,0,0)(0,0,1,12)\n",
    "        'P11': [{'order': (1,0,0), 'seasonal_order': (1,0,0,12)}], # SARIMA(1,0,0)(1,0,0,12)\n",
    "        'P12': [{'order': (3,0,0)}],  # AR(3)\n",
    "        'P13': [\n",
    "            {'order': (0,0,0), 'seasonal_order': (1,0,0,17)},  # SARIMA(0,0,0)(1,0,0,17)\n",
    "            {'order': (1,0,0)}], # Also test standard AR(1) as fallback\n",
    "        'P14': [{'order': (0,0,0), 'seasonal_order': (1,0,0,6)}], # SARIMA(0,0,0)(1,0,0,6)\n",
    "        'P16': [{'order': (3,0,3)}],  # ARMA(3,3)\n",
    "        'P20': [{'order': (0,0,0), 'seasonal_order': (1,0,0,13)}], # SARIMA(0,0,0)(1,0,0,13)\n",
    "        'P36': [{'order': (0,0,0), 'seasonal_order': (1,0,1,10)}]} # SARIMA(0,0,0)(1,0,1,10)\n",
    "    \n",
    "    # --- Test Manual Configurations ---\n",
    "    if product_id in manual_configs:\n",
    "        for config in manual_configs[product_id]:\n",
    "            for winsorize in [True, False]:\n",
    "                try:\n",
    "                    result = choose_parameters_arima(product_id=product_id, df_train=data,\n",
    "                                                     feature_set=[f for f in data.columns if f != target_col],\n",
    "                                                     target_col=target_col,\n",
    "                                                     winsorize=winsorize,\n",
    "                                                     manual_order=config['order'],\n",
    "                                                     manual_seasonal_order=config.get('seasonal_order'))\n",
    "                    if result:\n",
    "                        print(f\"✅ Manual Success - Order: {result['model_order']} \"\n",
    "                              f\"Seasonal: {result['seasonal_order']} \"\n",
    "                              f\"RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Manual config failed: {str(e)}\")\n",
    "    \n",
    "    # --- Test Auto-ARIMA Configurations ---\n",
    "    seasonal_periods = {\n",
    "        'P3': 12, 'P4': 6, 'P5': 6, 'P9': 12,\n",
    "        'P11': 12, 'P13': 17, 'P14': 6, 'P20': 13, 'P36': 10}\n",
    "    \n",
    "    seasonal_period = seasonal_periods.get(product_id)\n",
    "    \n",
    "    normal_features = [col for col in data.columns if col != target_col]\n",
    "    feature_combinations = generate_feature_combinations(normal_features, max_features=6)\n",
    "    feature_combinations = [[]] + feature_combinations  # Include no-features option\n",
    "    \n",
    "    for winsorize in [True, False]:\n",
    "        for features in feature_combinations:\n",
    "            try:\n",
    "                result = choose_parameters_arima(product_id=product_id, df_train=data, feature_set=features,\n",
    "                                                 target_col=target_col, winsorize=winsorize, seasonal_period=seasonal_period)\n",
    "                if result:\n",
    "                    print(f\"✅ AutoARIMA Success - Features: {len(features)} \"\n",
    "                          f\"RMSE: {result['metrics']['RMSE']:.2f}\")\n",
    "                    results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ AutoARIMA failed: {str(e)}\")\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(f\"No valid ARIMA configurations for {product_id}\")\n",
    "    \n",
    "    # Select best configuration\n",
    "    best_config = min(results, key=lambda x: x['metrics']['RMSE'])\n",
    "    \n",
    "    print(f\"\\n✅ Best ARIMA configuration for {product_id}:\")\n",
    "    print(f\"- Method: {best_config['method']}\")\n",
    "    print(f\"- Winsorize: {best_config['winsorize']}\")\n",
    "    print(f\"- Features: {best_config['features']}\")\n",
    "    print(f\"- Order: {best_config['model_order']}\")\n",
    "    print(f\"- Seasonal Order: {best_config['seasonal_order']}\")\n",
    "    print(f\"- RMSE: {best_config['metrics']['RMSE']:.2f}\")\n",
    "    \n",
    "    return best_config, results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_product_ids = set(product_dfs.keys())\n",
    "\n",
    "all_results_arima = []  # List to store all results\n",
    "best_configs_arima = []  # List to store best configurations\n",
    "\n",
    "for product_id in all_product_ids:\n",
    "    print(f\"Processing product: {product_id}\")\n",
    "    \n",
    "    try:\n",
    "        best_config_arima, results_timegpt = find_best_arima_config(product_id, product_dfs[product_id])\n",
    "        \n",
    "        if best_config_arima is not None:\n",
    "            best_configs_arima.append(best_config_arima)  # Store best config\n",
    "        else:\n",
    "            print(f\"Warning: No best config returned for product {product_id}\")\n",
    "        \n",
    "        if results_timegpt is not None:\n",
    "            all_results_arima.extend(results_timegpt)  # Store results\n",
    "        else:\n",
    "            print(f\"Warning: No results returned for product {product_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing product {product_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df_all_results_arima = convert_results_to_df(all_results_arima) if all_results_arima else pd.DataFrame()\n",
    "df_best_configs_arima = pd.DataFrame(best_configs_arima) if best_configs_arima else pd.DataFrame()\n",
    "\n",
    "df_all_results_arima.to_csv(\"arima_best_configs.csv\", index=False)\n",
    "df_best_configs_arima.to_csv(\"arima_best_configs_details.csv\", index=False)\n",
    "\n",
    "print(\"Processing completed! Results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**3.6. Results Comparison**</span> <a id='compare'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='overflow-x: auto; white-space: nowrap; padding-bottom: 10px; border-bottom: 2px solid #ddd;'>\n",
       "    \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P1</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P1</td>\n",
       "      <td>True</td>\n",
       "      <td>PRO27826_org, MAB_ELE_SHP1100</td>\n",
       "      <td>3475393.6312</td>\n",
       "      <td>-0.3474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P1</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_SHP840', 'PRO27826_org', 'MAB_ELE_SHP1100']</td>\n",
       "      <td>2164903.6986</td>\n",
       "      <td>-0.5316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P1</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27276_org', 'PRO27826_org', 'MAB_ELE_PRO276', 'MAB_ELE_SHP1100']</td>\n",
       "      <td>2077977.6128</td>\n",
       "      <td>-0.9453</td>\n",
       "      <td>arima: auto; model_order: (1, 0, 0); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P1</td>\n",
       "      <td>True</td>\n",
       "      <td>['P1_lag_6', 'P1_lag_18', 'P1_ma_6', 'P1_ma_18']</td>\n",
       "      <td>1756694.2558</td>\n",
       "      <td>22.3548</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}</td>\n",
       "      <td>{'P1_lag_6': 0.30282173, 'P1_lag_18': 0.18046999, 'P1_ma_6': 0.30943987, 'P1_ma_18': 0.20726843}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P3</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P3</td>\n",
       "      <td>True</td>\n",
       "      <td>PRO27826_org</td>\n",
       "      <td>1400586.7625</td>\n",
       "      <td>-0.5273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P3</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27840_org']</td>\n",
       "      <td>4038685.9300</td>\n",
       "      <td>5.8188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P3</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27840_org']</td>\n",
       "      <td>2177873.1620</td>\n",
       "      <td>-0.3089</td>\n",
       "      <td>arima: auto; model_order: (1, 0, 1); seasonal_order: (0, 1, 1, 12)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P3</td>\n",
       "      <td>True</td>\n",
       "      <td>['P3_lag_12', 'P3_ma_12']</td>\n",
       "      <td>1142677.4785</td>\n",
       "      <td>-0.1396</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.9}</td>\n",
       "      <td>{'P3_lag_12': 0.5673661, 'P3_ma_12': 0.43263388}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P4</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P4</td>\n",
       "      <td>True</td>\n",
       "      <td>P4_lag_3</td>\n",
       "      <td>101402.7610</td>\n",
       "      <td>-0.4136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P4</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27380_org']</td>\n",
       "      <td>71391.6465</td>\n",
       "      <td>-0.5762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P4</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27380_org', 'WKLWEUR840_org', 'PRO27276_org', 'MAB_ELE_SHP380']</td>\n",
       "      <td>66180.8700</td>\n",
       "      <td>-0.5615</td>\n",
       "      <td>arima: manual; model_order: (0, 0, 0); seasonal_order: (1, 0, 0, 6)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P4</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27276_org']</td>\n",
       "      <td>54778.4267</td>\n",
       "      <td>-0.5468</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 1.0}</td>\n",
       "      <td>{'PRO27276_org': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P5</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P5</td>\n",
       "      <td>True</td>\n",
       "      <td>P5_lag_17</td>\n",
       "      <td>3874843.0186</td>\n",
       "      <td>-0.0804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P5</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27826_org', 'PRO271000_org']</td>\n",
       "      <td>2499421.6161</td>\n",
       "      <td>-0.1928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P5</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27826_org', 'PRO271000_org', 'MAB_ELE_PRO156']</td>\n",
       "      <td>2152211.6710</td>\n",
       "      <td>-0.5443</td>\n",
       "      <td>arima: auto; model_order: (0, 0, 0); seasonal_order: (0, 0, 0, 6)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P5</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27826_org']</td>\n",
       "      <td>1331522.0653</td>\n",
       "      <td>-0.5324</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.9}</td>\n",
       "      <td>{'PRO27826_org': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P6</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P6</td>\n",
       "      <td>True</td>\n",
       "      <td>PRO27276_org, RohCRUDE_PETRO1000_org</td>\n",
       "      <td>402068.4330</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P6</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27840_org']</td>\n",
       "      <td>165017.5023</td>\n",
       "      <td>-0.2988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P6</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27840_org', 'RohCRUDE_PETRO1000_org']</td>\n",
       "      <td>178899.4184</td>\n",
       "      <td>-0.7267</td>\n",
       "      <td>arima: auto; model_order: (1, 0, 0); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P6</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>113093.3087</td>\n",
       "      <td>1.0149</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P8</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P8</td>\n",
       "      <td>True</td>\n",
       "      <td>PRI27840_org</td>\n",
       "      <td>447712.4886</td>\n",
       "      <td>-0.3512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P8</td>\n",
       "      <td>True</td>\n",
       "      <td>['RohCOPPER1000_org']</td>\n",
       "      <td>518120.0389</td>\n",
       "      <td>0.7186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P8</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27840_org']</td>\n",
       "      <td>394832.8009</td>\n",
       "      <td>-0.7190</td>\n",
       "      <td>arima: auto; model_order: (1, 0, 2); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P8</td>\n",
       "      <td>True</td>\n",
       "      <td>['P8_lag_10', 'PRI27840_org', 'RohCOPPER1000_org', 'P8_ma_1', 'P8_ma_2', 'P8_ma_5', 'P8_ma_6', 'P8_ma_10']</td>\n",
       "      <td>108604.7529</td>\n",
       "      <td>6.7598</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}</td>\n",
       "      <td>{'P8_lag_10': 0.01589855, 'PRI27840_org': 0.039296027, 'RohCOPPER1000_org': 0.000489684, 'P8_ma_1': 0.94431573, 'P8_ma_2': 0.0, 'P8_ma_5': 0.0, 'P8_ma_6': 0.0, 'P8_ma_10': 0.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P9</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P9</td>\n",
       "      <td>True</td>\n",
       "      <td>P9_lag_12</td>\n",
       "      <td>5759.7658</td>\n",
       "      <td>-0.3789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P9</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27826_org', 'PRO28250_org']</td>\n",
       "      <td>16660.3161</td>\n",
       "      <td>6.3520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P9</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27826_org', 'PRO271000_org', 'PRO28250_org', 'MAB_ELE_PRO156']</td>\n",
       "      <td>5487.0260</td>\n",
       "      <td>-0.1833</td>\n",
       "      <td>arima: auto; model_order: (0, 0, 0); seasonal_order: (0, 0, 1, 12)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P9</td>\n",
       "      <td>False</td>\n",
       "      <td>['P9_lag_12', 'PRO27826_org', 'PRO271000_org', 'PRO28250_org', 'MAB_ELE_PRO156']</td>\n",
       "      <td>4008.9303</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 50, 'subsample': 0.9}</td>\n",
       "      <td>{'P9_lag_12': 0.1977397, 'PRO27826_org': 0.2058079, 'PRO271000_org': 0.32995924, 'PRO28250_org': 0.17148145, 'MAB_ELE_PRO156': 0.09501167}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P11</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P11</td>\n",
       "      <td>True</td>\n",
       "      <td>PRO27826_org, MAB_ELE_SHP392, MAB_ELE_SHP840</td>\n",
       "      <td>1059588.2394</td>\n",
       "      <td>-0.0451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P11</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_SHP276']</td>\n",
       "      <td>1910222.9242</td>\n",
       "      <td>3.4138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P11</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_SHP392']</td>\n",
       "      <td>780316.8904</td>\n",
       "      <td>-0.2118</td>\n",
       "      <td>arima: auto; model_order: (0, 0, 0); seasonal_order: (1, 0, 0, 12)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P11</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_SHP276']</td>\n",
       "      <td>626291.7346</td>\n",
       "      <td>-0.1633</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.05, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.9}</td>\n",
       "      <td>{'MAB_ELE_SHP276': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P12</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P12</td>\n",
       "      <td>True</td>\n",
       "      <td>P12_lag_1</td>\n",
       "      <td>110218.0367</td>\n",
       "      <td>-0.1264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P12</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27840_org', 'RohCOPPER1000_org']</td>\n",
       "      <td>121752.3175</td>\n",
       "      <td>0.1919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P12</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_PRO156']</td>\n",
       "      <td>100594.6894</td>\n",
       "      <td>-0.0542</td>\n",
       "      <td>arima: auto; model_order: (2, 1, 0); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P12</td>\n",
       "      <td>False</td>\n",
       "      <td>['P12_lag_12', 'P12_lag_18', 'P12_ma_1', 'P12_ma_12', 'P12_ma_18']</td>\n",
       "      <td>17858.2838</td>\n",
       "      <td>2.0481</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 0.9}</td>\n",
       "      <td>{'P12_lag_12': 0.018296111, 'P12_lag_18': 0.070963286, 'P12_ma_1': 0.8362892, 'P12_ma_12': 0.015341099, 'P12_ma_18': 0.059110217}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P13</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P13</td>\n",
       "      <td>True</td>\n",
       "      <td>P13_lag_8, MAB_ELE_PRO756, PRO27756_org, MAB_ELE_PRO276, PRI27840_org</td>\n",
       "      <td>11790.6738</td>\n",
       "      <td>-0.2123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P13</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27756_org']</td>\n",
       "      <td>13862.3953</td>\n",
       "      <td>-0.0480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P13</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27756_org']</td>\n",
       "      <td>10352.7444</td>\n",
       "      <td>-0.2999</td>\n",
       "      <td>arima: auto; model_order: (0, 0, 1); seasonal_order: (1, 0, 1, 17)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P13</td>\n",
       "      <td>False</td>\n",
       "      <td>['P13_lag_8', 'P13_ma_17', 'P13_ma_5', 'P13_ma_8']</td>\n",
       "      <td>10281.0456</td>\n",
       "      <td>-0.0769</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}</td>\n",
       "      <td>{'P13_lag_8': 0.08143915, 'P13_ma_17': 0.0, 'P13_ma_5': 0.8254354, 'P13_ma_8': 0.0931255}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P14</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P14</td>\n",
       "      <td>False</td>\n",
       "      <td>PRO27392_org, PRO28380_org, PRO27756_org</td>\n",
       "      <td>13538.4500</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P14</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27392_org', 'PRO28380_org']</td>\n",
       "      <td>10862.7748</td>\n",
       "      <td>-0.2337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P14</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27392_org', 'PRO28380_org', 'PRO27756_org']</td>\n",
       "      <td>11245.6947</td>\n",
       "      <td>-0.2437</td>\n",
       "      <td>arima: manual; model_order: (0, 0, 0); seasonal_order: (1, 0, 0, 6)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P14</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27392_org', 'PRO28380_org']</td>\n",
       "      <td>12716.0629</td>\n",
       "      <td>4.4095</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}</td>\n",
       "      <td>{'PRO27392_org': 0.46613783, 'PRO28380_org': 0.5338622}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P16</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P16</td>\n",
       "      <td>True</td>\n",
       "      <td>P16_lag_3</td>\n",
       "      <td>77236.7473</td>\n",
       "      <td>-0.5404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P16</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_PRO756', 'PRO28826_org']</td>\n",
       "      <td>105075.4854</td>\n",
       "      <td>-0.8144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P16</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>94194.8468</td>\n",
       "      <td>-0.8594</td>\n",
       "      <td>arima: auto; model_order: (1, 1, 0); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P16</td>\n",
       "      <td>False</td>\n",
       "      <td>['P16_lag_3', 'P16_lag_8', 'P16_lag_10', 'P16_ma_3', 'P16_ma_7', 'P16_ma_8', 'P16_ma_9', 'P16_ma_10']</td>\n",
       "      <td>56763.3690</td>\n",
       "      <td>4.0353</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1.0}</td>\n",
       "      <td>{'P16_lag_3': 0.03334086, 'P16_lag_8': 0.0713965, 'P16_lag_10': 0.12336791, 'P16_ma_3': 0.14022194, 'P16_ma_7': 0.59331197, 'P16_ma_8': 0.025533924, 'P16_ma_9': 0.009630987, 'P16_ma_10': 0.003195826}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P20</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P20</td>\n",
       "      <td>True</td>\n",
       "      <td>MAB_ELE_SHP250</td>\n",
       "      <td>2099.7026</td>\n",
       "      <td>-0.1154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P20</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO271000_org', 'PRO27250_org', 'PRO28392_org']</td>\n",
       "      <td>2203.1225</td>\n",
       "      <td>0.1744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P20</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27250_org']</td>\n",
       "      <td>2047.9236</td>\n",
       "      <td>-0.2173</td>\n",
       "      <td>arima: auto; model_order: (2, 0, 2); seasonal_order: (0, 0, 0, 13)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P20</td>\n",
       "      <td>False</td>\n",
       "      <td>['PRO27250_org', 'PRO28392_org']</td>\n",
       "      <td>685.6289</td>\n",
       "      <td>-0.1966</td>\n",
       "      <td>{'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'subsample': 0.9}</td>\n",
       "      <td>{'PRO27250_org': 0.48802555, 'PRO28392_org': 0.51197445}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>P36</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>P36</td>\n",
       "      <td>True</td>\n",
       "      <td>P36_lag_10</td>\n",
       "      <td>23561.8490</td>\n",
       "      <td>-0.5178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>P36</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_PRO156']</td>\n",
       "      <td>19721.3283</td>\n",
       "      <td>-0.4479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>P36</td>\n",
       "      <td>True</td>\n",
       "      <td>['MAB_ELE_PRO756', 'PRO27756_org', 'MAB_ELE_PRO156']</td>\n",
       "      <td>69003.6010</td>\n",
       "      <td>1.3745</td>\n",
       "      <td>arima: auto; model_order: (0, 0, 1); seasonal_order: (0, 1, 0, 10)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>P36</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO27756_org']</td>\n",
       "      <td>17449.4000</td>\n",
       "      <td>-0.3049</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1.0}</td>\n",
       "      <td>{'PRO27756_org': 1.0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "        <div style='display:inline-block; margin-right:25px; vertical-align:top;'>\n",
       "            <h4 style='text-align:center;'>Sales_Total</h4>\n",
       "            <div style='max-height:500px; overflow-y:auto; border:1px solid #ccc; padding:5px;'>\n",
       "                <table class=\"dataframe dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model</th>\n",
       "      <th>product_id</th>\n",
       "      <th>winsorize</th>\n",
       "      <th>features</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Overfit_Score</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Feature Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>TimeGPT</td>\n",
       "      <td>Sales_CPI</td>\n",
       "      <td>True</td>\n",
       "      <td>PRO27826_org</td>\n",
       "      <td>6050198.5830</td>\n",
       "      <td>-0.5468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Prophet</td>\n",
       "      <td>Sales_CPI</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27276_org', 'PRO27826_org']</td>\n",
       "      <td>13022125.5702</td>\n",
       "      <td>1.9384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ARIMA</td>\n",
       "      <td>Sales_CPI</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRO271000_org']</td>\n",
       "      <td>3750154.7173</td>\n",
       "      <td>-0.6474</td>\n",
       "      <td>arima: auto; model_order: (1, 0, 0); seasonal_order: (0, 0, 0, 0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Sales_CPI</td>\n",
       "      <td>True</td>\n",
       "      <td>['PRI27276_org', 'PRO27826_org']</td>\n",
       "      <td>3708435.1689</td>\n",
       "      <td>-0.2707</td>\n",
       "      <td>{'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}</td>\n",
       "      <td>{'PRI27276_org': 0.47724086, 'PRO27826_org': 0.52275914}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            </div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_excel_sheets_side_by_side(\"Modeling_results.xlsx\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"background-color:#000027; padding:5px; border-radius:5px;\">**4. Exogenous Variables Prediction**</span> <a id='market'></a>  \n",
    "\n",
    "Click [here](#table-of-contents) ⬆️ to return to the Index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for he Macro Features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to classify variables\n",
    "def classify_variable(series):\n",
    "    \"\"\"Classifies variable based on normality and stationarity tests.\"\"\"\n",
    "    \n",
    "    # Remove NaN values for testing\n",
    "    clean_series = series.dropna()\n",
    "\n",
    "    # Check for normality\n",
    "    if len(clean_series) > 3:\n",
    "        stat, p_value = shapiro(clean_series)\n",
    "        is_normal = p_value > 0.05  # p-value > 0.05 means normal\n",
    "    else:\n",
    "        is_normal = False  # Not enough data to test normality\n",
    "\n",
    "    # Check for stationarity\n",
    "    if len(clean_series) > 3:\n",
    "        adf_stat, adf_p_value, _, _, _, _ = adfuller(clean_series)\n",
    "        is_stationary = adf_p_value < 0.05  # p-value < 0.05 means stationary\n",
    "    else:\n",
    "        is_stationary = False  # Not enough data\n",
    "\n",
    "    return is_normal, is_stationary\n",
    "\n",
    "# Function to automatically fill missing values\n",
    "def auto_impute_missing_values(df_train, df_test):\n",
    "    \"\"\"Automatically selects the best imputation method for each missing variable.\"\"\"\n",
    "    \n",
    "    # Identify missing columns in test set\n",
    "    missing_columns = df_test.columns[df_test.isnull().any()]\n",
    "    \n",
    "    # Iterate through missing columns\n",
    "    for col in missing_columns:\n",
    "        print(f\"Processing: {col}\")\n",
    "\n",
    "        series = df_train[col]  # Use train data for imputation\n",
    "        is_normal, is_stationary = classify_variable(series)\n",
    "\n",
    "        if is_normal:\n",
    "            # Case 1: Normally distributed → Sample from normal distribution\n",
    "            print(f\" - {col} is normal → Using Mean & Std Sampling\")\n",
    "            mean_value, std_value = series.mean(), series.std()\n",
    "            num_missing = df_test[col].isnull().sum()\n",
    "            predictions = norm.rvs(loc=mean_value, scale=std_value, size=num_missing)\n",
    "        \n",
    "        elif is_stationary:\n",
    "            # Case 2: Stationary but non-normal → Simple Exponential Smoothing\n",
    "            print(f\" - {col} is stationary → Using Simple Exponential Smoothing\")\n",
    "            model = SimpleExpSmoothing(series.dropna()).fit()\n",
    "            predictions = model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        elif not is_stationary:\n",
    "            # Case 3: Non-Stationary → ARIMA\n",
    "            print(f\" - {col} is non-stationary → Using ARIMA\")\n",
    "            model = ARIMA(series.dropna(), order=(1, 1, 1))  # (p,d,q) chosen based on domain knowledge\n",
    "            fitted_model = model.fit()\n",
    "            predictions = fitted_model.forecast(steps=df_test[col].isnull().sum())\n",
    "\n",
    "        else:\n",
    "            # Case 4: If nothing works → Use XGBoost Regression\n",
    "            print(f\" - {col} is complex → Using XGBoost Regression\")\n",
    "            train_data = df_train.dropna(subset=[col])  # Drop missing values for training\n",
    "            X_train = train_data.drop(columns=[col])  # Exclude target column\n",
    "            y_train = train_data[col]  # Target column\n",
    "\n",
    "            X_test = df_test.loc[df_test[col].isnull(), X_train.columns]  # Only missing values\n",
    "\n",
    "            model = XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "        # Assign predictions\n",
    "        missing_indexes = df_test[df_test[col].isnull()].index\n",
    "        df_test.loc[missing_indexes, col] = predictions\n",
    "\n",
    "    return df_test\n",
    "\n",
    "# Example usage\n",
    "df_train = remerged_data[1]  # Use remerged train data\n",
    "df_test = test_1.copy()  # Copy test set\n",
    "\n",
    "# Apply automatic imputation\n",
    "df_test_filled = auto_impute_missing_values(df_train, df_test)\n",
    "\n",
    "# Check results\n",
    "print(df_test_filled.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
